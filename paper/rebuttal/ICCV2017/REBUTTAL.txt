We thank the reviewers for their time and constructive comments.

Venue: R2 writes: “The application has real value but is not of sufficient interest to the general CV community.” Biomedical Image Analysis is included in the call for papers of ICCV 2017, and the field of connectomics has previously given rise to interesting papers at I/ECCV (Liu et al. 2016, Parag et al. 2015, Azadi et al. 2014, Andres et al. 2012, Vazquez-Reina et al. 2011). R3 writes: “The methodological contribution in terms of putting the expert into the proofreading loop is relevant for being presented at ICCV.” We believe our method will also be interesting to researchers working on tasks beyond connectomics, as segmentation proofreading for labeled dataset collection and correction is widely applicable in computer vision.

Merge error detection: R1 notes that our description of merge error detection is not clear. We will replace it with the following simpler description: “To find merge errors, we generate candidate splits within a label segment and use our split error classifier to rate them. If the classifier yields a result close to 0, meaning the candidate split is actually likely to be a correct one, then we assume that the label has been merged in error and that this candidate split will fix the merge error.” To maintain reproducibility, in the paper we reported all parameters in addition to promising to release the source code.

Merge error detection performance: R1 states that merge error detection performance is “so poor that it should not be included in the paper.” We see our approach as a first step to correcting the more-difficult-to-find merge errors. R3 writes: “Learning the split errors and reusing the classifier for also selecting merge error propositions is a neat idea, and - despite still showing limitations in this work - is a good starting point to build upon in future work.” We will stronger emphasize the ‘early’ nature of this part of the work, and stress the limitations of our method.

- R1: Is this kind of discussion on over-segmentation/‘why merge error correction is harder’ suitable?
Typical connectomics segmentations begin with an over-segmentation (lines 498-499), in which we attempt to find all possible cell boundary edges. At this stage, some edges are missed due to low contrast - these missing edges cause merge errors. To correct this, we need to imagine where any number of edges might be among empty space, and this is simply a harder visual task than assessing whether an identified edge is correct. This is true even for a human: on the AC4 dataset, our experts only agree with the selection oracle in two thirds of merge error cases. 


P-values: R1 writes that “for VI comparison, p values should be provided. It should be stated if the p values are significant.” We would like to include the following text in the paper:
We use a single factor between-subject design with the factor being the proofreading method (GP, FP, or Dojo). Our hypothesis is that VI reduction is significantly better with GP than with other tools. For this, we treat VI as a continuous variable and use analysis of variance (ANOVA) followed by parametric tests (Welch's t-test).
AC4 subvolume. For novice performance, we observe a significant effect (alpha = 0.05) of which proofreading tool is used for the three conditions GP, FP, and Dojo [F(2,27) = 6.446, p = 0.005] when comparing the mean VI outcome. Post hoc comparisons (after Bonferroni correction) indicate that the mean VI for GP is significantly lower than for FP [t_27 = -2.7696, p = 0.0168], and that the mean VI for GP is significantly lower than for Dojo [t_27 = -4.407, p < 0.001]. This means that novices using GP perform significantly better than using FP and Dojo. A similar trend is visible when comparing the expert performance between GP and FP as the change in mean VI of GP is significantly better ([F(1,18) = 7.054, p = 0.016] and [t_18 = -2.6559, p = 0.0216]). For automatic selection with threshold, the difference in mean VI is very large and GP also performs significantly better ([F(1,18) = 89.902, p < 0.001] and [t_18 = 9.482, p < 0.001]). The final VI scores of the selection oracle with GP and FP are very similar and the difference between them is not significant [F(1,18) = 0.795, p = 0.384]. However, the VI reduction rate of GP is much higher as seen in Figure 6 (right).
L. Cylinder. The automatic selection with threshold yields similar results as on the other dataset and we observe a significant improvement when using GP instead of FP ([F(1,98) = 26.676, p < 0.001], post hoc comparison [t_98 = 5.1648, p < 0.001]). The selection oracles of GP and FP result in very similar final VI scores and the difference is not significant [F(1,98) = 0.071, p = 0.790], but GP reaches minimum VI faster in 10,000 corrections versus FP in 26,170 corrections.
