{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using gpu device 0: GeForce GTX TITAN (CNMeM is disabled, CuDNN 4007)\n",
      "/home/d/nolearn/local/lib/python2.7/site-packages/theano/tensor/signal/downsample.py:6: UserWarning: downsample module has been moved to the theano.tensor.signal.pool module.\n",
      "  \"downsample module has been moved to the theano.tensor.signal.pool module.\")\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import time\n",
    "import string\n",
    "\n",
    "import numpy as np\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "\n",
    "import os; import sys; sys.path.append('..')\n",
    "import gp\n",
    "\n",
    "import gp.nets as nets\n",
    "import gp.nets.BatchNormLayer as BatchNormLayer\n",
    "\n",
    "import lasagne\n",
    "\n",
    "sys.setrecursionlimit(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# helper function for projection_b\n",
    "def ceildiv(a, b):\n",
    "    return -(-a // b)\n",
    "\n",
    "def build_cnn(input_var=None, n=1, num_filters=8, cudnn='no'):\n",
    "    import lasagne # For some odd reason it can't read the global import, please PR/Issue if you know why\n",
    "    projection_type = 'B'\n",
    "    # Setting up layers\n",
    "    if cudnn == 'yes':\n",
    "        import lasagne.layers.dnn\n",
    "        conv = lasagne.layers.dnn.Conv2DDNNLayer # cuDNN\n",
    "    else:\n",
    "        conv = lasagne.layers.Conv2DLayer\n",
    "    dropout = lasagne.layers.DropoutLayer\n",
    "    nonlin = lasagne.nonlinearities.rectify\n",
    "    nonlin_layer = lasagne.layers.NonlinearityLayer\n",
    "    sumlayer = lasagne.layers.ElemwiseSumLayer\n",
    "    #batchnorm = BatchNormLayer.BatchNormLayer\n",
    "    batchnorm = lasagne.layers.BatchNormLayer\n",
    "\n",
    "    # Setting the projection type for when reducing height/width\n",
    "    # and increasing dimensions.\n",
    "    # Default is 'B' as B performs slightly better\n",
    "    # and A requires newer version of lasagne with ExpressionLayer\n",
    "    projection_type = 'B'\n",
    "    if projection_type == 'A':\n",
    "        expression = lasagne.layers.ExpressionLayer\n",
    "        pad = lasagne.layers.PadLayer\n",
    "\n",
    "    if projection_type == 'A':\n",
    "        # option A for projection as described in paper\n",
    "        # (should perform slightly worse than B)\n",
    "        def projection(l_inp):\n",
    "            n_filters = l_inp.output_shape[1]*2\n",
    "            l = expression(l_inp, lambda X: X[:, :, ::2, ::2], lambda s: (s[0], s[1], ceildiv(s[2], 2), ceildiv(s[3], 2)))\n",
    "            l = pad(l, [n_filters//4,0,0], batch_ndim=1)\n",
    "            return l\n",
    "\n",
    "    if projection_type == 'B':\n",
    "        # option B for projection as described in paper\n",
    "        def projection(l_inp):\n",
    "            # twice normal channels when projecting!\n",
    "            n_filters = l_inp.output_shape[1]*2 \n",
    "            l = conv(l_inp, num_filters=n_filters, filter_size=(1, 1),\n",
    "                     stride=(2, 2), nonlinearity=None, pad='same', b=None)\n",
    "            l = batchnorm(l)\n",
    "            return l\n",
    "\n",
    "    # helper function to handle filters/strides when increasing dims\n",
    "    def filters_increase_dims(l, increase_dims):\n",
    "        in_num_filters = l.output_shape[1]\n",
    "        if increase_dims:\n",
    "            first_stride = (2, 2)\n",
    "            out_num_filters = in_num_filters*2\n",
    "        else:\n",
    "            first_stride = (1, 1)\n",
    "            out_num_filters = in_num_filters\n",
    " \n",
    "        return out_num_filters, first_stride\n",
    "\n",
    "    # block as described and used in cifar in the original paper:\n",
    "    # http://arxiv.org/abs/1512.03385\n",
    "    def res_block_v1(l_inp, nonlinearity=nonlin, increase_dim=False):\n",
    "        # first figure filters/strides\n",
    "        n_filters, first_stride = filters_increase_dims(l_inp, increase_dim)\n",
    "        # conv -> BN -> nonlin -> conv -> BN -> sum -> nonlin\n",
    "        l = conv(l_inp, num_filters=n_filters, filter_size=(3, 3),\n",
    "                 stride=first_stride, nonlinearity=None, pad='same',\n",
    "                 W=lasagne.init.HeNormal(gain='relu'))\n",
    "        l = batchnorm(l)\n",
    "\n",
    "        l = nonlin_layer(l, nonlinearity=nonlin)\n",
    "#         l = dropout(l, p=.2)\n",
    "#         print('adding dropout')        \n",
    "        \n",
    "        l = conv(l, num_filters=n_filters, filter_size=(3, 3),\n",
    "                 stride=(1, 1), nonlinearity=None, pad='same',\n",
    "                 W=lasagne.init.HeNormal(gain='relu'))\n",
    "        l = batchnorm(l)\n",
    "        if increase_dim:\n",
    "            # Use projection (A, B) as described in paper\n",
    "            p = projection(l_inp)\n",
    "        else:\n",
    "            # Identity shortcut\n",
    "            p = l_inp\n",
    "        l = sumlayer([l, p])\n",
    "        l = nonlin_layer(l, nonlinearity=nonlin)\n",
    "        return l\n",
    "\n",
    "    # block as described in second paper on the subject (by same authors):\n",
    "    # http://arxiv.org/abs/1603.05027\n",
    "    def res_block_v2(l_inp, nonlinearity=nonlin, increase_dim=False):\n",
    "        # first figure filters/strides\n",
    "        n_filters, first_stride = filters_increase_dims(l_inp, increase_dim)\n",
    "        # BN -> nonlin -> conv -> BN -> nonlin -> conv -> sum\n",
    "        l = batchnorm(l_inp)\n",
    "        l = nonlin_layer(l, nonlinearity=nonlin)\n",
    "        l = conv(l, num_filters=n_filters, filter_size=(3, 3),\n",
    "                 stride=first_stride, nonlinearity=None, pad='same',\n",
    "                 W=lasagne.init.HeNormal(gain='relu'))\n",
    "        l = batchnorm(l)\n",
    "        l = nonlin_layer(l, nonlinearity=nonlin)\n",
    "        l = conv(l, num_filters=n_filters, filter_size=(3, 3),\n",
    "                 stride=(1, 1), nonlinearity=None, pad='same',\n",
    "                 W=lasagne.init.HeNormal(gain='relu'))\n",
    "        if increase_dim:\n",
    "            # Use projection (A, B) as described in paper\n",
    "            p = projection(l_inp)\n",
    "        else:\n",
    "            # Identity shortcut\n",
    "            p = l_inp\n",
    "        l = sumlayer([l, p])\n",
    "        return l\n",
    "\n",
    "    def bottleneck_block(l_inp, nonlinearity=nonlin, increase_dim=False):\n",
    "        # first figure filters/strides\n",
    "        n_filters, first_stride = filters_increase_dims(l_inp, increase_dim)\n",
    "        # conv -> BN -> nonlin -> conv -> BN -> nonlin -> conv -> BN -> sum\n",
    "        # -> nonlin\n",
    "        # first make the bottleneck, scale the filters ..!\n",
    "        scale = 4 # as per bottleneck architecture used in paper\n",
    "        scaled_filters = n_filters/scale\n",
    "        l = conv(l_inp, num_filters=scaled_filters, filter_size=(1, 1),\n",
    "                 stride=first_stride, nonlinearity=None, pad='same',\n",
    "                 W=lasagne.init.HeNormal(gain='relu'))\n",
    "        l = batchnorm(l)\n",
    "        l = nonlin_layer(l, nonlinearity=nonlin)\n",
    "        l = conv(l, num_filters=scaled_filters, filter_size=(3, 3),\n",
    "                 stride=(1, 1), nonlinearity=None, pad='same',\n",
    "                 W=lasagne.init.HeNormal(gain='relu'))\n",
    "        l = batchnorm(l)\n",
    "        l = nonlin_layer(l, nonlinearity=nonlin)\n",
    "        l = conv(l, num_filters=n_filters, filter_size=(1, 1),\n",
    "                 stride=(1, 1), nonlinearity=None, pad='same',\n",
    "                 W=lasagne.init.HeNormal(gain='relu'))\n",
    "        if increase_dim:\n",
    "            # Use projection (A, B) as described in paper\n",
    "            p = projection(l_inp)\n",
    "        else:\n",
    "            # Identity shortcut\n",
    "            p = l_inp\n",
    "        l = sumlayer([l, p])\n",
    "        l = nonlin_layer(l, nonlinearity=nonlin)\n",
    "        return l\n",
    "\n",
    "    # Bottleneck architecture with more efficiency (the post with Kaiming He's response)\n",
    "    # https://www.reddit.com/r/MachineLearning/comments/3ywi6x/deep_residual_learning_the_bottleneck/ \n",
    "    def bottleneck_block_fast(l_inp, nonlinearity=nonlin, increase_dim=False):\n",
    "        # first figure filters/strides\n",
    "        n_filters, last_stride = filters_increase_dims(l_inp, increase_dim)\n",
    "        # conv -> BN -> nonlin -> conv -> BN -> nonlin -> conv -> BN -> sum\n",
    "        # -> nonlin\n",
    "        # first make the bottleneck, scale the filters ..!\n",
    "        scale = 4 # as per bottleneck architecture used in paper\n",
    "        scaled_filters = n_filters/scale\n",
    "        l = conv(l_inp, num_filters=scaled_filters, filter_size=(1, 1),\n",
    "                 stride=(1, 1), nonlinearity=None, pad='same',\n",
    "                 W=lasagne.init.HeNormal(gain='relu'))\n",
    "        l = batchnorm(l)\n",
    "        l = nonlin_layer(l, nonlinearity=nonlin)\n",
    "        l = conv(l, num_filters=scaled_filters, filter_size=(3, 3),\n",
    "                 stride=(1, 1), nonlinearity=None, pad='same',\n",
    "                 W=lasagne.init.HeNormal(gain='relu'))\n",
    "        l = batchnorm(l)\n",
    "        l = nonlin_layer(l, nonlinearity=nonlin)\n",
    "        l = conv(l, num_filters=n_filters, filter_size=(1, 1),\n",
    "                 stride=last_stride, nonlinearity=None, pad='same',\n",
    "                 W=lasagne.init.HeNormal(gain='relu'))\n",
    "        if increase_dim:\n",
    "            # Use projection (A, B) as described in paper\n",
    "            p = projection(l_inp)\n",
    "        else:\n",
    "            # Identity shortcut\n",
    "            p = l_inp\n",
    "        l = sumlayer([l, p])\n",
    "        l = nonlin_layer(l, nonlinearity=nonlin)\n",
    "        return l\n",
    "       \n",
    "    res_block = res_block_v1\n",
    "\n",
    "    # Stacks the residual blocks, makes it easy to model size of architecture with int n   \n",
    "    def blockstack(l, n, nonlinearity=nonlin):\n",
    "        print('NNN',n)\n",
    "        for _ in range(n):\n",
    "            print ('new')\n",
    "            l = res_block(l, nonlinearity=nonlin)\n",
    "        return l\n",
    "\n",
    "    # Building the network\n",
    "    l_in = lasagne.layers.InputLayer(shape=(None, 4, 75, 75),\n",
    "                                        input_var=input_var)\n",
    "    # First layer! just a plain convLayer\n",
    "    l1 = conv(l_in, num_filters=num_filters, stride=(1, 1),\n",
    "              filter_size=(3, 3), nonlinearity=None, pad='same')\n",
    "    l1 = batchnorm(l1)\n",
    "    l1 = nonlin_layer(l1, nonlinearity=nonlin)\n",
    "\n",
    "    # Stacking bottlenecks and increasing dims! (while reducing shape size)\n",
    "#     l1_bs = blockstack(l1, n=n)\n",
    "#     l1_id = res_block(l1_bs, increase_dim=True)\n",
    "\n",
    "#     l2_bs = blockstack(l1_id, n=n)\n",
    "#     l2_id = res_block(l2_bs, increase_dim=True)\n",
    "\n",
    "#     l3_bs = blockstack(l2_id, n=n)\n",
    "\n",
    "    l3_bs = blockstack(l1, n=n)\n",
    "\n",
    "    l3_do = dropout(l3_bs, p=.5)\n",
    "    \n",
    "    # And, finally, the 10-unit output layer:\n",
    "    network = lasagne.layers.DenseLayer(\n",
    "            l3_do,\n",
    "#             l1,\n",
    "            num_units=2,\n",
    "            nonlinearity=lasagne.nonlinearities.softmax)\n",
    "\n",
    "    return network\n",
    "\n",
    "\n",
    "# ############################# Batch iterator ###############################\n",
    "# This is just a simple helper function iterating over training data in\n",
    "# mini-batches of a particular size, optionally in random order. It assumes\n",
    "# data is available as numpy arrays. For big datasets, you could load numpy\n",
    "# arrays as memory-mapped files (np.load(..., mmap_mode='r')), or write your\n",
    "# own custom data iteration function. For small datasets, you can also copy\n",
    "# them to GPU at once for slightly improved performance. This would involve\n",
    "# several changes in the main program, though, and is not demonstrated here.\n",
    "\n",
    "def iterate_minibatches(inputs, targets, batchsize, shuffle=False):\n",
    "    assert len(inputs) == len(targets)\n",
    "    if shuffle:\n",
    "        indices = np.arange(len(inputs))\n",
    "        np.random.shuffle(indices)\n",
    "    for start_idx in range(0, len(inputs) - batchsize + 1, batchsize):\n",
    "        if shuffle:\n",
    "            excerpt = indices[start_idx:start_idx + batchsize]\n",
    "        else:\n",
    "            excerpt = slice(start_idx, start_idx + batchsize)\n",
    "            \n",
    "            Xb = inputs[excerpt]\n",
    "            yb = targets[excerpt]\n",
    "            \n",
    "            Xb = Xb - .5\n",
    "            \n",
    "            k_s = np.array([0,1,2,3],dtype=np.uint8)\n",
    "            for i in range(len(Xb)):\n",
    "                k = np.random.choice(k_s)\n",
    "                for j in range(Xb.shape[1]):\n",
    "                    Xb[j][0] = np.rot90(Xb[j][0], k)\n",
    "                    \n",
    "            yield Xb, yb\n",
    "            \n",
    "#         yield inputs[excerpt], targets[excerpt]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded /home/d/patches//ipmlb/ in 0.147146940231 seconds.\n"
     ]
    }
   ],
   "source": [
    "PATCH_PATH = ('ipmlb')\n",
    "X_train, y_train, X_test, y_test = gp.Patch.load_rgba(PATCH_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_val = X_train[-X_train.shape[0]/4:]\n",
    "y_val = y_train[-X_train.shape[0]/4:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train2 = X_train[:-X_train.shape[0]/4]\n",
    "y_train2 = y_train[:-X_train.shape[0]/4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "n=2\n",
    "num_filters=32\n",
    "num_epochs=200\n",
    "cudnn='yes'\n",
    "print(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building model and compiling functions...\n",
      "NNN 2\n",
      "new\n",
      "new\n",
      "  layer output shapes:\n",
      "    InputLayer                       (None, 4, 75, 75)\n",
      "    Conv2DDNNLayer                   (None, 32, 75, 75)\n",
      "    BatchNormLayer                   (None, 32, 75, 75)\n",
      "    NonlinearityLayer                (None, 32, 75, 75)\n",
      "    Conv2DDNNLayer                   (None, 32, 75, 75)\n",
      "    BatchNormLayer                   (None, 32, 75, 75)\n",
      "    NonlinearityLayer                (None, 32, 75, 75)\n",
      "    Conv2DDNNLayer                   (None, 32, 75, 75)\n",
      "    BatchNormLayer                   (None, 32, 75, 75)\n",
      "    ElemwiseSumLayer                 (None, 32, 75, 75)\n",
      "    NonlinearityLayer                (None, 32, 75, 75)\n",
      "    Conv2DDNNLayer                   (None, 32, 75, 75)\n",
      "    BatchNormLayer                   (None, 32, 75, 75)\n",
      "    NonlinearityLayer                (None, 32, 75, 75)\n",
      "    Conv2DDNNLayer                   (None, 32, 75, 75)\n",
      "    BatchNormLayer                   (None, 32, 75, 75)\n",
      "    ElemwiseSumLayer                 (None, 32, 75, 75)\n",
      "    NonlinearityLayer                (None, 32, 75, 75)\n",
      "    DropoutLayer                     (None, 32, 75, 75)\n",
      "    DenseLayer                       (None, 2)\n",
      "  no. of InputLayers: 1\n",
      "  no. of Conv2DLayers: 5\n",
      "  no. of BatchNormLayers: 5\n",
      "  no. of NonlinearityLayers: 5\n",
      "  no. of DenseLayers: 1\n",
      "  no. of ElemwiseSumLayers: 2\n",
      "  no. of Unknown Layers: 1\n",
      "  total no. of layers: 20\n",
      "  no. of parameters: 398818\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Prepare Theano variables for inputs and targets\n",
    "input_var = T.tensor4('inputs')\n",
    "target_var = T.ivector('targets')\n",
    "\n",
    "# Create neural network model (depending on first command line parameter)\n",
    "print(\"Building model and compiling functions...\")\n",
    "network = build_cnn(input_var, n, num_filters, cudnn)\n",
    "all_layers = lasagne.layers.get_all_layers(network)\n",
    "num_params = lasagne.layers.count_params(network)\n",
    "num_conv = 0\n",
    "num_nonlin = 0\n",
    "num_input = 0\n",
    "num_batchnorm = 0\n",
    "num_elemsum = 0\n",
    "num_dense = 0\n",
    "num_unknown = 0\n",
    "print(\"  layer output shapes:\")\n",
    "for layer in all_layers:\n",
    "    name = string.ljust(layer.__class__.__name__, 32)\n",
    "    print(\"    %s %s\" %(name, lasagne.layers.get_output_shape(layer)))\n",
    "    if \"Conv2D\" in name:\n",
    "        num_conv += 1\n",
    "    elif \"NonlinearityLayer\" in name:\n",
    "        num_nonlin += 1\n",
    "    elif \"InputLayer\" in name:\n",
    "        num_input += 1\n",
    "    elif \"BatchNormLayer\" in name:\n",
    "        num_batchnorm += 1\n",
    "    elif \"ElemwiseSumLayer\" in name:\n",
    "        num_elemsum += 1\n",
    "    elif \"DenseLayer\" in name:\n",
    "        num_dense += 1\n",
    "    else:\n",
    "        num_unknown += 1\n",
    "print(\"  no. of InputLayers: %d\" % num_input)\n",
    "print(\"  no. of Conv2DLayers: %d\" % num_conv)\n",
    "print(\"  no. of BatchNormLayers: %d\" % num_batchnorm)\n",
    "print(\"  no. of NonlinearityLayers: %d\" % num_nonlin)\n",
    "print(\"  no. of DenseLayers: %d\" % num_dense)\n",
    "print(\"  no. of ElemwiseSumLayers: %d\" % num_elemsum)\n",
    "print(\"  no. of Unknown Layers: %d\" % num_unknown)\n",
    "print(\"  total no. of layers: %d\" % len(all_layers))\n",
    "print(\"  no. of parameters: %d\" % num_params)\n",
    "# Create a loss expression for training, i.e., a scalar objective we want\n",
    "# to minimize (for our multi-class problem, it is the cross-entropy loss):\n",
    "prediction = lasagne.layers.get_output(network)\n",
    "loss = lasagne.objectives.categorical_crossentropy(prediction, target_var)\n",
    "loss = loss.mean()\n",
    "# We could add some weight decay as well here, see lasagne.regularization.\n",
    "\n",
    "# Create update expressions for training, i.e., how to modify the\n",
    "# parameters at each training step. Here, we'll use Stochastic Gradient\n",
    "# Descent (SGD) with Nesterov momentum, but Lasagne offers plenty more.\n",
    "params = lasagne.layers.get_all_params(network, trainable=True)\n",
    "\n",
    "# several learning rates for low initial learning rates and\n",
    "# learning rate anealing (id is epoch)\n",
    "# learning_rate_schedule = {\n",
    "# 0: 0.0001, # low initial learning rate as described in paper\n",
    "# 2: 0.01,\n",
    "# 100: 0.001,\n",
    "# 150: 0.0001\n",
    "# }\n",
    "\n",
    "\n",
    "learning_rate = theano.shared(np.float32(0.03))\n",
    "momentum = theano.shared(np.float32(0.9))\n",
    "\n",
    "updates = lasagne.updates.nesterov_momentum(\n",
    "        loss, params, learning_rate=learning_rate, momentum=momentum)\n",
    "\n",
    "# Create a loss expression for validation/testing. The crucial difference\n",
    "# here is that we do a deterministic forward pass through the network,\n",
    "# disabling dropout layers.\n",
    "test_prediction = lasagne.layers.get_output(network, deterministic=True)\n",
    "test_loss = lasagne.objectives.categorical_crossentropy(test_prediction,\n",
    "                                                        target_var)\n",
    "test_loss = test_loss.mean()\n",
    "# As a bonus, also create an expression for the classification accuracy:\n",
    "test_acc = T.mean(T.eq(T.argmax(test_prediction, axis=1), target_var),\n",
    "                  dtype=theano.config.floatX)\n",
    "\n",
    "# Compile a function performing a training step on a mini-batch (by giving\n",
    "# the updates dictionary) and returning the corresponding training loss:\n",
    "train_fn = theano.function([input_var, target_var], loss, updates=updates)\n",
    "\n",
    "# Compile a second function computing the validation loss and accuracy:\n",
    "val_fn = theano.function([input_var, target_var], [test_loss, test_acc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with np.load('/home/d/resnet3_71.npz') as f:\n",
    "    param_values = [f['arr_%d' % i] for i in range(len(f.files))]\n",
    "    lasagne.layers.set_all_param_values(network, param_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_prediction = lasagne.layers.get_output(network, deterministic=True)\n",
    "test_loss = lasagne.objectives.categorical_crossentropy(test_prediction,\n",
    "                                                        target_var)\n",
    "test_loss = test_loss.mean()\n",
    "# As a bonus, also create an expression for the classification accuracy:\n",
    "test_acc = T.mean(T.eq(T.argmax(test_prediction, axis=1), target_var),\n",
    "                  dtype=theano.config.floatX)\n",
    "\n",
    "pred_fn = theano.function([input_var, target_var], [test_prediction, test_loss, test_acc])\n",
    "pred2_fn = theano.function([input_var], [test_prediction])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "6000\n",
      "7000\n",
      "8000\n",
      "9000\n",
      "10000\n",
      "11000\n",
      "12000\n",
      "13000\n",
      "14000\n",
      "15000\n",
      "16000\n",
      "17000\n"
     ]
    }
   ],
   "source": [
    "all_preds = []\n",
    "for i,p in enumerate(X_test):\n",
    "    pred = pred2_fn(p.reshape(1,4,75,75))\n",
    "    all_preds.append(pred[0][:,1][0].astype(np.uint8))\n",
    "    if i % 1000 == 0:\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, accuracy_score, roc_curve, auc, precision_recall_fscore_support, f1_score, precision_recall_curve, average_precision_score, zero_one_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.61      0.94      0.74      8780\n",
      "          1       0.87      0.38      0.53      8780\n",
      "\n",
      "avg / total       0.74      0.66      0.64     17560\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, all_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final results:\n",
      "  test loss:\t\t\t0.417827\n",
      "  test accuracy:\t\t91.68 %\n"
     ]
    }
   ],
   "source": [
    "# After training, we compute and print the test error:\n",
    "test_err = 0\n",
    "test_acc = 0\n",
    "test_batches = 0\n",
    "for batch in iterate_minibatches(X_test, y_test, 128, shuffle=False):\n",
    "    inputs, targets = batch\n",
    "    err, acc = val_fn(inputs, targets)\n",
    "    test_err += err\n",
    "    test_acc += acc\n",
    "    test_batches += 1\n",
    "print(\"Final results:\")\n",
    "print(\"  test loss:\\t\\t\\t{:.6f}\".format(test_err / test_batches))\n",
    "print(\"  test accuracy:\\t\\t{:.2f} %\".format(\n",
    "    test_acc / test_batches * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
