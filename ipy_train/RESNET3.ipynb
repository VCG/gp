{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using gpu device 0: GeForce GTX TITAN (CNMeM is disabled, CuDNN 4007)\n",
      "/home/d/nolearn/local/lib/python2.7/site-packages/theano/tensor/signal/downsample.py:6: UserWarning: downsample module has been moved to the theano.tensor.signal.pool module.\n",
      "  \"downsample module has been moved to the theano.tensor.signal.pool module.\")\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import time\n",
    "import string\n",
    "\n",
    "import numpy as np\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "\n",
    "import os; import sys; sys.path.append('..')\n",
    "import gp\n",
    "\n",
    "import gp.nets as nets\n",
    "import gp.nets.BatchNormLayer as BatchNormLayer\n",
    "\n",
    "import lasagne\n",
    "\n",
    "sys.setrecursionlimit(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# helper function for projection_b\n",
    "def ceildiv(a, b):\n",
    "    return -(-a // b)\n",
    "\n",
    "def build_cnn(input_var=None, n=1, num_filters=8, cudnn='no'):\n",
    "    import lasagne # For some odd reason it can't read the global import, please PR/Issue if you know why\n",
    "    projection_type = 'B'\n",
    "    # Setting up layers\n",
    "    if cudnn == 'yes':\n",
    "        import lasagne.layers.dnn\n",
    "        conv = lasagne.layers.dnn.Conv2DDNNLayer # cuDNN\n",
    "    else:\n",
    "        conv = lasagne.layers.Conv2DLayer\n",
    "    dropout = lasagne.layers.DropoutLayer\n",
    "    nonlin = lasagne.nonlinearities.rectify\n",
    "    nonlin_layer = lasagne.layers.NonlinearityLayer\n",
    "    sumlayer = lasagne.layers.ElemwiseSumLayer\n",
    "    #batchnorm = BatchNormLayer.BatchNormLayer\n",
    "    batchnorm = lasagne.layers.BatchNormLayer\n",
    "\n",
    "    # Setting the projection type for when reducing height/width\n",
    "    # and increasing dimensions.\n",
    "    # Default is 'B' as B performs slightly better\n",
    "    # and A requires newer version of lasagne with ExpressionLayer\n",
    "    projection_type = 'B'\n",
    "    if projection_type == 'A':\n",
    "        expression = lasagne.layers.ExpressionLayer\n",
    "        pad = lasagne.layers.PadLayer\n",
    "\n",
    "    if projection_type == 'A':\n",
    "        # option A for projection as described in paper\n",
    "        # (should perform slightly worse than B)\n",
    "        def projection(l_inp):\n",
    "            n_filters = l_inp.output_shape[1]*2\n",
    "            l = expression(l_inp, lambda X: X[:, :, ::2, ::2], lambda s: (s[0], s[1], ceildiv(s[2], 2), ceildiv(s[3], 2)))\n",
    "            l = pad(l, [n_filters//4,0,0], batch_ndim=1)\n",
    "            return l\n",
    "\n",
    "    if projection_type == 'B':\n",
    "        # option B for projection as described in paper\n",
    "        def projection(l_inp):\n",
    "            # twice normal channels when projecting!\n",
    "            n_filters = l_inp.output_shape[1]*2 \n",
    "            l = conv(l_inp, num_filters=n_filters, filter_size=(1, 1),\n",
    "                     stride=(2, 2), nonlinearity=None, pad='same', b=None)\n",
    "            l = batchnorm(l)\n",
    "            return l\n",
    "\n",
    "    # helper function to handle filters/strides when increasing dims\n",
    "    def filters_increase_dims(l, increase_dims):\n",
    "        in_num_filters = l.output_shape[1]\n",
    "        if increase_dims:\n",
    "            first_stride = (2, 2)\n",
    "            out_num_filters = in_num_filters*2\n",
    "        else:\n",
    "            first_stride = (1, 1)\n",
    "            out_num_filters = in_num_filters\n",
    " \n",
    "        return out_num_filters, first_stride\n",
    "\n",
    "    # block as described and used in cifar in the original paper:\n",
    "    # http://arxiv.org/abs/1512.03385\n",
    "    def res_block_v1(l_inp, nonlinearity=nonlin, increase_dim=False):\n",
    "        # first figure filters/strides\n",
    "        n_filters, first_stride = filters_increase_dims(l_inp, increase_dim)\n",
    "        # conv -> BN -> nonlin -> conv -> BN -> sum -> nonlin\n",
    "        l = conv(l_inp, num_filters=n_filters, filter_size=(3, 3),\n",
    "                 stride=first_stride, nonlinearity=None, pad='same',\n",
    "                 W=lasagne.init.HeNormal(gain='relu'))\n",
    "        l = batchnorm(l)\n",
    "\n",
    "        l = nonlin_layer(l, nonlinearity=nonlin)\n",
    "#         l = dropout(l, p=.2)\n",
    "#         print('adding dropout')        \n",
    "        \n",
    "        l = conv(l, num_filters=n_filters, filter_size=(3, 3),\n",
    "                 stride=(1, 1), nonlinearity=None, pad='same',\n",
    "                 W=lasagne.init.HeNormal(gain='relu'))\n",
    "        l = batchnorm(l)\n",
    "        if increase_dim:\n",
    "            # Use projection (A, B) as described in paper\n",
    "            p = projection(l_inp)\n",
    "        else:\n",
    "            # Identity shortcut\n",
    "            p = l_inp\n",
    "        l = sumlayer([l, p])\n",
    "        l = nonlin_layer(l, nonlinearity=nonlin)\n",
    "        return l\n",
    "\n",
    "    # block as described in second paper on the subject (by same authors):\n",
    "    # http://arxiv.org/abs/1603.05027\n",
    "    def res_block_v2(l_inp, nonlinearity=nonlin, increase_dim=False):\n",
    "        # first figure filters/strides\n",
    "        n_filters, first_stride = filters_increase_dims(l_inp, increase_dim)\n",
    "        # BN -> nonlin -> conv -> BN -> nonlin -> conv -> sum\n",
    "        l = batchnorm(l_inp)\n",
    "        l = nonlin_layer(l, nonlinearity=nonlin)\n",
    "        l = conv(l, num_filters=n_filters, filter_size=(3, 3),\n",
    "                 stride=first_stride, nonlinearity=None, pad='same',\n",
    "                 W=lasagne.init.HeNormal(gain='relu'))\n",
    "        l = batchnorm(l)\n",
    "        l = nonlin_layer(l, nonlinearity=nonlin)\n",
    "        l = conv(l, num_filters=n_filters, filter_size=(3, 3),\n",
    "                 stride=(1, 1), nonlinearity=None, pad='same',\n",
    "                 W=lasagne.init.HeNormal(gain='relu'))\n",
    "        if increase_dim:\n",
    "            # Use projection (A, B) as described in paper\n",
    "            p = projection(l_inp)\n",
    "        else:\n",
    "            # Identity shortcut\n",
    "            p = l_inp\n",
    "        l = sumlayer([l, p])\n",
    "        return l\n",
    "\n",
    "    def bottleneck_block(l_inp, nonlinearity=nonlin, increase_dim=False):\n",
    "        # first figure filters/strides\n",
    "        n_filters, first_stride = filters_increase_dims(l_inp, increase_dim)\n",
    "        # conv -> BN -> nonlin -> conv -> BN -> nonlin -> conv -> BN -> sum\n",
    "        # -> nonlin\n",
    "        # first make the bottleneck, scale the filters ..!\n",
    "        scale = 4 # as per bottleneck architecture used in paper\n",
    "        scaled_filters = n_filters/scale\n",
    "        l = conv(l_inp, num_filters=scaled_filters, filter_size=(1, 1),\n",
    "                 stride=first_stride, nonlinearity=None, pad='same',\n",
    "                 W=lasagne.init.HeNormal(gain='relu'))\n",
    "        l = batchnorm(l)\n",
    "        l = nonlin_layer(l, nonlinearity=nonlin)\n",
    "        l = conv(l, num_filters=scaled_filters, filter_size=(3, 3),\n",
    "                 stride=(1, 1), nonlinearity=None, pad='same',\n",
    "                 W=lasagne.init.HeNormal(gain='relu'))\n",
    "        l = batchnorm(l)\n",
    "        l = nonlin_layer(l, nonlinearity=nonlin)\n",
    "        l = conv(l, num_filters=n_filters, filter_size=(1, 1),\n",
    "                 stride=(1, 1), nonlinearity=None, pad='same',\n",
    "                 W=lasagne.init.HeNormal(gain='relu'))\n",
    "        if increase_dim:\n",
    "            # Use projection (A, B) as described in paper\n",
    "            p = projection(l_inp)\n",
    "        else:\n",
    "            # Identity shortcut\n",
    "            p = l_inp\n",
    "        l = sumlayer([l, p])\n",
    "        l = nonlin_layer(l, nonlinearity=nonlin)\n",
    "        return l\n",
    "\n",
    "    # Bottleneck architecture with more efficiency (the post with Kaiming He's response)\n",
    "    # https://www.reddit.com/r/MachineLearning/comments/3ywi6x/deep_residual_learning_the_bottleneck/ \n",
    "    def bottleneck_block_fast(l_inp, nonlinearity=nonlin, increase_dim=False):\n",
    "        # first figure filters/strides\n",
    "        n_filters, last_stride = filters_increase_dims(l_inp, increase_dim)\n",
    "        # conv -> BN -> nonlin -> conv -> BN -> nonlin -> conv -> BN -> sum\n",
    "        # -> nonlin\n",
    "        # first make the bottleneck, scale the filters ..!\n",
    "        scale = 4 # as per bottleneck architecture used in paper\n",
    "        scaled_filters = n_filters/scale\n",
    "        l = conv(l_inp, num_filters=scaled_filters, filter_size=(1, 1),\n",
    "                 stride=(1, 1), nonlinearity=None, pad='same',\n",
    "                 W=lasagne.init.HeNormal(gain='relu'))\n",
    "        l = batchnorm(l)\n",
    "        l = nonlin_layer(l, nonlinearity=nonlin)\n",
    "        l = conv(l, num_filters=scaled_filters, filter_size=(3, 3),\n",
    "                 stride=(1, 1), nonlinearity=None, pad='same',\n",
    "                 W=lasagne.init.HeNormal(gain='relu'))\n",
    "        l = batchnorm(l)\n",
    "        l = nonlin_layer(l, nonlinearity=nonlin)\n",
    "        l = conv(l, num_filters=n_filters, filter_size=(1, 1),\n",
    "                 stride=last_stride, nonlinearity=None, pad='same',\n",
    "                 W=lasagne.init.HeNormal(gain='relu'))\n",
    "        if increase_dim:\n",
    "            # Use projection (A, B) as described in paper\n",
    "            p = projection(l_inp)\n",
    "        else:\n",
    "            # Identity shortcut\n",
    "            p = l_inp\n",
    "        l = sumlayer([l, p])\n",
    "        l = nonlin_layer(l, nonlinearity=nonlin)\n",
    "        return l\n",
    "       \n",
    "    res_block = res_block_v1\n",
    "\n",
    "    # Stacks the residual blocks, makes it easy to model size of architecture with int n   \n",
    "    def blockstack(l, n, nonlinearity=nonlin):\n",
    "        print('NNN',n)\n",
    "        for _ in range(n):\n",
    "            print ('new')\n",
    "            l = res_block(l, nonlinearity=nonlin)\n",
    "        return l\n",
    "\n",
    "    # Building the network\n",
    "    l_in = lasagne.layers.InputLayer(shape=(None, 4, 75, 75),\n",
    "                                        input_var=input_var)\n",
    "    # First layer! just a plain convLayer\n",
    "    l1 = conv(l_in, num_filters=num_filters, stride=(1, 1),\n",
    "              filter_size=(3, 3), nonlinearity=None, pad='same')\n",
    "    l1 = batchnorm(l1)\n",
    "    l1 = nonlin_layer(l1, nonlinearity=nonlin)\n",
    "\n",
    "    # Stacking bottlenecks and increasing dims! (while reducing shape size)\n",
    "#     l1_bs = blockstack(l1, n=n)\n",
    "#     l1_id = res_block(l1_bs, increase_dim=True)\n",
    "\n",
    "#     l2_bs = blockstack(l1_id, n=n)\n",
    "#     l2_id = res_block(l2_bs, increase_dim=True)\n",
    "\n",
    "#     l3_bs = blockstack(l2_id, n=n)\n",
    "\n",
    "    l3_bs = blockstack(l1, n=n)\n",
    "\n",
    "    l3_do = dropout(l3_bs, p=.5)\n",
    "    \n",
    "    # And, finally, the 10-unit output layer:\n",
    "    network = lasagne.layers.DenseLayer(\n",
    "            l3_do,\n",
    "#             l1,\n",
    "            num_units=2,\n",
    "            nonlinearity=lasagne.nonlinearities.softmax)\n",
    "\n",
    "    return network\n",
    "\n",
    "\n",
    "# ############################# Batch iterator ###############################\n",
    "# This is just a simple helper function iterating over training data in\n",
    "# mini-batches of a particular size, optionally in random order. It assumes\n",
    "# data is available as numpy arrays. For big datasets, you could load numpy\n",
    "# arrays as memory-mapped files (np.load(..., mmap_mode='r')), or write your\n",
    "# own custom data iteration function. For small datasets, you can also copy\n",
    "# them to GPU at once for slightly improved performance. This would involve\n",
    "# several changes in the main program, though, and is not demonstrated here.\n",
    "\n",
    "def iterate_minibatches(inputs, targets, batchsize, shuffle=False):\n",
    "    assert len(inputs) == len(targets)\n",
    "    if shuffle:\n",
    "        indices = np.arange(len(inputs))\n",
    "        np.random.shuffle(indices)\n",
    "    for start_idx in range(0, len(inputs) - batchsize + 1, batchsize):\n",
    "        if shuffle:\n",
    "            excerpt = indices[start_idx:start_idx + batchsize]\n",
    "        else:\n",
    "            excerpt = slice(start_idx, start_idx + batchsize)\n",
    "            \n",
    "            Xb = inputs[excerpt]\n",
    "            yb = targets[excerpt]\n",
    "            \n",
    "            Xb = Xb - .5\n",
    "            \n",
    "            k_s = np.array([0,1,2,3],dtype=np.uint8)\n",
    "            for i in range(len(Xb)):\n",
    "                k = np.random.choice(k_s)\n",
    "                for j in range(Xb.shape[1]):\n",
    "                    Xb[j][0] = np.rot90(Xb[j][0], k)\n",
    "                    \n",
    "            yield Xb, yb\n",
    "            \n",
    "#         yield inputs[excerpt], targets[excerpt]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded /home/d/patches//ipmlb/ in 0.192438840866 seconds.\n"
     ]
    }
   ],
   "source": [
    "PATCH_PATH = ('ipmlb')\n",
    "X_train, y_train, X_test, y_test = gp.Patch.load_rgba(PATCH_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_val = X_train[-X_train.shape[0]/4:]\n",
    "y_val = y_train[-X_train.shape[0]/4:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train2 = X_train[:-X_train.shape[0]/4]\n",
    "y_train2 = y_train[:-X_train.shape[0]/4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "n=2\n",
    "num_filters=32\n",
    "num_epochs=200\n",
    "cudnn='yes'\n",
    "print(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building model and compiling functions...\n",
      "NNN 2\n",
      "new\n",
      "new\n",
      "  layer output shapes:\n",
      "    InputLayer                       (None, 4, 75, 75)\n",
      "    Conv2DDNNLayer                   (None, 32, 75, 75)\n",
      "    BatchNormLayer                   (None, 32, 75, 75)\n",
      "    NonlinearityLayer                (None, 32, 75, 75)\n",
      "    Conv2DDNNLayer                   (None, 32, 75, 75)\n",
      "    BatchNormLayer                   (None, 32, 75, 75)\n",
      "    NonlinearityLayer                (None, 32, 75, 75)\n",
      "    Conv2DDNNLayer                   (None, 32, 75, 75)\n",
      "    BatchNormLayer                   (None, 32, 75, 75)\n",
      "    ElemwiseSumLayer                 (None, 32, 75, 75)\n",
      "    NonlinearityLayer                (None, 32, 75, 75)\n",
      "    Conv2DDNNLayer                   (None, 32, 75, 75)\n",
      "    BatchNormLayer                   (None, 32, 75, 75)\n",
      "    NonlinearityLayer                (None, 32, 75, 75)\n",
      "    Conv2DDNNLayer                   (None, 32, 75, 75)\n",
      "    BatchNormLayer                   (None, 32, 75, 75)\n",
      "    ElemwiseSumLayer                 (None, 32, 75, 75)\n",
      "    NonlinearityLayer                (None, 32, 75, 75)\n",
      "    DropoutLayer                     (None, 32, 75, 75)\n",
      "    DenseLayer                       (None, 2)\n",
      "  no. of InputLayers: 1\n",
      "  no. of Conv2DLayers: 5\n",
      "  no. of BatchNormLayers: 5\n",
      "  no. of NonlinearityLayers: 5\n",
      "  no. of DenseLayers: 1\n",
      "  no. of ElemwiseSumLayers: 2\n",
      "  no. of Unknown Layers: 1\n",
      "  total no. of layers: 20\n",
      "  no. of parameters: 398818\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Prepare Theano variables for inputs and targets\n",
    "input_var = T.tensor4('inputs')\n",
    "target_var = T.ivector('targets')\n",
    "\n",
    "# Create neural network model (depending on first command line parameter)\n",
    "print(\"Building model and compiling functions...\")\n",
    "network = build_cnn(input_var, n, num_filters, cudnn)\n",
    "all_layers = lasagne.layers.get_all_layers(network)\n",
    "num_params = lasagne.layers.count_params(network)\n",
    "num_conv = 0\n",
    "num_nonlin = 0\n",
    "num_input = 0\n",
    "num_batchnorm = 0\n",
    "num_elemsum = 0\n",
    "num_dense = 0\n",
    "num_unknown = 0\n",
    "print(\"  layer output shapes:\")\n",
    "for layer in all_layers:\n",
    "    name = string.ljust(layer.__class__.__name__, 32)\n",
    "    print(\"    %s %s\" %(name, lasagne.layers.get_output_shape(layer)))\n",
    "    if \"Conv2D\" in name:\n",
    "        num_conv += 1\n",
    "    elif \"NonlinearityLayer\" in name:\n",
    "        num_nonlin += 1\n",
    "    elif \"InputLayer\" in name:\n",
    "        num_input += 1\n",
    "    elif \"BatchNormLayer\" in name:\n",
    "        num_batchnorm += 1\n",
    "    elif \"ElemwiseSumLayer\" in name:\n",
    "        num_elemsum += 1\n",
    "    elif \"DenseLayer\" in name:\n",
    "        num_dense += 1\n",
    "    else:\n",
    "        num_unknown += 1\n",
    "print(\"  no. of InputLayers: %d\" % num_input)\n",
    "print(\"  no. of Conv2DLayers: %d\" % num_conv)\n",
    "print(\"  no. of BatchNormLayers: %d\" % num_batchnorm)\n",
    "print(\"  no. of NonlinearityLayers: %d\" % num_nonlin)\n",
    "print(\"  no. of DenseLayers: %d\" % num_dense)\n",
    "print(\"  no. of ElemwiseSumLayers: %d\" % num_elemsum)\n",
    "print(\"  no. of Unknown Layers: %d\" % num_unknown)\n",
    "print(\"  total no. of layers: %d\" % len(all_layers))\n",
    "print(\"  no. of parameters: %d\" % num_params)\n",
    "# Create a loss expression for training, i.e., a scalar objective we want\n",
    "# to minimize (for our multi-class problem, it is the cross-entropy loss):\n",
    "prediction = lasagne.layers.get_output(network)\n",
    "loss = lasagne.objectives.categorical_crossentropy(prediction, target_var)\n",
    "loss = loss.mean()\n",
    "# We could add some weight decay as well here, see lasagne.regularization.\n",
    "\n",
    "# Create update expressions for training, i.e., how to modify the\n",
    "# parameters at each training step. Here, we'll use Stochastic Gradient\n",
    "# Descent (SGD) with Nesterov momentum, but Lasagne offers plenty more.\n",
    "params = lasagne.layers.get_all_params(network, trainable=True)\n",
    "\n",
    "# several learning rates for low initial learning rates and\n",
    "# learning rate anealing (id is epoch)\n",
    "# learning_rate_schedule = {\n",
    "# 0: 0.0001, # low initial learning rate as described in paper\n",
    "# 2: 0.01,\n",
    "# 100: 0.001,\n",
    "# 150: 0.0001\n",
    "# }\n",
    "\n",
    "\n",
    "learning_rate = theano.shared(np.float32(0.03))\n",
    "momentum = theano.shared(np.float32(0.9))\n",
    "\n",
    "updates = lasagne.updates.nesterov_momentum(\n",
    "        loss, params, learning_rate=learning_rate, momentum=momentum)\n",
    "\n",
    "# Create a loss expression for validation/testing. The crucial difference\n",
    "# here is that we do a deterministic forward pass through the network,\n",
    "# disabling dropout layers.\n",
    "test_prediction = lasagne.layers.get_output(network, deterministic=True)\n",
    "test_loss = lasagne.objectives.categorical_crossentropy(test_prediction,\n",
    "                                                        target_var)\n",
    "test_loss = test_loss.mean()\n",
    "# As a bonus, also create an expression for the classification accuracy:\n",
    "test_acc = T.mean(T.eq(T.argmax(test_prediction, axis=1), target_var),\n",
    "                  dtype=theano.config.floatX)\n",
    "\n",
    "# Compile a function performing a training step on a mini-batch (by giving\n",
    "# the updates dictionary) and returning the corresponding training loss:\n",
    "train_fn = theano.function([input_var, target_var], loss, updates=updates)\n",
    "\n",
    "# Compile a second function computing the validation loss and accuracy:\n",
    "val_fn = theano.function([input_var, target_var], [test_loss, test_acc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ls = np.linspace(0.001, 0.00001, 500)\n",
    "ms = np.linspace(0.9,0.999, 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.9       ,  0.90004952,  0.90009905, ...,  0.99890095,\n",
       "        0.99895048,  0.999     ])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n",
      "Epoch 1 of 200 took 662.606s\n",
      "  training loss:\t\t0.937953\n",
      "  validation loss:\t\t0.223316\n",
      "  validation accuracy:\t\t91.60 %\n",
      "Epoch 2 of 200 took 664.411s\n",
      "  training loss:\t\t0.226017\n",
      "  validation loss:\t\t0.201293\n",
      "  validation accuracy:\t\t92.68 %\n",
      "Epoch 3 of 200 took 687.225s\n",
      "  training loss:\t\t0.196231\n",
      "  validation loss:\t\t0.189342\n",
      "  validation accuracy:\t\t93.29 %\n",
      "Epoch 4 of 200 took 659.851s\n",
      "  training loss:\t\t0.179828\n",
      "  validation loss:\t\t0.184115\n",
      "  validation accuracy:\t\t93.38 %\n",
      "Epoch 5 of 200 took 733.110s\n",
      "  training loss:\t\t0.167589\n",
      "  validation loss:\t\t0.168800\n",
      "  validation accuracy:\t\t94.01 %\n",
      "Epoch 6 of 200 took 662.906s\n",
      "  training loss:\t\t0.156839\n",
      "  validation loss:\t\t0.164934\n",
      "  validation accuracy:\t\t94.10 %\n",
      "Epoch 7 of 200 took 665.412s\n",
      "  training loss:\t\t0.146163\n",
      "  validation loss:\t\t0.155299\n",
      "  validation accuracy:\t\t94.55 %\n",
      "Epoch 8 of 200 took 666.527s\n",
      "  training loss:\t\t0.135897\n",
      "  validation loss:\t\t0.148727\n",
      "  validation accuracy:\t\t94.76 %\n",
      "Epoch 9 of 200 took 666.543s\n",
      "  training loss:\t\t0.127363\n",
      "  validation loss:\t\t0.139568\n",
      "  validation accuracy:\t\t95.17 %\n",
      "Epoch 10 of 200 took 667.111s\n",
      "  training loss:\t\t0.118440\n",
      "  validation loss:\t\t0.137999\n",
      "  validation accuracy:\t\t95.21 %\n",
      "Epoch 11 of 200 took 666.885s\n",
      "  training loss:\t\t0.109327\n",
      "  validation loss:\t\t0.124119\n",
      "  validation accuracy:\t\t95.82 %\n",
      "Epoch 12 of 200 took 665.790s\n",
      "  training loss:\t\t0.102899\n",
      "  validation loss:\t\t0.126674\n",
      "  validation accuracy:\t\t95.74 %\n",
      "Epoch 13 of 200 took 663.812s\n",
      "  training loss:\t\t0.096295\n",
      "  validation loss:\t\t0.123467\n",
      "  validation accuracy:\t\t95.81 %\n",
      "Epoch 14 of 200 took 660.642s\n",
      "  training loss:\t\t0.089404\n",
      "  validation loss:\t\t0.104791\n",
      "  validation accuracy:\t\t96.70 %\n",
      "Epoch 15 of 200 took 657.007s\n",
      "  training loss:\t\t0.084205\n",
      "  validation loss:\t\t0.105902\n",
      "  validation accuracy:\t\t96.53 %\n",
      "Epoch 16 of 200 took 657.243s\n",
      "  training loss:\t\t0.079594\n",
      "  validation loss:\t\t0.101337\n",
      "  validation accuracy:\t\t96.73 %\n",
      "Epoch 17 of 200 took 659.938s\n",
      "  training loss:\t\t0.074845\n",
      "  validation loss:\t\t0.091654\n",
      "  validation accuracy:\t\t97.18 %\n",
      "Epoch 18 of 200 took 658.894s\n",
      "  training loss:\t\t0.070366\n",
      "  validation loss:\t\t0.089954\n",
      "  validation accuracy:\t\t97.18 %\n",
      "Epoch 19 of 200 took 659.680s\n",
      "  training loss:\t\t0.067047\n",
      "  validation loss:\t\t0.080955\n",
      "  validation accuracy:\t\t97.56 %\n",
      "Epoch 20 of 200 took 663.577s\n",
      "  training loss:\t\t0.063577\n",
      "  validation loss:\t\t0.082177\n",
      "  validation accuracy:\t\t97.46 %\n",
      "Epoch 21 of 200 took 657.155s\n",
      "  training loss:\t\t0.060548\n",
      "  validation loss:\t\t0.080996\n",
      "  validation accuracy:\t\t97.55 %\n",
      "Epoch 22 of 200 took 658.137s\n",
      "  training loss:\t\t0.057299\n",
      "  validation loss:\t\t0.074788\n",
      "  validation accuracy:\t\t97.77 %\n",
      "Epoch 23 of 200 took 657.487s\n",
      "  training loss:\t\t0.055619\n",
      "  validation loss:\t\t0.071725\n",
      "  validation accuracy:\t\t97.96 %\n",
      "Epoch 24 of 200 took 663.795s\n",
      "  training loss:\t\t0.052473\n",
      "  validation loss:\t\t0.072526\n",
      "  validation accuracy:\t\t97.90 %\n",
      "Epoch 25 of 200 took 658.621s\n",
      "  training loss:\t\t0.051052\n",
      "  validation loss:\t\t0.068644\n",
      "  validation accuracy:\t\t98.09 %\n",
      "Epoch 26 of 200 took 657.160s\n",
      "  training loss:\t\t0.047935\n",
      "  validation loss:\t\t0.064766\n",
      "  validation accuracy:\t\t98.23 %\n",
      "Epoch 27 of 200 took 663.590s\n",
      "  training loss:\t\t0.046345\n",
      "  validation loss:\t\t0.065319\n",
      "  validation accuracy:\t\t98.25 %\n",
      "Epoch 28 of 200 took 661.698s\n",
      "  training loss:\t\t0.045118\n",
      "  validation loss:\t\t0.070969\n",
      "  validation accuracy:\t\t98.00 %\n",
      "Epoch 29 of 200 took 657.249s\n",
      "  training loss:\t\t0.042683\n",
      "  validation loss:\t\t0.066242\n",
      "  validation accuracy:\t\t98.19 %\n",
      "Epoch 30 of 200 took 663.999s\n",
      "  training loss:\t\t0.041307\n",
      "  validation loss:\t\t0.066110\n",
      "  validation accuracy:\t\t98.17 %\n",
      "Epoch 31 of 200 took 663.793s\n",
      "  training loss:\t\t0.040973\n",
      "  validation loss:\t\t0.060375\n",
      "  validation accuracy:\t\t98.44 %\n",
      "Epoch 32 of 200 took 657.646s\n",
      "  training loss:\t\t0.038105\n",
      "  validation loss:\t\t0.059657\n",
      "  validation accuracy:\t\t98.50 %\n",
      "Epoch 33 of 200 took 663.888s\n",
      "  training loss:\t\t0.038133\n",
      "  validation loss:\t\t0.067607\n",
      "  validation accuracy:\t\t98.18 %\n",
      "Epoch 34 of 200 took 657.576s\n",
      "  training loss:\t\t0.036906\n",
      "  validation loss:\t\t0.056237\n",
      "  validation accuracy:\t\t98.63 %\n",
      "Epoch 35 of 200 took 664.014s\n",
      "  training loss:\t\t0.036314\n",
      "  validation loss:\t\t0.051644\n",
      "  validation accuracy:\t\t98.81 %\n",
      "Epoch 36 of 200 took 657.686s\n",
      "  training loss:\t\t0.034619\n",
      "  validation loss:\t\t0.061010\n",
      "  validation accuracy:\t\t98.48 %\n",
      "Epoch 37 of 200 took 663.985s\n",
      "  training loss:\t\t0.033807\n",
      "  validation loss:\t\t0.061600\n",
      "  validation accuracy:\t\t98.48 %\n",
      "Epoch 38 of 200 took 663.525s\n",
      "  training loss:\t\t0.032480\n",
      "  validation loss:\t\t0.055099\n",
      "  validation accuracy:\t\t98.69 %\n",
      "Epoch 39 of 200 took 661.946s\n",
      "  training loss:\t\t0.031126\n",
      "  validation loss:\t\t0.054035\n",
      "  validation accuracy:\t\t98.78 %\n",
      "Epoch 40 of 200 took 661.845s\n",
      "  training loss:\t\t0.031411\n",
      "  validation loss:\t\t0.054087\n",
      "  validation accuracy:\t\t98.77 %\n",
      "Epoch 41 of 200 took 663.690s\n",
      "  training loss:\t\t0.029822\n",
      "  validation loss:\t\t0.056645\n",
      "  validation accuracy:\t\t98.69 %\n",
      "Epoch 42 of 200 took 660.735s\n",
      "  training loss:\t\t0.029895\n",
      "  validation loss:\t\t0.055020\n",
      "  validation accuracy:\t\t98.75 %\n",
      "Epoch 43 of 200 took 663.765s\n",
      "  training loss:\t\t0.028971\n",
      "  validation loss:\t\t0.056711\n",
      "  validation accuracy:\t\t98.71 %\n",
      "Epoch 44 of 200 took 663.534s\n",
      "  training loss:\t\t0.028184\n",
      "  validation loss:\t\t0.053281\n",
      "  validation accuracy:\t\t98.86 %\n",
      "Epoch 45 of 200 took 663.664s\n",
      "  training loss:\t\t0.027933\n",
      "  validation loss:\t\t0.049880\n",
      "  validation accuracy:\t\t98.94 %\n",
      "Epoch 46 of 200 took 663.416s\n",
      "  training loss:\t\t0.026475\n",
      "  validation loss:\t\t0.056416\n",
      "  validation accuracy:\t\t98.73 %\n",
      "Epoch 47 of 200 took 657.543s\n",
      "  training loss:\t\t0.026823\n",
      "  validation loss:\t\t0.055897\n",
      "  validation accuracy:\t\t98.75 %\n",
      "Epoch 48 of 200 took 657.988s\n",
      "  training loss:\t\t0.025803\n",
      "  validation loss:\t\t0.057899\n",
      "  validation accuracy:\t\t98.74 %\n",
      "Epoch 49 of 200 took 663.945s\n",
      "  training loss:\t\t0.025874\n",
      "  validation loss:\t\t0.054348\n",
      "  validation accuracy:\t\t98.79 %\n",
      "Epoch 50 of 200 took 660.083s\n",
      "  training loss:\t\t0.024649\n",
      "  validation loss:\t\t0.047255\n",
      "  validation accuracy:\t\t99.05 %\n",
      "Epoch 51 of 200 took 665.081s\n",
      "  training loss:\t\t0.024287\n",
      "  validation loss:\t\t0.052188\n",
      "  validation accuracy:\t\t98.86 %\n",
      "Epoch 52 of 200 took 661.586s\n",
      "  training loss:\t\t0.024219\n",
      "  validation loss:\t\t0.051638\n",
      "  validation accuracy:\t\t98.88 %\n",
      "Epoch 53 of 200 took 664.298s\n",
      "  training loss:\t\t0.023321\n",
      "  validation loss:\t\t0.050389\n",
      "  validation accuracy:\t\t98.95 %\n",
      "Epoch 54 of 200 took 660.701s\n",
      "  training loss:\t\t0.022970\n",
      "  validation loss:\t\t0.053118\n",
      "  validation accuracy:\t\t98.89 %\n",
      "Epoch 55 of 200 took 658.973s\n",
      "  training loss:\t\t0.023467\n",
      "  validation loss:\t\t0.049495\n",
      "  validation accuracy:\t\t99.04 %\n",
      "Epoch 56 of 200 took 664.368s\n",
      "  training loss:\t\t0.022338\n",
      "  validation loss:\t\t0.051100\n",
      "  validation accuracy:\t\t98.95 %\n",
      "Epoch 57 of 200 took 664.473s\n",
      "  training loss:\t\t0.022697\n",
      "  validation loss:\t\t0.052051\n",
      "  validation accuracy:\t\t98.94 %\n",
      "Epoch 58 of 200 took 664.073s\n",
      "  training loss:\t\t0.022099\n",
      "  validation loss:\t\t0.058287\n",
      "  validation accuracy:\t\t98.76 %\n",
      "Epoch 59 of 200 took 665.214s\n",
      "  training loss:\t\t0.022367\n",
      "  validation loss:\t\t0.046671\n",
      "  validation accuracy:\t\t99.05 %\n",
      "Epoch 60 of 200 took 664.504s\n",
      "  training loss:\t\t0.020793\n",
      "  validation loss:\t\t0.052088\n",
      "  validation accuracy:\t\t98.95 %\n",
      "Epoch 61 of 200 took 665.758s\n",
      "  training loss:\t\t0.019419\n",
      "  validation loss:\t\t0.049025\n",
      "  validation accuracy:\t\t99.10 %\n",
      "Epoch 62 of 200 took 665.126s\n",
      "  training loss:\t\t0.019939\n",
      "  validation loss:\t\t0.048744\n",
      "  validation accuracy:\t\t99.06 %\n",
      "Epoch 63 of 200 took 667.139s\n",
      "  training loss:\t\t0.020278\n",
      "  validation loss:\t\t0.046441\n",
      "  validation accuracy:\t\t99.06 %\n",
      "Epoch 64 of 200 took 665.361s\n",
      "  training loss:\t\t0.019602\n",
      "  validation loss:\t\t0.052416\n",
      "  validation accuracy:\t\t98.97 %\n",
      "Epoch 65 of 200 took 672.012s\n",
      "  training loss:\t\t0.019823\n",
      "  validation loss:\t\t0.045010\n",
      "  validation accuracy:\t\t99.13 %\n",
      "Epoch 66 of 200 took 671.612s\n",
      "  training loss:\t\t0.018273\n",
      "  validation loss:\t\t0.046685\n",
      "  validation accuracy:\t\t99.10 %\n",
      "Epoch 67 of 200 took 666.796s\n",
      "  training loss:\t\t0.018982\n",
      "  validation loss:\t\t0.052471\n",
      "  validation accuracy:\t\t98.99 %\n",
      "Epoch 68 of 200 took 666.159s\n",
      "  training loss:\t\t0.019384\n",
      "  validation loss:\t\t0.046585\n",
      "  validation accuracy:\t\t99.11 %\n",
      "Epoch 69 of 200 took 664.275s\n",
      "  training loss:\t\t0.018489\n",
      "  validation loss:\t\t0.049853\n",
      "  validation accuracy:\t\t99.06 %\n",
      "Epoch 70 of 200 took 665.192s\n",
      "  training loss:\t\t0.018842\n",
      "  validation loss:\t\t0.052743\n",
      "  validation accuracy:\t\t98.99 %\n",
      "Epoch 71 of 200 took 664.890s\n",
      "  training loss:\t\t0.017918\n",
      "  validation loss:\t\t0.046609\n",
      "  validation accuracy:\t\t99.13 %\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-fb0ea7412741>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0miterate_minibatches\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m128\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m         \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtargets\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m         \u001b[0mtrain_err\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mtrain_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m         \u001b[0mtrain_batches\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/d/nolearn/local/lib/python2.7/site-packages/theano/compile/function_module.pyc\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    857\u001b[0m         \u001b[0mt0_fn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    858\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 859\u001b[1;33m             \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    860\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    861\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'position_of_error'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "# Finally, launch the training loop.\n",
    "print(\"Starting training...\")\n",
    "t_loss = []\n",
    "v_loss = []\n",
    "# We iterate over epochs:\n",
    "for epoch in range(num_epochs):\n",
    "#     if epoch in learning_rate_schedule:\n",
    "#         lr = np.float32(learning_rate_schedule[epoch])\n",
    "#         print(\" setting learning rate to %.7f\" % lr)\n",
    "    \n",
    "    learning_rate.set_value(np.float32(ls[epoch]))\n",
    "    momentum.set_value(np.float32(ms[epoch]))\n",
    "    # In each epoch, we do a full pass over the training data:\n",
    "    train_err = 0\n",
    "    train_batches = 0\n",
    "    start_time = time.time()\n",
    "    for batch in iterate_minibatches(X_train2, y_train2, 128, shuffle=False):\n",
    "        inputs, targets = batch\n",
    "        train_err += train_fn(inputs, targets)\n",
    "        train_batches += 1\n",
    "\n",
    "    # And a full pass over the validation data:\n",
    "    val_err = 0\n",
    "    val_acc = 0\n",
    "    val_batches = 0\n",
    "    for batch in iterate_minibatches(X_val, y_val, 128, shuffle=False):\n",
    "        inputs, targets = batch\n",
    "        err, acc = val_fn(inputs, targets)\n",
    "        val_err += err\n",
    "        val_acc += acc\n",
    "        val_batches += 1\n",
    "\n",
    "    # Then we print the results for this epoch:\n",
    "    print(\"Epoch {} of {} took {:.3f}s\".format(\n",
    "        epoch + 1, num_epochs, time.time() - start_time))\n",
    "    print(\"  training loss:\\t\\t{:.6f}\".format(train_err / train_batches))\n",
    "    print(\"  validation loss:\\t\\t{:.6f}\".format(val_err / val_batches))\n",
    "    print(\"  validation accuracy:\\t\\t{:.2f} %\".format(\n",
    "        val_acc / val_batches * 100))\n",
    "    \n",
    "    t_loss.append(train_err / train_batches)\n",
    "    v_loss.append(val_err / val_batches)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final results:\n",
      "  test loss:\t\t\t0.417827\n",
      "  test accuracy:\t\t91.68 %\n"
     ]
    }
   ],
   "source": [
    "# After training, we compute and print the test error:\n",
    "test_err = 0\n",
    "test_acc = 0\n",
    "test_batches = 0\n",
    "for batch in iterate_minibatches(X_test, y_test, 128, shuffle=False):\n",
    "    inputs, targets = batch\n",
    "    err, acc = val_fn(inputs, targets)\n",
    "    test_err += err\n",
    "    test_acc += acc\n",
    "    test_batches += 1\n",
    "print(\"Final results:\")\n",
    "print(\"  test loss:\\t\\t\\t{:.6f}\".format(test_err / test_batches))\n",
    "print(\"  test accuracy:\\t\\t{:.2f} %\".format(\n",
    "    test_acc / test_batches * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.savez('/home/d/resnet3_71.npz', *lasagne.layers.get_all_param_values(network))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<lasagne.layers.special.NonlinearityLayer at 0x7f674cf3b410>"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "network.input_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_loss(net):\n",
    "    train_loss = [row['train_loss'] for row in net.train_history_]\n",
    "    valid_loss = [row['valid_loss'] for row in net.train_history_]\n",
    "    plt.plot(train_loss, label='train loss')\n",
    "    plt.plot(valid_loss, label='valid loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.ylabel('loss')\n",
    "    plt.legend(loc='best')\n",
    "    return plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DenseLayer' object has no attribute 'train_history_'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-eedbaacf80c0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mplot_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnetwork\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-13-f862f5ec8e53>\u001b[0m in \u001b[0;36mplot_loss\u001b[1;34m(net)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mplot_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0mtrain_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mrow\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'train_loss'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mrow\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mnet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_history_\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m     \u001b[0mvalid_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mrow\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'valid_loss'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mrow\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mnet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_history_\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'train loss'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalid_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'valid loss'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'DenseLayer' object has no attribute 'train_history_'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Finally, launch the training loop.\n",
    "print(\"Starting training...\")\n",
    "# We iterate over epochs:\n",
    "for epoch in range(200,200+num_epochs):\n",
    "    if epoch in learning_rate_schedule:\n",
    "        lr = np.float32(learning_rate_schedule[epoch])\n",
    "        print(\" setting learning rate to %.7f\" % lr)\n",
    "        learning_rate.set_value(lr)\n",
    "    # In each epoch, we do a full pass over the training data:\n",
    "    train_err = 0\n",
    "    train_batches = 0\n",
    "    start_time = time.time()\n",
    "    for batch in iterate_minibatches(X_train2, y_train2, 128, shuffle=False):\n",
    "        inputs, targets = batch\n",
    "        train_err += train_fn(inputs, targets)\n",
    "        train_batches += 1\n",
    "\n",
    "    # And a full pass over the validation data:\n",
    "    val_err = 0\n",
    "    val_acc = 0\n",
    "    val_batches = 0\n",
    "    for batch in iterate_minibatches(X_val, y_val, 128, shuffle=False):\n",
    "        inputs, targets = batch\n",
    "        err, acc = val_fn(inputs, targets)\n",
    "        val_err += err\n",
    "        val_acc += acc\n",
    "        val_batches += 1\n",
    "\n",
    "    # Then we print the results for this epoch:\n",
    "    print(\"Epoch {} of {} took {:.3f}s\".format(\n",
    "        epoch + 1, num_epochs, time.time() - start_time))\n",
    "    print(\"  training loss:\\t\\t{:.6f}\".format(train_err / train_batches))\n",
    "    print(\"  validation loss:\\t\\t{:.6f}\".format(val_err / val_batches))\n",
    "    print(\"  validation accuracy:\\t\\t{:.2f} %\".format(\n",
    "        val_acc / val_batches * 100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 0.0001, 2: 0.01, 100: 0.001, 150: 0.0001}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learning_rate_schedule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "328"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.15785868913180343"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_err / train_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final results:\n",
      "  test loss:\t\t\t0.190347\n",
      "  test accuracy:\t\t93.34 %\n"
     ]
    }
   ],
   "source": [
    "# After training, we compute and print the test error:\n",
    "test_err = 0\n",
    "test_acc = 0\n",
    "test_batches = 0\n",
    "for batch in iterate_minibatches(X_test, y_test, 128, shuffle=False):\n",
    "    inputs, targets = batch\n",
    "    err, acc = val_fn(inputs, targets)\n",
    "    test_err += err\n",
    "    test_acc += acc\n",
    "    test_batches += 1\n",
    "print(\"Final results:\")\n",
    "print(\"  test loss:\\t\\t\\t{:.6f}\".format(test_err / test_batches))\n",
    "print(\"  test accuracy:\\t\\t{:.2f} %\".format(\n",
    "    test_acc / test_batches * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final results:\n",
      "  test loss:\t\t\t0.190400\n",
      "  test accuracy:\t\t93.36 %\n"
     ]
    }
   ],
   "source": [
    "# After training, we compute and print the test error:\n",
    "test_err = 0\n",
    "test_acc = 0\n",
    "test_batches = 0\n",
    "for batch in iterate_minibatches(X_test, y_test, 128, shuffle=False):\n",
    "    inputs, targets = batch\n",
    "    err, acc = val_fn(inputs, targets)\n",
    "    test_err += err\n",
    "    test_acc += acc\n",
    "    test_batches += 1\n",
    "print(\"Final results:\")\n",
    "print(\"  test loss:\\t\\t\\t{:.6f}\".format(test_err / test_batches))\n",
    "print(\"  test accuracy:\\t\\t{:.2f} %\".format(\n",
    "    test_acc / test_batches * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "94.174360795454547"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_acc / val_batches * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.16246618782593444"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_err / val_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.savez('/home/d/resnet2_after328.npz', *lasagne.layers.get_all_param_values(network))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "    # Optionally, you could now dump the network weights to a file like this:\n",
    "    # np.savez('model.npz', *lasagne.layers.get_all_param_values(network))\n",
    "    #\n",
    "    # And load them again later on like this:\n",
    "    # with np.load('model.npz') as f:\n",
    "    #     param_values = [f['arr_%d' % i] for i in range(len(f.files))]\n",
    "    # lasagne.layers.set_all_param_values(network, param_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#\n",
    "#\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Outputs must be theano Variable or Out instances. Received 125.6015625 of type <type 'numpy.float64'>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-19-759fc37d2e5a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mpred_fn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtheano\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0minput_var\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_var\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mtest_prediction\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_acc\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m/home/d/nolearn/local/lib/python2.7/site-packages/theano/compile/function.pyc\u001b[0m in \u001b[0;36mfunction\u001b[1;34m(inputs, outputs, mode, updates, givens, no_default_updates, accept_inplace, name, rebuild_strict, allow_input_downcast, profile, on_unused_input)\u001b[0m\n\u001b[0;32m    318\u001b[0m                    \u001b[0mon_unused_input\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mon_unused_input\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    319\u001b[0m                    \u001b[0mprofile\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mprofile\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 320\u001b[1;33m                    output_keys=output_keys)\n\u001b[0m\u001b[0;32m    321\u001b[0m     \u001b[1;31m# We need to add the flag check_aliased inputs if we have any mutable or\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    322\u001b[0m     \u001b[1;31m# borrowed used defined inputs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/d/nolearn/local/lib/python2.7/site-packages/theano/compile/pfunc.pyc\u001b[0m in \u001b[0;36mpfunc\u001b[1;34m(params, outputs, mode, updates, givens, no_default_updates, accept_inplace, name, rebuild_strict, allow_input_downcast, profile, on_unused_input, output_keys)\u001b[0m\n\u001b[0;32m    440\u001b[0m                                          \u001b[0mrebuild_strict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrebuild_strict\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    441\u001b[0m                                          \u001b[0mcopy_inputs_over\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 442\u001b[1;33m                                          no_default_updates=no_default_updates)\n\u001b[0m\u001b[0;32m    443\u001b[0m     \u001b[1;31m# extracting the arguments\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    444\u001b[0m     \u001b[0minput_variables\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcloned_extended_outputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mother_stuff\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moutput_vars\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/d/nolearn/local/lib/python2.7/site-packages/theano/compile/pfunc.pyc\u001b[0m in \u001b[0;36mrebuild_collect_shared\u001b[1;34m(outputs, inputs, replace, updates, rebuild_strict, copy_inputs_over, no_default_updates)\u001b[0m\n\u001b[0;32m    225\u001b[0m                 raise TypeError('Outputs must be theano Variable or '\n\u001b[0;32m    226\u001b[0m                                 \u001b[1;34m'Out instances. Received '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 227\u001b[1;33m                                 ' of type ' + str(type(v)))\n\u001b[0m\u001b[0;32m    228\u001b[0m             \u001b[1;31m# computed_list.append(cloned_v)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    229\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: Outputs must be theano Variable or Out instances. Received 125.6015625 of type <type 'numpy.float64'>"
     ]
    }
   ],
   "source": [
    "pred_fn = theano.function([input_var, target_var], [test_prediction, test_loss, test_acc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "    test_prediction = lasagne.layers.get_output(network, deterministic=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "MissingInputError",
     "evalue": "(\"An input of the graph, used to compute Shape(targets), was not provided and not given a value.Use the Theano flag exception_verbosity='high',for more information on this error.\", targets)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMissingInputError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-40-a06ce32be050>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtarget_var\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m/home/d/nolearn/local/lib/python2.7/site-packages/theano/gof/graph.pyc\u001b[0m in \u001b[0;36meval\u001b[1;34m(self, inputs_to_values)\u001b[0m\n\u001b[0;32m    518\u001b[0m         \u001b[0minputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msorted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs_to_values\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    519\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0minputs\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fn_cache\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 520\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fn_cache\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtheano\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    521\u001b[0m         \u001b[0margs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0minputs_to_values\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mparam\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mparam\u001b[0m \u001b[1;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    522\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/d/nolearn/local/lib/python2.7/site-packages/theano/compile/function.pyc\u001b[0m in \u001b[0;36mfunction\u001b[1;34m(inputs, outputs, mode, updates, givens, no_default_updates, accept_inplace, name, rebuild_strict, allow_input_downcast, profile, on_unused_input)\u001b[0m\n\u001b[0;32m    318\u001b[0m                    \u001b[0mon_unused_input\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mon_unused_input\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    319\u001b[0m                    \u001b[0mprofile\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mprofile\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 320\u001b[1;33m                    output_keys=output_keys)\n\u001b[0m\u001b[0;32m    321\u001b[0m     \u001b[1;31m# We need to add the flag check_aliased inputs if we have any mutable or\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    322\u001b[0m     \u001b[1;31m# borrowed used defined inputs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/d/nolearn/local/lib/python2.7/site-packages/theano/compile/pfunc.pyc\u001b[0m in \u001b[0;36mpfunc\u001b[1;34m(params, outputs, mode, updates, givens, no_default_updates, accept_inplace, name, rebuild_strict, allow_input_downcast, profile, on_unused_input, output_keys)\u001b[0m\n\u001b[0;32m    477\u001b[0m                          \u001b[0maccept_inplace\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maccept_inplace\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    478\u001b[0m                          \u001b[0mprofile\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mprofile\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mon_unused_input\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mon_unused_input\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 479\u001b[1;33m                          output_keys=output_keys)\n\u001b[0m\u001b[0;32m    480\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    481\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/d/nolearn/local/lib/python2.7/site-packages/theano/compile/function_module.pyc\u001b[0m in \u001b[0;36morig_function\u001b[1;34m(inputs, outputs, mode, accept_inplace, name, profile, on_unused_input, output_keys)\u001b[0m\n\u001b[0;32m   1774\u001b[0m                    \u001b[0mprofile\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mprofile\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1775\u001b[0m                    \u001b[0mon_unused_input\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mon_unused_input\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1776\u001b[1;33m                    \u001b[0moutput_keys\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moutput_keys\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1777\u001b[0m             defaults)\n\u001b[0;32m   1778\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/d/nolearn/local/lib/python2.7/site-packages/theano/compile/function_module.pyc\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, inputs, outputs, mode, accept_inplace, function_builder, profile, on_unused_input, fgraph, output_keys)\u001b[0m\n\u001b[0;32m   1426\u001b[0m             \u001b[1;31m# OUTPUT VARIABLES)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1427\u001b[0m             fgraph, additional_outputs = std_fgraph(inputs, outputs,\n\u001b[1;32m-> 1428\u001b[1;33m                                                     accept_inplace)\n\u001b[0m\u001b[0;32m   1429\u001b[0m             \u001b[0mfgraph\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprofile\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprofile\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1430\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/d/nolearn/local/lib/python2.7/site-packages/theano/compile/function_module.pyc\u001b[0m in \u001b[0;36mstd_fgraph\u001b[1;34m(input_specs, output_specs, accept_inplace)\u001b[0m\n\u001b[0;32m    175\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    176\u001b[0m     fgraph = gof.fg.FunctionGraph(orig_inputs, orig_outputs,\n\u001b[1;32m--> 177\u001b[1;33m                                   update_mapping=update_mapping)\n\u001b[0m\u001b[0;32m    178\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    179\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mnode\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mfgraph\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_nodes\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/d/nolearn/local/lib/python2.7/site-packages/theano/gof/fg.pyc\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, inputs, outputs, features, clone, update_mapping)\u001b[0m\n\u001b[0;32m    169\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    170\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0moutput\u001b[0m \u001b[1;32min\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 171\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__import_r__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreason\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"init\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    172\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    173\u001b[0m             \u001b[0moutput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclients\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'output'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/d/nolearn/local/lib/python2.7/site-packages/theano/gof/fg.pyc\u001b[0m in \u001b[0;36m__import_r__\u001b[1;34m(self, variable, reason)\u001b[0m\n\u001b[0;32m    358\u001b[0m         \u001b[1;31m# Imports the owners of the variables\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    359\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mvariable\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mowner\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mvariable\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mowner\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_nodes\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 360\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__import__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvariable\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mowner\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreason\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mreason\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    361\u001b[0m         if (variable.owner is None and\n\u001b[0;32m    362\u001b[0m                 \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvariable\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgraph\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mConstant\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/d/nolearn/local/lib/python2.7/site-packages/theano/gof/fg.pyc\u001b[0m in \u001b[0;36m__import__\u001b[1;34m(self, apply_node, check, reason)\u001b[0m\n\u001b[0;32m    472\u001b[0m                             \u001b[1;34m\"for more information on this error.\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    473\u001b[0m                             % str(node)),\n\u001b[1;32m--> 474\u001b[1;33m                             r)\n\u001b[0m\u001b[0;32m    475\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    476\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mnode\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mnew_nodes\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMissingInputError\u001b[0m: (\"An input of the graph, used to compute Shape(targets), was not provided and not given a value.Use the Theano flag exception_verbosity='high',for more information on this error.\", targets)"
     ]
    }
   ],
   "source": [
    "target_var.shape.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
