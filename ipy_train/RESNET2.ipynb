{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using gpu device 0: GeForce GTX TITAN (CNMeM is disabled, CuDNN 4007)\n",
      "/home/d/nolearn/local/lib/python2.7/site-packages/theano/tensor/signal/downsample.py:6: UserWarning: downsample module has been moved to the theano.tensor.signal.pool module.\n",
      "  \"downsample module has been moved to the theano.tensor.signal.pool module.\")\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import time\n",
    "import string\n",
    "\n",
    "import numpy as np\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "\n",
    "import os; import sys; sys.path.append('..')\n",
    "import gp\n",
    "\n",
    "import gp.nets as nets\n",
    "import gp.nets.BatchNormLayer as BatchNormLayer\n",
    "\n",
    "import lasagne\n",
    "\n",
    "sys.setrecursionlimit(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# helper function for projection_b\n",
    "def ceildiv(a, b):\n",
    "    return -(-a // b)\n",
    "\n",
    "def build_cnn(input_var=None, n=1, num_filters=8, cudnn='no'):\n",
    "    import lasagne # For some odd reason it can't read the global import, please PR/Issue if you know why\n",
    "    projection_type = 'B'\n",
    "    # Setting up layers\n",
    "    if cudnn == 'yes':\n",
    "        import lasagne.layers.dnn\n",
    "        conv = lasagne.layers.dnn.Conv2DDNNLayer # cuDNN\n",
    "    else:\n",
    "        conv = lasagne.layers.Conv2DLayer\n",
    "    dropout = lasagne.layers.DropoutLayer\n",
    "    nonlin = lasagne.nonlinearities.rectify\n",
    "    nonlin_layer = lasagne.layers.NonlinearityLayer\n",
    "    sumlayer = lasagne.layers.ElemwiseSumLayer\n",
    "    #batchnorm = BatchNormLayer.BatchNormLayer\n",
    "    batchnorm = lasagne.layers.BatchNormLayer\n",
    "\n",
    "    # Setting the projection type for when reducing height/width\n",
    "    # and increasing dimensions.\n",
    "    # Default is 'B' as B performs slightly better\n",
    "    # and A requires newer version of lasagne with ExpressionLayer\n",
    "    projection_type = 'B'\n",
    "    if projection_type == 'A':\n",
    "        expression = lasagne.layers.ExpressionLayer\n",
    "        pad = lasagne.layers.PadLayer\n",
    "\n",
    "    if projection_type == 'A':\n",
    "        # option A for projection as described in paper\n",
    "        # (should perform slightly worse than B)\n",
    "        def projection(l_inp):\n",
    "            n_filters = l_inp.output_shape[1]*2\n",
    "            l = expression(l_inp, lambda X: X[:, :, ::2, ::2], lambda s: (s[0], s[1], ceildiv(s[2], 2), ceildiv(s[3], 2)))\n",
    "            l = pad(l, [n_filters//4,0,0], batch_ndim=1)\n",
    "            return l\n",
    "\n",
    "    if projection_type == 'B':\n",
    "        # option B for projection as described in paper\n",
    "        def projection(l_inp):\n",
    "            # twice normal channels when projecting!\n",
    "            n_filters = l_inp.output_shape[1]*2 \n",
    "            l = conv(l_inp, num_filters=n_filters, filter_size=(1, 1),\n",
    "                     stride=(2, 2), nonlinearity=None, pad='same', b=None)\n",
    "            l = batchnorm(l)\n",
    "            return l\n",
    "\n",
    "    # helper function to handle filters/strides when increasing dims\n",
    "    def filters_increase_dims(l, increase_dims):\n",
    "        in_num_filters = l.output_shape[1]\n",
    "        if increase_dims:\n",
    "            first_stride = (2, 2)\n",
    "            out_num_filters = in_num_filters*2\n",
    "        else:\n",
    "            first_stride = (1, 1)\n",
    "            out_num_filters = in_num_filters\n",
    " \n",
    "        return out_num_filters, first_stride\n",
    "\n",
    "    # block as described and used in cifar in the original paper:\n",
    "    # http://arxiv.org/abs/1512.03385\n",
    "    def res_block_v1(l_inp, nonlinearity=nonlin, increase_dim=False):\n",
    "        # first figure filters/strides\n",
    "        n_filters, first_stride = filters_increase_dims(l_inp, increase_dim)\n",
    "        # conv -> BN -> nonlin -> conv -> BN -> sum -> nonlin\n",
    "        l = conv(l_inp, num_filters=n_filters, filter_size=(3, 3),\n",
    "                 stride=first_stride, nonlinearity=None, pad='same',\n",
    "                 W=lasagne.init.HeNormal(gain='relu'))\n",
    "        l = batchnorm(l)\n",
    "\n",
    "        l = nonlin_layer(l, nonlinearity=nonlin)\n",
    "        l = dropout(l, p=.2)\n",
    "        print('adding dropout')        \n",
    "        \n",
    "        l = conv(l, num_filters=n_filters, filter_size=(3, 3),\n",
    "                 stride=(1, 1), nonlinearity=None, pad='same',\n",
    "                 W=lasagne.init.HeNormal(gain='relu'))\n",
    "        l = batchnorm(l)\n",
    "        if increase_dim:\n",
    "            # Use projection (A, B) as described in paper\n",
    "            p = projection(l_inp)\n",
    "        else:\n",
    "            # Identity shortcut\n",
    "            p = l_inp\n",
    "        l = sumlayer([l, p])\n",
    "        l = nonlin_layer(l, nonlinearity=nonlin)\n",
    "        return l\n",
    "\n",
    "    # block as described in second paper on the subject (by same authors):\n",
    "    # http://arxiv.org/abs/1603.05027\n",
    "    def res_block_v2(l_inp, nonlinearity=nonlin, increase_dim=False):\n",
    "        # first figure filters/strides\n",
    "        n_filters, first_stride = filters_increase_dims(l_inp, increase_dim)\n",
    "        # BN -> nonlin -> conv -> BN -> nonlin -> conv -> sum\n",
    "        l = batchnorm(l_inp)\n",
    "        l = nonlin_layer(l, nonlinearity=nonlin)\n",
    "        l = conv(l, num_filters=n_filters, filter_size=(3, 3),\n",
    "                 stride=first_stride, nonlinearity=None, pad='same',\n",
    "                 W=lasagne.init.HeNormal(gain='relu'))\n",
    "        l = batchnorm(l)\n",
    "        l = nonlin_layer(l, nonlinearity=nonlin)\n",
    "        l = conv(l, num_filters=n_filters, filter_size=(3, 3),\n",
    "                 stride=(1, 1), nonlinearity=None, pad='same',\n",
    "                 W=lasagne.init.HeNormal(gain='relu'))\n",
    "        if increase_dim:\n",
    "            # Use projection (A, B) as described in paper\n",
    "            p = projection(l_inp)\n",
    "        else:\n",
    "            # Identity shortcut\n",
    "            p = l_inp\n",
    "        l = sumlayer([l, p])\n",
    "        return l\n",
    "\n",
    "    def bottleneck_block(l_inp, nonlinearity=nonlin, increase_dim=False):\n",
    "        # first figure filters/strides\n",
    "        n_filters, first_stride = filters_increase_dims(l_inp, increase_dim)\n",
    "        # conv -> BN -> nonlin -> conv -> BN -> nonlin -> conv -> BN -> sum\n",
    "        # -> nonlin\n",
    "        # first make the bottleneck, scale the filters ..!\n",
    "        scale = 4 # as per bottleneck architecture used in paper\n",
    "        scaled_filters = n_filters/scale\n",
    "        l = conv(l_inp, num_filters=scaled_filters, filter_size=(1, 1),\n",
    "                 stride=first_stride, nonlinearity=None, pad='same',\n",
    "                 W=lasagne.init.HeNormal(gain='relu'))\n",
    "        l = batchnorm(l)\n",
    "        l = nonlin_layer(l, nonlinearity=nonlin)\n",
    "        l = conv(l, num_filters=scaled_filters, filter_size=(3, 3),\n",
    "                 stride=(1, 1), nonlinearity=None, pad='same',\n",
    "                 W=lasagne.init.HeNormal(gain='relu'))\n",
    "        l = batchnorm(l)\n",
    "        l = nonlin_layer(l, nonlinearity=nonlin)\n",
    "        l = conv(l, num_filters=n_filters, filter_size=(1, 1),\n",
    "                 stride=(1, 1), nonlinearity=None, pad='same',\n",
    "                 W=lasagne.init.HeNormal(gain='relu'))\n",
    "        if increase_dim:\n",
    "            # Use projection (A, B) as described in paper\n",
    "            p = projection(l_inp)\n",
    "        else:\n",
    "            # Identity shortcut\n",
    "            p = l_inp\n",
    "        l = sumlayer([l, p])\n",
    "        l = nonlin_layer(l, nonlinearity=nonlin)\n",
    "        return l\n",
    "\n",
    "    # Bottleneck architecture with more efficiency (the post with Kaiming He's response)\n",
    "    # https://www.reddit.com/r/MachineLearning/comments/3ywi6x/deep_residual_learning_the_bottleneck/ \n",
    "    def bottleneck_block_fast(l_inp, nonlinearity=nonlin, increase_dim=False):\n",
    "        # first figure filters/strides\n",
    "        n_filters, last_stride = filters_increase_dims(l_inp, increase_dim)\n",
    "        # conv -> BN -> nonlin -> conv -> BN -> nonlin -> conv -> BN -> sum\n",
    "        # -> nonlin\n",
    "        # first make the bottleneck, scale the filters ..!\n",
    "        scale = 4 # as per bottleneck architecture used in paper\n",
    "        scaled_filters = n_filters/scale\n",
    "        l = conv(l_inp, num_filters=scaled_filters, filter_size=(1, 1),\n",
    "                 stride=(1, 1), nonlinearity=None, pad='same',\n",
    "                 W=lasagne.init.HeNormal(gain='relu'))\n",
    "        l = batchnorm(l)\n",
    "        l = nonlin_layer(l, nonlinearity=nonlin)\n",
    "        l = conv(l, num_filters=scaled_filters, filter_size=(3, 3),\n",
    "                 stride=(1, 1), nonlinearity=None, pad='same',\n",
    "                 W=lasagne.init.HeNormal(gain='relu'))\n",
    "        l = batchnorm(l)\n",
    "        l = nonlin_layer(l, nonlinearity=nonlin)\n",
    "        l = conv(l, num_filters=n_filters, filter_size=(1, 1),\n",
    "                 stride=last_stride, nonlinearity=None, pad='same',\n",
    "                 W=lasagne.init.HeNormal(gain='relu'))\n",
    "        if increase_dim:\n",
    "            # Use projection (A, B) as described in paper\n",
    "            p = projection(l_inp)\n",
    "        else:\n",
    "            # Identity shortcut\n",
    "            p = l_inp\n",
    "        l = sumlayer([l, p])\n",
    "        l = nonlin_layer(l, nonlinearity=nonlin)\n",
    "        return l\n",
    "       \n",
    "    res_block = res_block_v1\n",
    "\n",
    "    # Stacks the residual blocks, makes it easy to model size of architecture with int n   \n",
    "    def blockstack(l, n, nonlinearity=nonlin):\n",
    "        print('NNN',n)\n",
    "        for _ in range(n):\n",
    "            print ('new')\n",
    "            l = res_block(l, nonlinearity=nonlin)\n",
    "        return l\n",
    "\n",
    "    # Building the network\n",
    "    l_in = lasagne.layers.InputLayer(shape=(None, 4, 75, 75),\n",
    "                                        input_var=input_var)\n",
    "    # First layer! just a plain convLayer\n",
    "    l1 = conv(l_in, num_filters=num_filters, stride=(1, 1),\n",
    "              filter_size=(3, 3), nonlinearity=None, pad='same')\n",
    "    l1 = batchnorm(l1)\n",
    "    l1 = nonlin_layer(l1, nonlinearity=nonlin)\n",
    "\n",
    "    # Stacking bottlenecks and increasing dims! (while reducing shape size)\n",
    "#     l1_bs = blockstack(l1, n=n)\n",
    "#     l1_id = res_block(l1_bs, increase_dim=True)\n",
    "\n",
    "#     l2_bs = blockstack(l1_id, n=n)\n",
    "#     l2_id = res_block(l2_bs, increase_dim=True)\n",
    "\n",
    "#     l3_bs = blockstack(l2_id, n=n)\n",
    "\n",
    "    l3_bs = blockstack(l1, n=n)\n",
    "\n",
    "    l3_do = dropout(l3_bs, p=.5)\n",
    "    \n",
    "    # And, finally, the 10-unit output layer:\n",
    "    network = lasagne.layers.DenseLayer(\n",
    "            l3_do,\n",
    "#             l1,\n",
    "            num_units=2,\n",
    "            nonlinearity=lasagne.nonlinearities.softmax)\n",
    "\n",
    "    return network\n",
    "\n",
    "\n",
    "# ############################# Batch iterator ###############################\n",
    "# This is just a simple helper function iterating over training data in\n",
    "# mini-batches of a particular size, optionally in random order. It assumes\n",
    "# data is available as numpy arrays. For big datasets, you could load numpy\n",
    "# arrays as memory-mapped files (np.load(..., mmap_mode='r')), or write your\n",
    "# own custom data iteration function. For small datasets, you can also copy\n",
    "# them to GPU at once for slightly improved performance. This would involve\n",
    "# several changes in the main program, though, and is not demonstrated here.\n",
    "\n",
    "def iterate_minibatches(inputs, targets, batchsize, shuffle=False):\n",
    "    assert len(inputs) == len(targets)\n",
    "    if shuffle:\n",
    "        indices = np.arange(len(inputs))\n",
    "        np.random.shuffle(indices)\n",
    "    for start_idx in range(0, len(inputs) - batchsize + 1, batchsize):\n",
    "        if shuffle:\n",
    "            excerpt = indices[start_idx:start_idx + batchsize]\n",
    "        else:\n",
    "            excerpt = slice(start_idx, start_idx + batchsize)\n",
    "            \n",
    "            Xb = inputs[excerpt]\n",
    "            yb = targets[excerpt]\n",
    "            \n",
    "            Xb = Xb - .5\n",
    "            \n",
    "            k_s = np.array([0,1,2,3],dtype=np.uint8)\n",
    "            for i in range(len(Xb)):\n",
    "                k = np.random.choice(k_s)\n",
    "                for j in range(Xb.shape[1]):\n",
    "                    Xb[j][0] = np.rot90(Xb[j][0], k)\n",
    "                    \n",
    "            yield Xb, yb\n",
    "            \n",
    "#         yield inputs[excerpt], targets[excerpt]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded /home/d/patches//ipmlb/ in 0.158769130707 seconds.\n"
     ]
    }
   ],
   "source": [
    "PATCH_PATH = ('ipmlb')\n",
    "X_train, y_train, X_test, y_test = gp.Patch.load_rgba(PATCH_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_val = X_train[-X_train.shape[0]/4:]\n",
    "y_val = y_train[-X_train.shape[0]/4:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train2 = X_train[:-X_train.shape[0]/4]\n",
    "y_train2 = y_train[:-X_train.shape[0]/4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "n=2\n",
    "num_filters=64\n",
    "num_epochs=200\n",
    "cudnn='yes'\n",
    "print(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building model and compiling functions...\n",
      "NNN 2\n",
      "new\n",
      "adding dropout\n",
      "new\n",
      "adding dropout\n",
      "  layer output shapes:\n",
      "    InputLayer                       (None, 4, 75, 75)\n",
      "    Conv2DDNNLayer                   (None, 64, 75, 75)\n",
      "    BatchNormLayer                   (None, 64, 75, 75)\n",
      "    NonlinearityLayer                (None, 64, 75, 75)\n",
      "    Conv2DDNNLayer                   (None, 64, 75, 75)\n",
      "    BatchNormLayer                   (None, 64, 75, 75)\n",
      "    NonlinearityLayer                (None, 64, 75, 75)\n",
      "    DropoutLayer                     (None, 64, 75, 75)\n",
      "    Conv2DDNNLayer                   (None, 64, 75, 75)\n",
      "    BatchNormLayer                   (None, 64, 75, 75)\n",
      "    ElemwiseSumLayer                 (None, 64, 75, 75)\n",
      "    NonlinearityLayer                (None, 64, 75, 75)\n",
      "    Conv2DDNNLayer                   (None, 64, 75, 75)\n",
      "    BatchNormLayer                   (None, 64, 75, 75)\n",
      "    NonlinearityLayer                (None, 64, 75, 75)\n",
      "    DropoutLayer                     (None, 64, 75, 75)\n",
      "    Conv2DDNNLayer                   (None, 64, 75, 75)\n",
      "    BatchNormLayer                   (None, 64, 75, 75)\n",
      "    ElemwiseSumLayer                 (None, 64, 75, 75)\n",
      "    NonlinearityLayer                (None, 64, 75, 75)\n",
      "    DropoutLayer                     (None, 64, 75, 75)\n",
      "    DenseLayer                       (None, 2)\n",
      "  no. of InputLayers: 1\n",
      "  no. of Conv2DLayers: 5\n",
      "  no. of BatchNormLayers: 5\n",
      "  no. of NonlinearityLayers: 5\n",
      "  no. of DenseLayers: 1\n",
      "  no. of ElemwiseSumLayers: 2\n",
      "  no. of Unknown Layers: 3\n",
      "  total no. of layers: 22\n",
      "  no. of parameters: 871362\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Prepare Theano variables for inputs and targets\n",
    "input_var = T.tensor4('inputs')\n",
    "target_var = T.ivector('targets')\n",
    "\n",
    "# Create neural network model (depending on first command line parameter)\n",
    "print(\"Building model and compiling functions...\")\n",
    "network = build_cnn(input_var, n, num_filters, cudnn)\n",
    "all_layers = lasagne.layers.get_all_layers(network)\n",
    "num_params = lasagne.layers.count_params(network)\n",
    "num_conv = 0\n",
    "num_nonlin = 0\n",
    "num_input = 0\n",
    "num_batchnorm = 0\n",
    "num_elemsum = 0\n",
    "num_dense = 0\n",
    "num_unknown = 0\n",
    "print(\"  layer output shapes:\")\n",
    "for layer in all_layers:\n",
    "    name = string.ljust(layer.__class__.__name__, 32)\n",
    "    print(\"    %s %s\" %(name, lasagne.layers.get_output_shape(layer)))\n",
    "    if \"Conv2D\" in name:\n",
    "        num_conv += 1\n",
    "    elif \"NonlinearityLayer\" in name:\n",
    "        num_nonlin += 1\n",
    "    elif \"InputLayer\" in name:\n",
    "        num_input += 1\n",
    "    elif \"BatchNormLayer\" in name:\n",
    "        num_batchnorm += 1\n",
    "    elif \"ElemwiseSumLayer\" in name:\n",
    "        num_elemsum += 1\n",
    "    elif \"DenseLayer\" in name:\n",
    "        num_dense += 1\n",
    "    else:\n",
    "        num_unknown += 1\n",
    "print(\"  no. of InputLayers: %d\" % num_input)\n",
    "print(\"  no. of Conv2DLayers: %d\" % num_conv)\n",
    "print(\"  no. of BatchNormLayers: %d\" % num_batchnorm)\n",
    "print(\"  no. of NonlinearityLayers: %d\" % num_nonlin)\n",
    "print(\"  no. of DenseLayers: %d\" % num_dense)\n",
    "print(\"  no. of ElemwiseSumLayers: %d\" % num_elemsum)\n",
    "print(\"  no. of Unknown Layers: %d\" % num_unknown)\n",
    "print(\"  total no. of layers: %d\" % len(all_layers))\n",
    "print(\"  no. of parameters: %d\" % num_params)\n",
    "# Create a loss expression for training, i.e., a scalar objective we want\n",
    "# to minimize (for our multi-class problem, it is the cross-entropy loss):\n",
    "prediction = lasagne.layers.get_output(network)\n",
    "loss = lasagne.objectives.categorical_crossentropy(prediction, target_var)\n",
    "loss = loss.mean()\n",
    "# We could add some weight decay as well here, see lasagne.regularization.\n",
    "\n",
    "# Create update expressions for training, i.e., how to modify the\n",
    "# parameters at each training step. Here, we'll use Stochastic Gradient\n",
    "# Descent (SGD) with Nesterov momentum, but Lasagne offers plenty more.\n",
    "params = lasagne.layers.get_all_params(network, trainable=True)\n",
    "\n",
    "# several learning rates for low initial learning rates and\n",
    "# learning rate anealing (id is epoch)\n",
    "learning_rate_schedule = {\n",
    "0: 0.0001, # low initial learning rate as described in paper\n",
    "2: 0.01,\n",
    "100: 0.001,\n",
    "150: 0.0001\n",
    "}\n",
    "\n",
    "\n",
    "learning_rate = theano.shared(np.float32(learning_rate_schedule[0]))\n",
    "\n",
    "updates = lasagne.updates.nesterov_momentum(\n",
    "        loss, params, learning_rate=learning_rate, momentum=0.9)\n",
    "\n",
    "# Create a loss expression for validation/testing. The crucial difference\n",
    "# here is that we do a deterministic forward pass through the network,\n",
    "# disabling dropout layers.\n",
    "test_prediction = lasagne.layers.get_output(network, deterministic=True)\n",
    "test_loss = lasagne.objectives.categorical_crossentropy(test_prediction,\n",
    "                                                        target_var)\n",
    "test_loss = test_loss.mean()\n",
    "# As a bonus, also create an expression for the classification accuracy:\n",
    "test_acc = T.mean(T.eq(T.argmax(test_prediction, axis=1), target_var),\n",
    "                  dtype=theano.config.floatX)\n",
    "\n",
    "# Compile a function performing a training step on a mini-batch (by giving\n",
    "# the updates dictionary) and returning the corresponding training loss:\n",
    "train_fn = theano.function([input_var, target_var], loss, updates=updates)\n",
    "\n",
    "# Compile a second function computing the validation loss and accuracy:\n",
    "val_fn = theano.function([input_var, target_var], [test_loss, test_acc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n",
      " setting learning rate to 0.0001000\n",
      "Epoch 1 of 200 took 1498.791s\n",
      "  training loss:\t\t0.961825\n",
      "  validation loss:\t\t1.122976\n",
      "  validation accuracy:\t\t81.27 %\n",
      "Epoch 2 of 200 took 1526.950s\n",
      "  training loss:\t\t0.565774\n",
      "  validation loss:\t\t0.449433\n",
      "  validation accuracy:\t\t91.16 %\n",
      " setting learning rate to 0.0100000\n",
      "Epoch 3 of 200 took 1507.428s\n",
      "  training loss:\t\t2.093450\n",
      "  validation loss:\t\t0.285670\n",
      "  validation accuracy:\t\t88.33 %\n",
      "Epoch 4 of 200 took 1507.298s\n",
      "  training loss:\t\t0.279917\n",
      "  validation loss:\t\t0.246781\n",
      "  validation accuracy:\t\t90.09 %\n",
      "Epoch 5 of 200 took 1508.682s\n",
      "  training loss:\t\t0.258657\n",
      "  validation loss:\t\t0.239607\n",
      "  validation accuracy:\t\t90.47 %\n",
      "Epoch 6 of 200 took 1506.945s\n",
      "  training loss:\t\t0.249869\n",
      "  validation loss:\t\t0.236154\n",
      "  validation accuracy:\t\t90.66 %\n",
      "Epoch 7 of 200 took 1505.387s\n",
      "  training loss:\t\t0.244102\n",
      "  validation loss:\t\t0.230588\n",
      "  validation accuracy:\t\t90.96 %\n",
      "Epoch 8 of 200 took 1514.948s\n",
      "  training loss:\t\t0.240329\n",
      "  validation loss:\t\t0.226531\n",
      "  validation accuracy:\t\t91.20 %\n",
      "Epoch 9 of 200 took 1512.738s\n",
      "  training loss:\t\t0.235772\n",
      "  validation loss:\t\t0.233000\n",
      "  validation accuracy:\t\t90.97 %\n",
      "Epoch 10 of 200 took 1509.199s\n",
      "  training loss:\t\t0.233176\n",
      "  validation loss:\t\t0.233735\n",
      "  validation accuracy:\t\t90.75 %\n",
      "Epoch 11 of 200 took 1525.770s\n",
      "  training loss:\t\t0.230647\n",
      "  validation loss:\t\t0.233434\n",
      "  validation accuracy:\t\t90.89 %\n",
      "Epoch 12 of 200 took 1519.916s\n",
      "  training loss:\t\t0.228786\n",
      "  validation loss:\t\t0.220758\n",
      "  validation accuracy:\t\t91.37 %\n",
      "Epoch 13 of 200 took 1507.771s\n",
      "  training loss:\t\t0.225797\n",
      "  validation loss:\t\t0.224929\n",
      "  validation accuracy:\t\t91.24 %\n",
      "Epoch 14 of 200 took 1514.398s\n",
      "  training loss:\t\t0.225282\n",
      "  validation loss:\t\t0.218318\n",
      "  validation accuracy:\t\t91.57 %\n",
      "Epoch 15 of 200 took 1508.658s\n",
      "  training loss:\t\t0.223266\n",
      "  validation loss:\t\t0.229018\n",
      "  validation accuracy:\t\t91.00 %\n",
      "Epoch 16 of 200 took 1506.144s\n",
      "  training loss:\t\t0.221863\n",
      "  validation loss:\t\t0.219444\n",
      "  validation accuracy:\t\t91.54 %\n",
      "Epoch 17 of 200 took 1506.564s\n",
      "  training loss:\t\t0.220154\n",
      "  validation loss:\t\t0.220299\n",
      "  validation accuracy:\t\t91.55 %\n",
      "Epoch 18 of 200 took 1507.678s\n",
      "  training loss:\t\t0.218990\n",
      "  validation loss:\t\t0.231785\n",
      "  validation accuracy:\t\t90.88 %\n",
      "Epoch 19 of 200 took 1524.830s\n",
      "  training loss:\t\t0.217147\n",
      "  validation loss:\t\t0.216564\n",
      "  validation accuracy:\t\t91.61 %\n",
      "Epoch 20 of 200 took 1524.712s\n",
      "  training loss:\t\t0.216373\n",
      "  validation loss:\t\t0.206964\n",
      "  validation accuracy:\t\t92.10 %\n",
      "Epoch 21 of 200 took 1508.225s\n",
      "  training loss:\t\t0.214247\n",
      "  validation loss:\t\t0.211649\n",
      "  validation accuracy:\t\t91.76 %\n",
      "Epoch 22 of 200 took 1506.380s\n",
      "  training loss:\t\t0.213710\n",
      "  validation loss:\t\t0.220855\n",
      "  validation accuracy:\t\t91.37 %\n",
      "Epoch 23 of 200 took 1506.004s\n",
      "  training loss:\t\t0.212522\n",
      "  validation loss:\t\t0.210128\n",
      "  validation accuracy:\t\t91.88 %\n",
      "Epoch 24 of 200 took 1505.129s\n",
      "  training loss:\t\t0.211067\n",
      "  validation loss:\t\t0.220119\n",
      "  validation accuracy:\t\t91.48 %\n",
      "Epoch 25 of 200 took 1506.667s\n",
      "  training loss:\t\t0.210480\n",
      "  validation loss:\t\t0.210966\n",
      "  validation accuracy:\t\t91.98 %\n",
      "Epoch 26 of 200 took 1507.640s\n",
      "  training loss:\t\t0.209499\n",
      "  validation loss:\t\t0.215005\n",
      "  validation accuracy:\t\t91.77 %\n",
      "Epoch 27 of 200 took 1506.793s\n",
      "  training loss:\t\t0.209093\n",
      "  validation loss:\t\t0.204795\n",
      "  validation accuracy:\t\t92.18 %\n",
      "Epoch 28 of 200 took 1508.622s\n",
      "  training loss:\t\t0.208699\n",
      "  validation loss:\t\t0.202529\n",
      "  validation accuracy:\t\t92.38 %\n",
      "Epoch 29 of 200 took 1507.176s\n",
      "  training loss:\t\t0.206759\n",
      "  validation loss:\t\t0.200795\n",
      "  validation accuracy:\t\t92.42 %\n",
      "Epoch 30 of 200 took 1507.696s\n",
      "  training loss:\t\t0.206742\n",
      "  validation loss:\t\t0.195887\n",
      "  validation accuracy:\t\t92.70 %\n",
      "Epoch 31 of 200 took 1508.094s\n",
      "  training loss:\t\t0.206067\n",
      "  validation loss:\t\t0.201008\n",
      "  validation accuracy:\t\t92.41 %\n",
      "Epoch 32 of 200 took 1506.025s\n",
      "  training loss:\t\t0.204613\n",
      "  validation loss:\t\t0.203142\n",
      "  validation accuracy:\t\t92.39 %\n",
      "Epoch 33 of 200 took 1507.371s\n",
      "  training loss:\t\t0.204323\n",
      "  validation loss:\t\t0.213597\n",
      "  validation accuracy:\t\t92.00 %\n",
      "Epoch 34 of 200 took 1511.065s\n",
      "  training loss:\t\t0.203174\n",
      "  validation loss:\t\t0.202487\n",
      "  validation accuracy:\t\t92.43 %\n",
      "Epoch 35 of 200 took 1504.781s\n",
      "  training loss:\t\t0.203144\n",
      "  validation loss:\t\t0.200675\n",
      "  validation accuracy:\t\t92.50 %\n",
      "Epoch 36 of 200 took 1507.535s\n",
      "  training loss:\t\t0.202166\n",
      "  validation loss:\t\t0.201433\n",
      "  validation accuracy:\t\t92.50 %\n",
      "Epoch 37 of 200 took 1506.208s\n",
      "  training loss:\t\t0.202303\n",
      "  validation loss:\t\t0.196942\n",
      "  validation accuracy:\t\t92.62 %\n",
      "Epoch 38 of 200 took 1509.300s\n",
      "  training loss:\t\t0.199947\n",
      "  validation loss:\t\t0.195728\n",
      "  validation accuracy:\t\t92.75 %\n",
      "Epoch 39 of 200 took 1505.912s\n",
      "  training loss:\t\t0.200251\n",
      "  validation loss:\t\t0.199178\n",
      "  validation accuracy:\t\t92.58 %\n",
      "Epoch 40 of 200 took 1507.859s\n",
      "  training loss:\t\t0.199014\n",
      "  validation loss:\t\t0.197294\n",
      "  validation accuracy:\t\t92.62 %\n",
      "Epoch 41 of 200 took 1506.292s\n",
      "  training loss:\t\t0.199221\n",
      "  validation loss:\t\t0.197847\n",
      "  validation accuracy:\t\t92.58 %\n",
      "Epoch 42 of 200 took 1509.431s\n",
      "  training loss:\t\t0.197861\n",
      "  validation loss:\t\t0.191727\n",
      "  validation accuracy:\t\t93.00 %\n",
      "Epoch 43 of 200 took 1506.933s\n",
      "  training loss:\t\t0.197231\n",
      "  validation loss:\t\t0.193900\n",
      "  validation accuracy:\t\t92.74 %\n",
      "Epoch 44 of 200 took 1506.082s\n",
      "  training loss:\t\t0.196165\n",
      "  validation loss:\t\t0.191801\n",
      "  validation accuracy:\t\t92.89 %\n",
      "Epoch 45 of 200 took 1506.885s\n",
      "  training loss:\t\t0.195288\n",
      "  validation loss:\t\t0.193221\n",
      "  validation accuracy:\t\t92.78 %\n",
      "Epoch 46 of 200 took 1507.105s\n",
      "  training loss:\t\t0.195615\n",
      "  validation loss:\t\t0.190010\n",
      "  validation accuracy:\t\t93.03 %\n",
      "Epoch 47 of 200 took 1506.028s\n",
      "  training loss:\t\t0.194663\n",
      "  validation loss:\t\t0.189887\n",
      "  validation accuracy:\t\t92.98 %\n",
      "Epoch 48 of 200 took 1506.123s\n",
      "  training loss:\t\t0.193870\n",
      "  validation loss:\t\t0.187602\n",
      "  validation accuracy:\t\t92.98 %\n",
      "Epoch 49 of 200 took 1506.759s\n",
      "  training loss:\t\t0.193997\n",
      "  validation loss:\t\t0.193663\n",
      "  validation accuracy:\t\t92.74 %\n",
      "Epoch 50 of 200 took 1506.326s\n",
      "  training loss:\t\t0.193358\n",
      "  validation loss:\t\t0.191388\n",
      "  validation accuracy:\t\t92.92 %\n",
      "Epoch 51 of 200 took 1507.533s\n",
      "  training loss:\t\t0.191639\n",
      "  validation loss:\t\t0.188968\n",
      "  validation accuracy:\t\t93.05 %\n",
      "Epoch 52 of 200 took 1508.796s\n",
      "  training loss:\t\t0.191222\n",
      "  validation loss:\t\t0.190042\n",
      "  validation accuracy:\t\t92.99 %\n",
      "Epoch 53 of 200 took 1507.220s\n",
      "  training loss:\t\t0.191666\n",
      "  validation loss:\t\t0.185993\n",
      "  validation accuracy:\t\t93.19 %\n",
      "Epoch 54 of 200 took 1508.458s\n",
      "  training loss:\t\t0.190329\n",
      "  validation loss:\t\t0.187130\n",
      "  validation accuracy:\t\t93.14 %\n",
      "Epoch 55 of 200 took 1506.595s\n",
      "  training loss:\t\t0.189661\n",
      "  validation loss:\t\t0.186467\n",
      "  validation accuracy:\t\t93.20 %\n",
      "Epoch 56 of 200 took 1508.233s\n",
      "  training loss:\t\t0.189474\n",
      "  validation loss:\t\t0.196114\n",
      "  validation accuracy:\t\t92.72 %\n",
      "Epoch 57 of 200 took 1526.420s\n",
      "  training loss:\t\t0.189094\n",
      "  validation loss:\t\t0.190709\n",
      "  validation accuracy:\t\t92.91 %\n",
      "Epoch 58 of 200 took 1526.677s\n",
      "  training loss:\t\t0.188683\n",
      "  validation loss:\t\t0.189177\n",
      "  validation accuracy:\t\t92.99 %\n",
      "Epoch 59 of 200 took 1525.483s\n",
      "  training loss:\t\t0.187996\n",
      "  validation loss:\t\t0.195060\n",
      "  validation accuracy:\t\t92.73 %\n",
      "Epoch 60 of 200 took 1526.433s\n",
      "  training loss:\t\t0.187774\n",
      "  validation loss:\t\t0.185760\n",
      "  validation accuracy:\t\t93.20 %\n",
      "Epoch 61 of 200 took 1527.648s\n",
      "  training loss:\t\t0.187436\n",
      "  validation loss:\t\t0.183836\n",
      "  validation accuracy:\t\t93.32 %\n",
      "Epoch 62 of 200 took 1508.554s\n",
      "  training loss:\t\t0.187105\n",
      "  validation loss:\t\t0.185145\n",
      "  validation accuracy:\t\t93.14 %\n",
      "Epoch 63 of 200 took 1525.053s\n",
      "  training loss:\t\t0.186091\n",
      "  validation loss:\t\t0.184884\n",
      "  validation accuracy:\t\t93.26 %\n",
      "Epoch 64 of 200 took 1515.204s\n",
      "  training loss:\t\t0.186262\n",
      "  validation loss:\t\t0.184188\n",
      "  validation accuracy:\t\t93.23 %\n",
      "Epoch 65 of 200 took 1525.087s\n",
      "  training loss:\t\t0.184831\n",
      "  validation loss:\t\t0.187452\n",
      "  validation accuracy:\t\t93.12 %\n",
      "Epoch 66 of 200 took 1524.674s\n",
      "  training loss:\t\t0.184642\n",
      "  validation loss:\t\t0.181639\n",
      "  validation accuracy:\t\t93.39 %\n",
      "Epoch 67 of 200 took 1507.838s\n",
      "  training loss:\t\t0.183867\n",
      "  validation loss:\t\t0.182476\n",
      "  validation accuracy:\t\t93.40 %\n",
      "Epoch 68 of 200 took 1514.739s\n",
      "  training loss:\t\t0.184669\n",
      "  validation loss:\t\t0.180992\n",
      "  validation accuracy:\t\t93.38 %\n",
      "Epoch 69 of 200 took 1506.332s\n",
      "  training loss:\t\t0.183107\n",
      "  validation loss:\t\t0.182231\n",
      "  validation accuracy:\t\t93.34 %\n",
      "Epoch 70 of 200 took 1507.727s\n",
      "  training loss:\t\t0.183506\n",
      "  validation loss:\t\t0.188703\n",
      "  validation accuracy:\t\t93.00 %\n",
      "Epoch 71 of 200 took 1506.314s\n",
      "  training loss:\t\t0.182828\n",
      "  validation loss:\t\t0.180383\n",
      "  validation accuracy:\t\t93.46 %\n",
      "Epoch 72 of 200 took 1507.335s\n",
      "  training loss:\t\t0.182315\n",
      "  validation loss:\t\t0.179047\n",
      "  validation accuracy:\t\t93.54 %\n",
      "Epoch 73 of 200 took 1506.141s\n",
      "  training loss:\t\t0.182838\n",
      "  validation loss:\t\t0.179023\n",
      "  validation accuracy:\t\t93.54 %\n",
      "Epoch 74 of 200 took 1506.300s\n",
      "  training loss:\t\t0.180977\n",
      "  validation loss:\t\t0.187420\n",
      "  validation accuracy:\t\t93.09 %\n",
      "Epoch 75 of 200 took 1508.062s\n",
      "  training loss:\t\t0.180745\n",
      "  validation loss:\t\t0.180782\n",
      "  validation accuracy:\t\t93.35 %\n",
      "Epoch 76 of 200 took 1506.297s\n",
      "  training loss:\t\t0.180909\n",
      "  validation loss:\t\t0.186900\n",
      "  validation accuracy:\t\t93.17 %\n",
      "Epoch 77 of 200 took 1507.725s\n",
      "  training loss:\t\t0.181663\n",
      "  validation loss:\t\t0.177964\n",
      "  validation accuracy:\t\t93.55 %\n",
      "Epoch 78 of 200 took 1506.682s\n",
      "  training loss:\t\t0.180890\n",
      "  validation loss:\t\t0.179105\n",
      "  validation accuracy:\t\t93.49 %\n",
      "Epoch 79 of 200 took 1507.728s\n",
      "  training loss:\t\t0.181077\n",
      "  validation loss:\t\t0.181460\n",
      "  validation accuracy:\t\t93.40 %\n",
      "Epoch 80 of 200 took 1506.715s\n",
      "  training loss:\t\t0.179975\n",
      "  validation loss:\t\t0.180767\n",
      "  validation accuracy:\t\t93.39 %\n",
      "Epoch 81 of 200 took 1507.198s\n",
      "  training loss:\t\t0.179547\n",
      "  validation loss:\t\t0.186197\n",
      "  validation accuracy:\t\t93.15 %\n",
      "Epoch 82 of 200 took 1506.206s\n",
      "  training loss:\t\t0.178959\n",
      "  validation loss:\t\t0.180894\n",
      "  validation accuracy:\t\t93.44 %\n",
      "Epoch 83 of 200 took 1506.325s\n",
      "  training loss:\t\t0.178613\n",
      "  validation loss:\t\t0.177134\n",
      "  validation accuracy:\t\t93.57 %\n",
      "Epoch 84 of 200 took 1506.769s\n",
      "  training loss:\t\t0.180030\n",
      "  validation loss:\t\t0.177498\n",
      "  validation accuracy:\t\t93.52 %\n",
      "Epoch 85 of 200 took 1506.449s\n",
      "  training loss:\t\t0.177646\n",
      "  validation loss:\t\t0.176252\n",
      "  validation accuracy:\t\t93.69 %\n",
      "Epoch 86 of 200 took 1507.324s\n",
      "  training loss:\t\t0.178076\n",
      "  validation loss:\t\t0.175614\n",
      "  validation accuracy:\t\t93.55 %\n",
      "Epoch 87 of 200 took 1505.907s\n",
      "  training loss:\t\t0.177494\n",
      "  validation loss:\t\t0.179977\n",
      "  validation accuracy:\t\t93.41 %\n",
      "Epoch 88 of 200 took 1508.397s\n",
      "  training loss:\t\t0.177656\n",
      "  validation loss:\t\t0.175484\n",
      "  validation accuracy:\t\t93.65 %\n",
      "Epoch 89 of 200 took 1507.318s\n",
      "  training loss:\t\t0.176916\n",
      "  validation loss:\t\t0.174290\n",
      "  validation accuracy:\t\t93.69 %\n",
      "Epoch 90 of 200 took 1493.444s\n",
      "  training loss:\t\t0.176263\n",
      "  validation loss:\t\t0.174722\n",
      "  validation accuracy:\t\t93.55 %\n",
      "Epoch 91 of 200 took 1489.073s\n",
      "  training loss:\t\t0.176091\n",
      "  validation loss:\t\t0.180315\n",
      "  validation accuracy:\t\t93.43 %\n",
      "Epoch 92 of 200 took 1507.091s\n",
      "  training loss:\t\t0.176049\n",
      "  validation loss:\t\t0.176713\n",
      "  validation accuracy:\t\t93.56 %\n",
      "Epoch 93 of 200 took 1505.784s\n",
      "  training loss:\t\t0.175398\n",
      "  validation loss:\t\t0.173243\n",
      "  validation accuracy:\t\t93.70 %\n",
      "Epoch 94 of 200 took 1507.187s\n",
      "  training loss:\t\t0.175623\n",
      "  validation loss:\t\t0.174340\n",
      "  validation accuracy:\t\t93.64 %\n",
      "Epoch 95 of 200 took 1488.744s\n",
      "  training loss:\t\t0.175410\n",
      "  validation loss:\t\t0.176620\n",
      "  validation accuracy:\t\t93.52 %\n",
      "Epoch 96 of 200 took 1506.147s\n",
      "  training loss:\t\t0.174806\n",
      "  validation loss:\t\t0.174663\n",
      "  validation accuracy:\t\t93.68 %\n",
      "Epoch 97 of 200 took 1505.885s\n",
      "  training loss:\t\t0.174098\n",
      "  validation loss:\t\t0.178406\n",
      "  validation accuracy:\t\t93.53 %\n",
      "Epoch 98 of 200 took 1505.953s\n",
      "  training loss:\t\t0.175327\n",
      "  validation loss:\t\t0.173387\n",
      "  validation accuracy:\t\t93.72 %\n",
      "Epoch 99 of 200 took 1505.733s\n",
      "  training loss:\t\t0.173164\n",
      "  validation loss:\t\t0.171627\n",
      "  validation accuracy:\t\t93.75 %\n",
      "Epoch 100 of 200 took 1505.842s\n",
      "  training loss:\t\t0.173866\n",
      "  validation loss:\t\t0.171117\n",
      "  validation accuracy:\t\t93.77 %\n",
      " setting learning rate to 0.0010000\n",
      "Epoch 101 of 200 took 1505.975s\n",
      "  training loss:\t\t0.167471\n",
      "  validation loss:\t\t0.166847\n",
      "  validation accuracy:\t\t93.88 %\n",
      "Epoch 102 of 200 took 1489.218s\n",
      "  training loss:\t\t0.166225\n",
      "  validation loss:\t\t0.166568\n",
      "  validation accuracy:\t\t93.94 %\n",
      "Epoch 103 of 200 took 1505.587s\n",
      "  training loss:\t\t0.165337\n",
      "  validation loss:\t\t0.165861\n",
      "  validation accuracy:\t\t93.95 %\n",
      "Epoch 104 of 200 took 1489.067s\n",
      "  training loss:\t\t0.164491\n",
      "  validation loss:\t\t0.166104\n",
      "  validation accuracy:\t\t93.95 %\n",
      "Epoch 105 of 200 took 1505.877s\n",
      "  training loss:\t\t0.165047\n",
      "  validation loss:\t\t0.165913\n",
      "  validation accuracy:\t\t94.00 %\n",
      "Epoch 106 of 200 took 1505.578s\n",
      "  training loss:\t\t0.163840\n",
      "  validation loss:\t\t0.165245\n",
      "  validation accuracy:\t\t94.03 %\n",
      "Epoch 107 of 200 took 1505.775s\n",
      "  training loss:\t\t0.164777\n",
      "  validation loss:\t\t0.165183\n",
      "  validation accuracy:\t\t94.06 %\n",
      "Epoch 108 of 200 took 1505.943s\n",
      "  training loss:\t\t0.165504\n",
      "  validation loss:\t\t0.165548\n",
      "  validation accuracy:\t\t94.02 %\n",
      "Epoch 109 of 200 took 1505.757s\n",
      "  training loss:\t\t0.165023\n",
      "  validation loss:\t\t0.165466\n",
      "  validation accuracy:\t\t94.01 %\n",
      "Epoch 110 of 200 took 1505.734s\n",
      "  training loss:\t\t0.163559\n",
      "  validation loss:\t\t0.165148\n",
      "  validation accuracy:\t\t94.01 %\n",
      "Epoch 111 of 200 took 1488.867s\n",
      "  training loss:\t\t0.163689\n",
      "  validation loss:\t\t0.165320\n",
      "  validation accuracy:\t\t94.00 %\n",
      "Epoch 112 of 200 took 1505.862s\n",
      "  training loss:\t\t0.163960\n",
      "  validation loss:\t\t0.165359\n",
      "  validation accuracy:\t\t94.03 %\n",
      "Epoch 113 of 200 took 1506.294s\n",
      "  training loss:\t\t0.163700\n",
      "  validation loss:\t\t0.165325\n",
      "  validation accuracy:\t\t94.06 %\n",
      "Epoch 114 of 200 took 1525.189s\n",
      "  training loss:\t\t0.163272\n",
      "  validation loss:\t\t0.165027\n",
      "  validation accuracy:\t\t94.00 %\n",
      "Epoch 115 of 200 took 1525.272s\n",
      "  training loss:\t\t0.163226\n",
      "  validation loss:\t\t0.164651\n",
      "  validation accuracy:\t\t94.05 %\n",
      "Epoch 116 of 200 took 1527.645s\n",
      "  training loss:\t\t0.163414\n",
      "  validation loss:\t\t0.164840\n",
      "  validation accuracy:\t\t94.05 %\n",
      "Epoch 117 of 200 took 1526.174s\n",
      "  training loss:\t\t0.163422\n",
      "  validation loss:\t\t0.164773\n",
      "  validation accuracy:\t\t94.05 %\n",
      "Epoch 118 of 200 took 1526.658s\n",
      "  training loss:\t\t0.163548\n",
      "  validation loss:\t\t0.164624\n",
      "  validation accuracy:\t\t94.07 %\n",
      "Epoch 119 of 200 took 1525.606s\n",
      "  training loss:\t\t0.162888\n",
      "  validation loss:\t\t0.164558\n",
      "  validation accuracy:\t\t94.09 %\n",
      "Epoch 120 of 200 took 1524.919s\n",
      "  training loss:\t\t0.163020\n",
      "  validation loss:\t\t0.164956\n",
      "  validation accuracy:\t\t94.07 %\n",
      "Epoch 121 of 200 took 1524.976s\n",
      "  training loss:\t\t0.162398\n",
      "  validation loss:\t\t0.164187\n",
      "  validation accuracy:\t\t94.09 %\n",
      "Epoch 122 of 200 took 1522.264s\n",
      "  training loss:\t\t0.163696\n",
      "  validation loss:\t\t0.164820\n",
      "  validation accuracy:\t\t94.02 %\n",
      "Epoch 123 of 200 took 1506.706s\n",
      "  training loss:\t\t0.163432\n",
      "  validation loss:\t\t0.164099\n",
      "  validation accuracy:\t\t94.08 %\n",
      "Epoch 124 of 200 took 1507.573s\n",
      "  training loss:\t\t0.162759\n",
      "  validation loss:\t\t0.164341\n",
      "  validation accuracy:\t\t94.12 %\n",
      "Epoch 125 of 200 took 1506.548s\n",
      "  training loss:\t\t0.162041\n",
      "  validation loss:\t\t0.163932\n",
      "  validation accuracy:\t\t94.12 %\n",
      "Epoch 126 of 200 took 1508.645s\n",
      "  training loss:\t\t0.163024\n",
      "  validation loss:\t\t0.163923\n",
      "  validation accuracy:\t\t94.09 %\n",
      "Epoch 127 of 200 took 1525.027s\n",
      "  training loss:\t\t0.163119\n",
      "  validation loss:\t\t0.164108\n",
      "  validation accuracy:\t\t94.12 %\n",
      "Epoch 128 of 200 took 1508.493s\n",
      "  training loss:\t\t0.162634\n",
      "  validation loss:\t\t0.163695\n",
      "  validation accuracy:\t\t94.11 %\n",
      "Epoch 129 of 200 took 1523.695s\n",
      "  training loss:\t\t0.162532\n",
      "  validation loss:\t\t0.164195\n",
      "  validation accuracy:\t\t94.08 %\n",
      "Epoch 130 of 200 took 1524.991s\n",
      "  training loss:\t\t0.162417\n",
      "  validation loss:\t\t0.163729\n",
      "  validation accuracy:\t\t94.09 %\n",
      "Epoch 131 of 200 took 1511.146s\n",
      "  training loss:\t\t0.161889\n",
      "  validation loss:\t\t0.164092\n",
      "  validation accuracy:\t\t94.10 %\n",
      "Epoch 132 of 200 took 1506.265s\n",
      "  training loss:\t\t0.162114\n",
      "  validation loss:\t\t0.163928\n",
      "  validation accuracy:\t\t94.17 %\n",
      "Epoch 133 of 200 took 1506.130s\n",
      "  training loss:\t\t0.161926\n",
      "  validation loss:\t\t0.163701\n",
      "  validation accuracy:\t\t94.17 %\n",
      "Epoch 134 of 200 took 1489.282s\n",
      "  training loss:\t\t0.162886\n",
      "  validation loss:\t\t0.163869\n",
      "  validation accuracy:\t\t94.09 %\n",
      "Epoch 135 of 200 took 1488.861s\n",
      "  training loss:\t\t0.162369\n",
      "  validation loss:\t\t0.163760\n",
      "  validation accuracy:\t\t94.12 %\n",
      "Epoch 136 of 200 took 1507.212s\n",
      "  training loss:\t\t0.161135\n",
      "  validation loss:\t\t0.163414\n",
      "  validation accuracy:\t\t94.16 %\n",
      "Epoch 137 of 200 took 1507.275s\n",
      "  training loss:\t\t0.161657\n",
      "  validation loss:\t\t0.163888\n",
      "  validation accuracy:\t\t94.15 %\n",
      "Epoch 138 of 200 took 1508.104s\n",
      "  training loss:\t\t0.162687\n",
      "  validation loss:\t\t0.163595\n",
      "  validation accuracy:\t\t94.12 %\n",
      "Epoch 139 of 200 took 1506.442s\n",
      "  training loss:\t\t0.161041\n",
      "  validation loss:\t\t0.163313\n",
      "  validation accuracy:\t\t94.11 %\n",
      "Epoch 140 of 200 took 1506.050s\n",
      "  training loss:\t\t0.160965\n",
      "  validation loss:\t\t0.163831\n",
      "  validation accuracy:\t\t94.13 %\n",
      "Epoch 141 of 200 took 1505.977s\n",
      "  training loss:\t\t0.160757\n",
      "  validation loss:\t\t0.163520\n",
      "  validation accuracy:\t\t94.12 %\n",
      "Epoch 142 of 200 took 1505.849s\n",
      "  training loss:\t\t0.161297\n",
      "  validation loss:\t\t0.163243\n",
      "  validation accuracy:\t\t94.12 %\n",
      "Epoch 143 of 200 took 1505.890s\n",
      "  training loss:\t\t0.161828\n",
      "  validation loss:\t\t0.164318\n",
      "  validation accuracy:\t\t94.11 %\n",
      "Epoch 144 of 200 took 1506.903s\n",
      "  training loss:\t\t0.160735\n",
      "  validation loss:\t\t0.163524\n",
      "  validation accuracy:\t\t94.10 %\n",
      "Epoch 145 of 200 took 1507.163s\n",
      "  training loss:\t\t0.161762\n",
      "  validation loss:\t\t0.163257\n",
      "  validation accuracy:\t\t94.13 %\n",
      "Epoch 146 of 200 took 1506.977s\n",
      "  training loss:\t\t0.160437\n",
      "  validation loss:\t\t0.163728\n",
      "  validation accuracy:\t\t94.16 %\n",
      "Epoch 147 of 200 took 1490.313s\n",
      "  training loss:\t\t0.160779\n",
      "  validation loss:\t\t0.162711\n",
      "  validation accuracy:\t\t94.16 %\n",
      "Epoch 148 of 200 took 1490.440s\n",
      "  training loss:\t\t0.161186\n",
      "  validation loss:\t\t0.162802\n",
      "  validation accuracy:\t\t94.16 %\n",
      "Epoch 149 of 200 took 1506.232s\n",
      "  training loss:\t\t0.160744\n",
      "  validation loss:\t\t0.162806\n",
      "  validation accuracy:\t\t94.12 %\n",
      "Epoch 150 of 200 took 1506.140s\n",
      "  training loss:\t\t0.161203\n",
      "  validation loss:\t\t0.163141\n",
      "  validation accuracy:\t\t94.12 %\n",
      " setting learning rate to 0.0001000\n",
      "Epoch 151 of 200 took 1506.257s\n",
      "  training loss:\t\t0.160567\n",
      "  validation loss:\t\t0.163275\n",
      "  validation accuracy:\t\t94.13 %\n",
      "Epoch 152 of 200 took 1488.917s\n",
      "  training loss:\t\t0.160114\n",
      "  validation loss:\t\t0.163130\n",
      "  validation accuracy:\t\t94.14 %\n",
      "Epoch 153 of 200 took 1505.937s\n",
      "  training loss:\t\t0.159284\n",
      "  validation loss:\t\t0.163032\n",
      "  validation accuracy:\t\t94.14 %\n",
      "Epoch 154 of 200 took 1505.833s\n",
      "  training loss:\t\t0.160500\n",
      "  validation loss:\t\t0.163035\n",
      "  validation accuracy:\t\t94.16 %\n",
      "Epoch 155 of 200 took 1506.392s\n",
      "  training loss:\t\t0.159691\n",
      "  validation loss:\t\t0.162984\n",
      "  validation accuracy:\t\t94.15 %\n",
      "Epoch 156 of 200 took 1506.631s\n",
      "  training loss:\t\t0.160421\n",
      "  validation loss:\t\t0.162862\n",
      "  validation accuracy:\t\t94.16 %\n",
      "Epoch 157 of 200 took 1507.661s\n",
      "  training loss:\t\t0.159904\n",
      "  validation loss:\t\t0.162880\n",
      "  validation accuracy:\t\t94.14 %\n",
      "Epoch 158 of 200 took 1506.127s\n",
      "  training loss:\t\t0.160733\n",
      "  validation loss:\t\t0.163173\n",
      "  validation accuracy:\t\t94.15 %\n",
      "Epoch 159 of 200 took 1506.510s\n",
      "  training loss:\t\t0.159482\n",
      "  validation loss:\t\t0.163199\n",
      "  validation accuracy:\t\t94.15 %\n",
      "Epoch 160 of 200 took 1507.684s\n",
      "  training loss:\t\t0.159867\n",
      "  validation loss:\t\t0.163042\n",
      "  validation accuracy:\t\t94.15 %\n",
      "Epoch 161 of 200 took 1510.961s\n",
      "  training loss:\t\t0.160314\n",
      "  validation loss:\t\t0.163134\n",
      "  validation accuracy:\t\t94.13 %\n",
      "Epoch 162 of 200 took 1506.245s\n",
      "  training loss:\t\t0.158885\n",
      "  validation loss:\t\t0.163073\n",
      "  validation accuracy:\t\t94.14 %\n",
      "Epoch 163 of 200 took 1504.877s\n",
      "  training loss:\t\t0.160111\n",
      "  validation loss:\t\t0.163128\n",
      "  validation accuracy:\t\t94.13 %\n",
      "Epoch 164 of 200 took 1506.223s\n",
      "  training loss:\t\t0.159346\n",
      "  validation loss:\t\t0.163063\n",
      "  validation accuracy:\t\t94.13 %\n",
      "Epoch 165 of 200 took 1508.414s\n",
      "  training loss:\t\t0.159499\n",
      "  validation loss:\t\t0.162852\n",
      "  validation accuracy:\t\t94.15 %\n",
      "Epoch 166 of 200 took 1506.044s\n",
      "  training loss:\t\t0.159424\n",
      "  validation loss:\t\t0.163095\n",
      "  validation accuracy:\t\t94.12 %\n",
      "Epoch 167 of 200 took 1507.425s\n",
      "  training loss:\t\t0.159642\n",
      "  validation loss:\t\t0.162945\n",
      "  validation accuracy:\t\t94.13 %\n",
      "Epoch 168 of 200 took 1506.177s\n",
      "  training loss:\t\t0.158930\n",
      "  validation loss:\t\t0.162766\n",
      "  validation accuracy:\t\t94.14 %\n",
      "Epoch 169 of 200 took 1509.602s\n",
      "  training loss:\t\t0.158797\n",
      "  validation loss:\t\t0.163166\n",
      "  validation accuracy:\t\t94.12 %\n",
      "Epoch 170 of 200 took 1518.978s\n",
      "  training loss:\t\t0.159465\n",
      "  validation loss:\t\t0.162629\n",
      "  validation accuracy:\t\t94.15 %\n",
      "Epoch 171 of 200 took 1509.462s\n",
      "  training loss:\t\t0.159920\n",
      "  validation loss:\t\t0.162999\n",
      "  validation accuracy:\t\t94.15 %\n",
      "Epoch 172 of 200 took 1521.873s\n",
      "  training loss:\t\t0.159760\n",
      "  validation loss:\t\t0.162944\n",
      "  validation accuracy:\t\t94.14 %\n",
      "Epoch 173 of 200 took 1508.265s\n",
      "  training loss:\t\t0.159839\n",
      "  validation loss:\t\t0.162857\n",
      "  validation accuracy:\t\t94.12 %\n",
      "Epoch 174 of 200 took 1506.827s\n",
      "  training loss:\t\t0.159911\n",
      "  validation loss:\t\t0.162986\n",
      "  validation accuracy:\t\t94.15 %\n",
      "Epoch 175 of 200 took 1507.706s\n",
      "  training loss:\t\t0.160214\n",
      "  validation loss:\t\t0.162812\n",
      "  validation accuracy:\t\t94.16 %\n",
      "Epoch 176 of 200 took 1525.028s\n",
      "  training loss:\t\t0.159430\n",
      "  validation loss:\t\t0.162884\n",
      "  validation accuracy:\t\t94.15 %\n",
      "Epoch 177 of 200 took 1525.371s\n",
      "  training loss:\t\t0.159507\n",
      "  validation loss:\t\t0.162881\n",
      "  validation accuracy:\t\t94.16 %\n",
      "Epoch 178 of 200 took 1526.376s\n",
      "  training loss:\t\t0.159362\n",
      "  validation loss:\t\t0.162823\n",
      "  validation accuracy:\t\t94.16 %\n",
      "Epoch 179 of 200 took 1521.707s\n",
      "  training loss:\t\t0.160299\n",
      "  validation loss:\t\t0.162932\n",
      "  validation accuracy:\t\t94.16 %\n",
      "Epoch 180 of 200 took 1526.368s\n",
      "  training loss:\t\t0.158861\n",
      "  validation loss:\t\t0.162820\n",
      "  validation accuracy:\t\t94.16 %\n",
      "Epoch 181 of 200 took 1525.579s\n",
      "  training loss:\t\t0.159427\n",
      "  validation loss:\t\t0.162666\n",
      "  validation accuracy:\t\t94.14 %\n",
      "Epoch 182 of 200 took 1525.948s\n",
      "  training loss:\t\t0.159916\n",
      "  validation loss:\t\t0.162802\n",
      "  validation accuracy:\t\t94.15 %\n",
      "Epoch 183 of 200 took 1526.220s\n",
      "  training loss:\t\t0.160392\n",
      "  validation loss:\t\t0.162973\n",
      "  validation accuracy:\t\t94.14 %\n",
      "Epoch 184 of 200 took 1525.406s\n",
      "  training loss:\t\t0.158991\n",
      "  validation loss:\t\t0.162845\n",
      "  validation accuracy:\t\t94.16 %\n",
      "Epoch 185 of 200 took 1525.597s\n",
      "  training loss:\t\t0.158997\n",
      "  validation loss:\t\t0.162711\n",
      "  validation accuracy:\t\t94.21 %\n",
      "Epoch 186 of 200 took 1525.726s\n",
      "  training loss:\t\t0.160067\n",
      "  validation loss:\t\t0.162719\n",
      "  validation accuracy:\t\t94.16 %\n",
      "Epoch 187 of 200 took 1526.249s\n",
      "  training loss:\t\t0.160013\n",
      "  validation loss:\t\t0.162730\n",
      "  validation accuracy:\t\t94.12 %\n",
      "Epoch 188 of 200 took 1525.624s\n",
      "  training loss:\t\t0.159820\n",
      "  validation loss:\t\t0.162809\n",
      "  validation accuracy:\t\t94.16 %\n",
      "Epoch 189 of 200 took 1526.503s\n",
      "  training loss:\t\t0.159548\n",
      "  validation loss:\t\t0.162863\n",
      "  validation accuracy:\t\t94.19 %\n",
      "Epoch 190 of 200 took 1526.052s\n",
      "  training loss:\t\t0.159997\n",
      "  validation loss:\t\t0.162657\n",
      "  validation accuracy:\t\t94.18 %\n",
      "Epoch 191 of 200 took 1526.140s\n",
      "  training loss:\t\t0.158435\n",
      "  validation loss:\t\t0.162993\n",
      "  validation accuracy:\t\t94.16 %\n",
      "Epoch 192 of 200 took 1526.140s\n",
      "  training loss:\t\t0.159682\n",
      "  validation loss:\t\t0.162774\n",
      "  validation accuracy:\t\t94.16 %\n",
      "Epoch 193 of 200 took 1525.335s\n",
      "  training loss:\t\t0.159949\n",
      "  validation loss:\t\t0.162783\n",
      "  validation accuracy:\t\t94.14 %\n",
      "Epoch 194 of 200 took 1525.145s\n",
      "  training loss:\t\t0.159524\n",
      "  validation loss:\t\t0.162668\n",
      "  validation accuracy:\t\t94.16 %\n",
      "Epoch 195 of 200 took 1526.212s\n",
      "  training loss:\t\t0.159908\n",
      "  validation loss:\t\t0.162950\n",
      "  validation accuracy:\t\t94.16 %\n",
      "Epoch 196 of 200 took 1526.059s\n",
      "  training loss:\t\t0.159680\n",
      "  validation loss:\t\t0.163074\n",
      "  validation accuracy:\t\t94.13 %\n",
      "Epoch 197 of 200 took 1524.797s\n",
      "  training loss:\t\t0.159525\n",
      "  validation loss:\t\t0.162658\n",
      "  validation accuracy:\t\t94.16 %\n",
      "Epoch 198 of 200 took 1526.074s\n",
      "  training loss:\t\t0.159666\n",
      "  validation loss:\t\t0.162674\n",
      "  validation accuracy:\t\t94.18 %\n",
      "Epoch 199 of 200 took 1525.487s\n",
      "  training loss:\t\t0.160041\n",
      "  validation loss:\t\t0.162711\n",
      "  validation accuracy:\t\t94.17 %\n",
      "Epoch 200 of 200 took 1524.796s\n",
      "  training loss:\t\t0.159968\n",
      "  validation loss:\t\t0.162715\n",
      "  validation accuracy:\t\t94.17 %\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Finally, launch the training loop.\n",
    "print(\"Starting training...\")\n",
    "# We iterate over epochs:\n",
    "for epoch in range(num_epochs):\n",
    "    if epoch in learning_rate_schedule:\n",
    "        lr = np.float32(learning_rate_schedule[epoch])\n",
    "        print(\" setting learning rate to %.7f\" % lr)\n",
    "        learning_rate.set_value(lr)\n",
    "    # In each epoch, we do a full pass over the training data:\n",
    "    train_err = 0\n",
    "    train_batches = 0\n",
    "    start_time = time.time()\n",
    "    for batch in iterate_minibatches(X_train2, y_train2, 128, shuffle=False):\n",
    "        inputs, targets = batch\n",
    "        train_err += train_fn(inputs, targets)\n",
    "        train_batches += 1\n",
    "\n",
    "    # And a full pass over the validation data:\n",
    "    val_err = 0\n",
    "    val_acc = 0\n",
    "    val_batches = 0\n",
    "    for batch in iterate_minibatches(X_val, y_val, 128, shuffle=False):\n",
    "        inputs, targets = batch\n",
    "        err, acc = val_fn(inputs, targets)\n",
    "        val_err += err\n",
    "        val_acc += acc\n",
    "        val_batches += 1\n",
    "\n",
    "    # Then we print the results for this epoch:\n",
    "    print(\"Epoch {} of {} took {:.3f}s\".format(\n",
    "        epoch + 1, num_epochs, time.time() - start_time))\n",
    "    print(\"  training loss:\\t\\t{:.6f}\".format(train_err / train_batches))\n",
    "    print(\"  validation loss:\\t\\t{:.6f}\".format(val_err / val_batches))\n",
    "    print(\"  validation accuracy:\\t\\t{:.2f} %\".format(\n",
    "        val_acc / val_batches * 100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final results:\n",
      "  test loss:\t\t\t0.189214\n",
      "  test accuracy:\t\t93.23 %\n"
     ]
    }
   ],
   "source": [
    "# After training, we compute and print the test error:\n",
    "test_err = 0\n",
    "test_acc = 0\n",
    "test_batches = 0\n",
    "for batch in iterate_minibatches(X_test, y_test, 128, shuffle=False):\n",
    "    inputs, targets = batch\n",
    "    err, acc = val_fn(inputs, targets)\n",
    "    test_err += err\n",
    "    test_acc += acc\n",
    "    test_batches += 1\n",
    "print(\"Final results:\")\n",
    "print(\"  test loss:\\t\\t\\t{:.6f}\".format(test_err / test_batches))\n",
    "print(\"  test accuracy:\\t\\t{:.2f} %\".format(\n",
    "    test_acc / test_batches * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.savez('/home/d/resnet2.npz', *lasagne.layers.get_all_param_values(network))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<lasagne.layers.special.NonlinearityLayer at 0x7f674cf3b410>"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "network.input_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_loss(net):\n",
    "    train_loss = [row['train_loss'] for row in net.train_history_]\n",
    "    valid_loss = [row['valid_loss'] for row in net.train_history_]\n",
    "    plt.plot(train_loss, label='train loss')\n",
    "    plt.plot(valid_loss, label='valid loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.ylabel('loss')\n",
    "    plt.legend(loc='best')\n",
    "    return plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DenseLayer' object has no attribute 'train_history_'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-eedbaacf80c0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mplot_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnetwork\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-13-f862f5ec8e53>\u001b[0m in \u001b[0;36mplot_loss\u001b[1;34m(net)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mplot_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0mtrain_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mrow\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'train_loss'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mrow\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mnet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_history_\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m     \u001b[0mvalid_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mrow\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'valid_loss'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mrow\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mnet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_history_\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'train loss'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalid_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'valid loss'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'DenseLayer' object has no attribute 'train_history_'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Finally, launch the training loop.\n",
    "print(\"Starting training...\")\n",
    "# We iterate over epochs:\n",
    "for epoch in range(200,200+num_epochs):\n",
    "    if epoch in learning_rate_schedule:\n",
    "        lr = np.float32(learning_rate_schedule[epoch])\n",
    "        print(\" setting learning rate to %.7f\" % lr)\n",
    "        learning_rate.set_value(lr)\n",
    "    # In each epoch, we do a full pass over the training data:\n",
    "    train_err = 0\n",
    "    train_batches = 0\n",
    "    start_time = time.time()\n",
    "    for batch in iterate_minibatches(X_train2, y_train2, 128, shuffle=False):\n",
    "        inputs, targets = batch\n",
    "        train_err += train_fn(inputs, targets)\n",
    "        train_batches += 1\n",
    "\n",
    "    # And a full pass over the validation data:\n",
    "    val_err = 0\n",
    "    val_acc = 0\n",
    "    val_batches = 0\n",
    "    for batch in iterate_minibatches(X_val, y_val, 128, shuffle=False):\n",
    "        inputs, targets = batch\n",
    "        err, acc = val_fn(inputs, targets)\n",
    "        val_err += err\n",
    "        val_acc += acc\n",
    "        val_batches += 1\n",
    "\n",
    "    # Then we print the results for this epoch:\n",
    "    print(\"Epoch {} of {} took {:.3f}s\".format(\n",
    "        epoch + 1, num_epochs, time.time() - start_time))\n",
    "    print(\"  training loss:\\t\\t{:.6f}\".format(train_err / train_batches))\n",
    "    print(\"  validation loss:\\t\\t{:.6f}\".format(val_err / val_batches))\n",
    "    print(\"  validation accuracy:\\t\\t{:.2f} %\".format(\n",
    "        val_acc / val_batches * 100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 0.0001, 2: 0.01, 100: 0.001, 150: 0.0001}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learning_rate_schedule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "328"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.15785868913180343"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_err / train_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final results:\n",
      "  test loss:\t\t\t0.190347\n",
      "  test accuracy:\t\t93.34 %\n"
     ]
    }
   ],
   "source": [
    "# After training, we compute and print the test error:\n",
    "test_err = 0\n",
    "test_acc = 0\n",
    "test_batches = 0\n",
    "for batch in iterate_minibatches(X_test, y_test, 128, shuffle=False):\n",
    "    inputs, targets = batch\n",
    "    err, acc = val_fn(inputs, targets)\n",
    "    test_err += err\n",
    "    test_acc += acc\n",
    "    test_batches += 1\n",
    "print(\"Final results:\")\n",
    "print(\"  test loss:\\t\\t\\t{:.6f}\".format(test_err / test_batches))\n",
    "print(\"  test accuracy:\\t\\t{:.2f} %\".format(\n",
    "    test_acc / test_batches * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final results:\n",
      "  test loss:\t\t\t0.190400\n",
      "  test accuracy:\t\t93.36 %\n"
     ]
    }
   ],
   "source": [
    "# After training, we compute and print the test error:\n",
    "test_err = 0\n",
    "test_acc = 0\n",
    "test_batches = 0\n",
    "for batch in iterate_minibatches(X_test, y_test, 128, shuffle=False):\n",
    "    inputs, targets = batch\n",
    "    err, acc = val_fn(inputs, targets)\n",
    "    test_err += err\n",
    "    test_acc += acc\n",
    "    test_batches += 1\n",
    "print(\"Final results:\")\n",
    "print(\"  test loss:\\t\\t\\t{:.6f}\".format(test_err / test_batches))\n",
    "print(\"  test accuracy:\\t\\t{:.2f} %\".format(\n",
    "    test_acc / test_batches * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "94.174360795454547"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_acc / val_batches * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.16246618782593444"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_err / val_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.savez('/home/d/resnet2_after328.npz', *lasagne.layers.get_all_param_values(network))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "    # Optionally, you could now dump the network weights to a file like this:\n",
    "    # np.savez('model.npz', *lasagne.layers.get_all_param_values(network))\n",
    "    #\n",
    "    # And load them again later on like this:\n",
    "    # with np.load('model.npz') as f:\n",
    "    #     param_values = [f['arr_%d' % i] for i in range(len(f.files))]\n",
    "    # lasagne.layers.set_all_param_values(network, param_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#\n",
    "#\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Outputs must be theano Variable or Out instances. Received 127.90625 of type <type 'numpy.float64'>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-34-759fc37d2e5a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mpred_fn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtheano\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0minput_var\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_var\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mtest_prediction\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_acc\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m/home/d/nolearn/local/lib/python2.7/site-packages/theano/compile/function.pyc\u001b[0m in \u001b[0;36mfunction\u001b[1;34m(inputs, outputs, mode, updates, givens, no_default_updates, accept_inplace, name, rebuild_strict, allow_input_downcast, profile, on_unused_input)\u001b[0m\n\u001b[0;32m    318\u001b[0m                    \u001b[0mon_unused_input\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mon_unused_input\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    319\u001b[0m                    \u001b[0mprofile\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mprofile\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 320\u001b[1;33m                    output_keys=output_keys)\n\u001b[0m\u001b[0;32m    321\u001b[0m     \u001b[1;31m# We need to add the flag check_aliased inputs if we have any mutable or\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    322\u001b[0m     \u001b[1;31m# borrowed used defined inputs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/d/nolearn/local/lib/python2.7/site-packages/theano/compile/pfunc.pyc\u001b[0m in \u001b[0;36mpfunc\u001b[1;34m(params, outputs, mode, updates, givens, no_default_updates, accept_inplace, name, rebuild_strict, allow_input_downcast, profile, on_unused_input, output_keys)\u001b[0m\n\u001b[0;32m    440\u001b[0m                                          \u001b[0mrebuild_strict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrebuild_strict\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    441\u001b[0m                                          \u001b[0mcopy_inputs_over\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 442\u001b[1;33m                                          no_default_updates=no_default_updates)\n\u001b[0m\u001b[0;32m    443\u001b[0m     \u001b[1;31m# extracting the arguments\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    444\u001b[0m     \u001b[0minput_variables\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcloned_extended_outputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mother_stuff\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moutput_vars\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/d/nolearn/local/lib/python2.7/site-packages/theano/compile/pfunc.pyc\u001b[0m in \u001b[0;36mrebuild_collect_shared\u001b[1;34m(outputs, inputs, replace, updates, rebuild_strict, copy_inputs_over, no_default_updates)\u001b[0m\n\u001b[0;32m    225\u001b[0m                 raise TypeError('Outputs must be theano Variable or '\n\u001b[0;32m    226\u001b[0m                                 \u001b[1;34m'Out instances. Received '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 227\u001b[1;33m                                 ' of type ' + str(type(v)))\n\u001b[0m\u001b[0;32m    228\u001b[0m             \u001b[1;31m# computed_list.append(cloned_v)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    229\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: Outputs must be theano Variable or Out instances. Received 127.90625 of type <type 'numpy.float64'>"
     ]
    }
   ],
   "source": [
    "pred_fn = theano.function([input_var, target_var], [test_prediction, test_loss, test_acc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "    test_prediction = lasagne.layers.get_output(network, deterministic=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "MissingInputError",
     "evalue": "(\"An input of the graph, used to compute Shape(targets), was not provided and not given a value.Use the Theano flag exception_verbosity='high',for more information on this error.\", targets)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMissingInputError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-40-a06ce32be050>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtarget_var\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m/home/d/nolearn/local/lib/python2.7/site-packages/theano/gof/graph.pyc\u001b[0m in \u001b[0;36meval\u001b[1;34m(self, inputs_to_values)\u001b[0m\n\u001b[0;32m    518\u001b[0m         \u001b[0minputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msorted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs_to_values\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    519\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0minputs\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fn_cache\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 520\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fn_cache\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtheano\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    521\u001b[0m         \u001b[0margs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0minputs_to_values\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mparam\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mparam\u001b[0m \u001b[1;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    522\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/d/nolearn/local/lib/python2.7/site-packages/theano/compile/function.pyc\u001b[0m in \u001b[0;36mfunction\u001b[1;34m(inputs, outputs, mode, updates, givens, no_default_updates, accept_inplace, name, rebuild_strict, allow_input_downcast, profile, on_unused_input)\u001b[0m\n\u001b[0;32m    318\u001b[0m                    \u001b[0mon_unused_input\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mon_unused_input\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    319\u001b[0m                    \u001b[0mprofile\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mprofile\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 320\u001b[1;33m                    output_keys=output_keys)\n\u001b[0m\u001b[0;32m    321\u001b[0m     \u001b[1;31m# We need to add the flag check_aliased inputs if we have any mutable or\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    322\u001b[0m     \u001b[1;31m# borrowed used defined inputs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/d/nolearn/local/lib/python2.7/site-packages/theano/compile/pfunc.pyc\u001b[0m in \u001b[0;36mpfunc\u001b[1;34m(params, outputs, mode, updates, givens, no_default_updates, accept_inplace, name, rebuild_strict, allow_input_downcast, profile, on_unused_input, output_keys)\u001b[0m\n\u001b[0;32m    477\u001b[0m                          \u001b[0maccept_inplace\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maccept_inplace\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    478\u001b[0m                          \u001b[0mprofile\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mprofile\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mon_unused_input\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mon_unused_input\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 479\u001b[1;33m                          output_keys=output_keys)\n\u001b[0m\u001b[0;32m    480\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    481\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/d/nolearn/local/lib/python2.7/site-packages/theano/compile/function_module.pyc\u001b[0m in \u001b[0;36morig_function\u001b[1;34m(inputs, outputs, mode, accept_inplace, name, profile, on_unused_input, output_keys)\u001b[0m\n\u001b[0;32m   1774\u001b[0m                    \u001b[0mprofile\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mprofile\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1775\u001b[0m                    \u001b[0mon_unused_input\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mon_unused_input\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1776\u001b[1;33m                    \u001b[0moutput_keys\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moutput_keys\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1777\u001b[0m             defaults)\n\u001b[0;32m   1778\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/d/nolearn/local/lib/python2.7/site-packages/theano/compile/function_module.pyc\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, inputs, outputs, mode, accept_inplace, function_builder, profile, on_unused_input, fgraph, output_keys)\u001b[0m\n\u001b[0;32m   1426\u001b[0m             \u001b[1;31m# OUTPUT VARIABLES)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1427\u001b[0m             fgraph, additional_outputs = std_fgraph(inputs, outputs,\n\u001b[1;32m-> 1428\u001b[1;33m                                                     accept_inplace)\n\u001b[0m\u001b[0;32m   1429\u001b[0m             \u001b[0mfgraph\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprofile\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprofile\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1430\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/d/nolearn/local/lib/python2.7/site-packages/theano/compile/function_module.pyc\u001b[0m in \u001b[0;36mstd_fgraph\u001b[1;34m(input_specs, output_specs, accept_inplace)\u001b[0m\n\u001b[0;32m    175\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    176\u001b[0m     fgraph = gof.fg.FunctionGraph(orig_inputs, orig_outputs,\n\u001b[1;32m--> 177\u001b[1;33m                                   update_mapping=update_mapping)\n\u001b[0m\u001b[0;32m    178\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    179\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mnode\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mfgraph\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_nodes\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/d/nolearn/local/lib/python2.7/site-packages/theano/gof/fg.pyc\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, inputs, outputs, features, clone, update_mapping)\u001b[0m\n\u001b[0;32m    169\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    170\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0moutput\u001b[0m \u001b[1;32min\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 171\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__import_r__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreason\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"init\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    172\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    173\u001b[0m             \u001b[0moutput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclients\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'output'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/d/nolearn/local/lib/python2.7/site-packages/theano/gof/fg.pyc\u001b[0m in \u001b[0;36m__import_r__\u001b[1;34m(self, variable, reason)\u001b[0m\n\u001b[0;32m    358\u001b[0m         \u001b[1;31m# Imports the owners of the variables\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    359\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mvariable\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mowner\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mvariable\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mowner\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_nodes\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 360\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__import__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvariable\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mowner\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreason\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mreason\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    361\u001b[0m         if (variable.owner is None and\n\u001b[0;32m    362\u001b[0m                 \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvariable\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgraph\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mConstant\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/d/nolearn/local/lib/python2.7/site-packages/theano/gof/fg.pyc\u001b[0m in \u001b[0;36m__import__\u001b[1;34m(self, apply_node, check, reason)\u001b[0m\n\u001b[0;32m    472\u001b[0m                             \u001b[1;34m\"for more information on this error.\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    473\u001b[0m                             % str(node)),\n\u001b[1;32m--> 474\u001b[1;33m                             r)\n\u001b[0m\u001b[0;32m    475\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    476\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mnode\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mnew_nodes\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMissingInputError\u001b[0m: (\"An input of the graph, used to compute Shape(targets), was not provided and not given a value.Use the Theano flag exception_verbosity='high',for more information on this error.\", targets)"
     ]
    }
   ],
   "source": [
    "target_var.shape.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
