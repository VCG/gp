{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using gpu device 0: GeForce GTX TITAN (CNMeM is disabled, CuDNN 4007)\n",
      "/home/d/nolearn/local/lib/python2.7/site-packages/theano/tensor/signal/downsample.py:6: UserWarning: downsample module has been moved to the theano.tensor.signal.pool module.\n",
      "  \"downsample module has been moved to the theano.tensor.signal.pool module.\")\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import time\n",
    "import string\n",
    "\n",
    "import numpy as np\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "\n",
    "import os; import sys; sys.path.append('..')\n",
    "import gp\n",
    "\n",
    "import gp.nets as nets\n",
    "import gp.nets.BatchNormLayer as BatchNormLayer\n",
    "\n",
    "import lasagne\n",
    "\n",
    "sys.setrecursionlimit(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from matplotlib.pyplot import imshow\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# helper function for projection_b\n",
    "def ceildiv(a, b):\n",
    "    return -(-a // b)\n",
    "\n",
    "def build_cnn(input_var=None, n=1, num_filters=8, cudnn='no'):\n",
    "    import lasagne # For some odd reason it can't read the global import, please PR/Issue if you know why\n",
    "    projection_type = 'B'\n",
    "    # Setting up layers\n",
    "    if cudnn == 'yes':\n",
    "        import lasagne.layers.dnn\n",
    "        conv = lasagne.layers.dnn.Conv2DDNNLayer # cuDNN\n",
    "    else:\n",
    "        conv = lasagne.layers.Conv2DLayer\n",
    "    dropout = lasagne.layers.DropoutLayer\n",
    "    nonlin = lasagne.nonlinearities.rectify\n",
    "    nonlin_layer = lasagne.layers.NonlinearityLayer\n",
    "    sumlayer = lasagne.layers.ElemwiseSumLayer\n",
    "    #batchnorm = BatchNormLayer.BatchNormLayer\n",
    "    batchnorm = lasagne.layers.BatchNormLayer\n",
    "\n",
    "    # Setting the projection type for when reducing height/width\n",
    "    # and increasing dimensions.\n",
    "    # Default is 'B' as B performs slightly better\n",
    "    # and A requires newer version of lasagne with ExpressionLayer\n",
    "    projection_type = 'B'\n",
    "    if projection_type == 'A':\n",
    "        expression = lasagne.layers.ExpressionLayer\n",
    "        pad = lasagne.layers.PadLayer\n",
    "\n",
    "    if projection_type == 'A':\n",
    "        # option A for projection as described in paper\n",
    "        # (should perform slightly worse than B)\n",
    "        def projection(l_inp):\n",
    "            n_filters = l_inp.output_shape[1]*2\n",
    "            l = expression(l_inp, lambda X: X[:, :, ::2, ::2], lambda s: (s[0], s[1], ceildiv(s[2], 2), ceildiv(s[3], 2)))\n",
    "            l = pad(l, [n_filters//4,0,0], batch_ndim=1)\n",
    "            return l\n",
    "\n",
    "    if projection_type == 'B':\n",
    "        # option B for projection as described in paper\n",
    "        def projection(l_inp):\n",
    "            # twice normal channels when projecting!\n",
    "            n_filters = l_inp.output_shape[1]*2 \n",
    "            l = conv(l_inp, num_filters=n_filters, filter_size=(1, 1),\n",
    "                     stride=(2, 2), nonlinearity=None, pad='same', b=None)\n",
    "            l = batchnorm(l)\n",
    "            return l\n",
    "\n",
    "    # helper function to handle filters/strides when increasing dims\n",
    "    def filters_increase_dims(l, increase_dims):\n",
    "        in_num_filters = l.output_shape[1]\n",
    "        if increase_dims:\n",
    "            first_stride = (2, 2)\n",
    "            out_num_filters = in_num_filters*2\n",
    "        else:\n",
    "            first_stride = (1, 1)\n",
    "            out_num_filters = in_num_filters\n",
    " \n",
    "        return out_num_filters, first_stride\n",
    "\n",
    "    # block as described and used in cifar in the original paper:\n",
    "    # http://arxiv.org/abs/1512.03385\n",
    "    def res_block_v1(l_inp, nonlinearity=nonlin, increase_dim=False):\n",
    "        # first figure filters/strides\n",
    "        n_filters, first_stride = filters_increase_dims(l_inp, increase_dim)\n",
    "        # conv -> BN -> nonlin -> conv -> BN -> sum -> nonlin\n",
    "        l = conv(l_inp, num_filters=n_filters, filter_size=(3, 3),\n",
    "                 stride=first_stride, nonlinearity=None, pad='same',\n",
    "                 W=lasagne.init.HeNormal(gain='relu'))\n",
    "        l = batchnorm(l)\n",
    "\n",
    "        l = nonlin_layer(l, nonlinearity=nonlin)\n",
    "        l = dropout(l, p=.2)\n",
    "        print('adding dropout')        \n",
    "        \n",
    "        l = conv(l, num_filters=n_filters, filter_size=(3, 3),\n",
    "                 stride=(1, 1), nonlinearity=None, pad='same',\n",
    "                 W=lasagne.init.HeNormal(gain='relu'))\n",
    "        l = batchnorm(l)\n",
    "        if increase_dim:\n",
    "            # Use projection (A, B) as described in paper\n",
    "            p = projection(l_inp)\n",
    "        else:\n",
    "            # Identity shortcut\n",
    "            p = l_inp\n",
    "        l = sumlayer([l, p])\n",
    "        l = nonlin_layer(l, nonlinearity=nonlin)\n",
    "        return l\n",
    "\n",
    "    # block as described in second paper on the subject (by same authors):\n",
    "    # http://arxiv.org/abs/1603.05027\n",
    "    def res_block_v2(l_inp, nonlinearity=nonlin, increase_dim=False):\n",
    "        # first figure filters/strides\n",
    "        n_filters, first_stride = filters_increase_dims(l_inp, increase_dim)\n",
    "        # BN -> nonlin -> conv -> BN -> nonlin -> conv -> sum\n",
    "        l = batchnorm(l_inp)\n",
    "        l = nonlin_layer(l, nonlinearity=nonlin)\n",
    "        l = conv(l, num_filters=n_filters, filter_size=(3, 3),\n",
    "                 stride=first_stride, nonlinearity=None, pad='same',\n",
    "                 W=lasagne.init.HeNormal(gain='relu'))\n",
    "        l = batchnorm(l)\n",
    "        l = nonlin_layer(l, nonlinearity=nonlin)\n",
    "        l = conv(l, num_filters=n_filters, filter_size=(3, 3),\n",
    "                 stride=(1, 1), nonlinearity=None, pad='same',\n",
    "                 W=lasagne.init.HeNormal(gain='relu'))\n",
    "        if increase_dim:\n",
    "            # Use projection (A, B) as described in paper\n",
    "            p = projection(l_inp)\n",
    "        else:\n",
    "            # Identity shortcut\n",
    "            p = l_inp\n",
    "        l = sumlayer([l, p])\n",
    "        return l\n",
    "\n",
    "    def bottleneck_block(l_inp, nonlinearity=nonlin, increase_dim=False):\n",
    "        # first figure filters/strides\n",
    "        n_filters, first_stride = filters_increase_dims(l_inp, increase_dim)\n",
    "        # conv -> BN -> nonlin -> conv -> BN -> nonlin -> conv -> BN -> sum\n",
    "        # -> nonlin\n",
    "        # first make the bottleneck, scale the filters ..!\n",
    "        scale = 4 # as per bottleneck architecture used in paper\n",
    "        scaled_filters = n_filters/scale\n",
    "        l = conv(l_inp, num_filters=scaled_filters, filter_size=(1, 1),\n",
    "                 stride=first_stride, nonlinearity=None, pad='same',\n",
    "                 W=lasagne.init.HeNormal(gain='relu'))\n",
    "        l = batchnorm(l)\n",
    "        l = nonlin_layer(l, nonlinearity=nonlin)\n",
    "        l = conv(l, num_filters=scaled_filters, filter_size=(3, 3),\n",
    "                 stride=(1, 1), nonlinearity=None, pad='same',\n",
    "                 W=lasagne.init.HeNormal(gain='relu'))\n",
    "        l = batchnorm(l)\n",
    "        l = nonlin_layer(l, nonlinearity=nonlin)\n",
    "        l = conv(l, num_filters=n_filters, filter_size=(1, 1),\n",
    "                 stride=(1, 1), nonlinearity=None, pad='same',\n",
    "                 W=lasagne.init.HeNormal(gain='relu'))\n",
    "        if increase_dim:\n",
    "            # Use projection (A, B) as described in paper\n",
    "            p = projection(l_inp)\n",
    "        else:\n",
    "            # Identity shortcut\n",
    "            p = l_inp\n",
    "        l = sumlayer([l, p])\n",
    "        l = nonlin_layer(l, nonlinearity=nonlin)\n",
    "        return l\n",
    "\n",
    "    # Bottleneck architecture with more efficiency (the post with Kaiming He's response)\n",
    "    # https://www.reddit.com/r/MachineLearning/comments/3ywi6x/deep_residual_learning_the_bottleneck/ \n",
    "    def bottleneck_block_fast(l_inp, nonlinearity=nonlin, increase_dim=False):\n",
    "        # first figure filters/strides\n",
    "        n_filters, last_stride = filters_increase_dims(l_inp, increase_dim)\n",
    "        # conv -> BN -> nonlin -> conv -> BN -> nonlin -> conv -> BN -> sum\n",
    "        # -> nonlin\n",
    "        # first make the bottleneck, scale the filters ..!\n",
    "        scale = 4 # as per bottleneck architecture used in paper\n",
    "        scaled_filters = n_filters/scale\n",
    "        l = conv(l_inp, num_filters=scaled_filters, filter_size=(1, 1),\n",
    "                 stride=(1, 1), nonlinearity=None, pad='same',\n",
    "                 W=lasagne.init.HeNormal(gain='relu'))\n",
    "        l = batchnorm(l)\n",
    "        l = nonlin_layer(l, nonlinearity=nonlin)\n",
    "        l = conv(l, num_filters=scaled_filters, filter_size=(3, 3),\n",
    "                 stride=(1, 1), nonlinearity=None, pad='same',\n",
    "                 W=lasagne.init.HeNormal(gain='relu'))\n",
    "        l = batchnorm(l)\n",
    "        l = nonlin_layer(l, nonlinearity=nonlin)\n",
    "        l = conv(l, num_filters=n_filters, filter_size=(1, 1),\n",
    "                 stride=last_stride, nonlinearity=None, pad='same',\n",
    "                 W=lasagne.init.HeNormal(gain='relu'))\n",
    "        if increase_dim:\n",
    "            # Use projection (A, B) as described in paper\n",
    "            p = projection(l_inp)\n",
    "        else:\n",
    "            # Identity shortcut\n",
    "            p = l_inp\n",
    "        l = sumlayer([l, p])\n",
    "        l = nonlin_layer(l, nonlinearity=nonlin)\n",
    "        return l\n",
    "       \n",
    "    res_block = res_block_v1\n",
    "\n",
    "    # Stacks the residual blocks, makes it easy to model size of architecture with int n   \n",
    "    def blockstack(l, n, nonlinearity=nonlin):\n",
    "        print('NNN',n)\n",
    "        for _ in range(n):\n",
    "            print ('new')\n",
    "            l = res_block(l, nonlinearity=nonlin)\n",
    "        return l\n",
    "\n",
    "    # Building the network\n",
    "    l_in = lasagne.layers.InputLayer(shape=(None, 4, 75, 75),\n",
    "                                        input_var=input_var)\n",
    "    # First layer! just a plain convLayer\n",
    "    l1 = conv(l_in, num_filters=num_filters, stride=(1, 1),\n",
    "              filter_size=(3, 3), nonlinearity=None, pad='same')\n",
    "    l1 = batchnorm(l1)\n",
    "    l1 = nonlin_layer(l1, nonlinearity=nonlin)\n",
    "\n",
    "    # Stacking bottlenecks and increasing dims! (while reducing shape size)\n",
    "#     l1_bs = blockstack(l1, n=n)\n",
    "#     l1_id = res_block(l1_bs, increase_dim=True)\n",
    "\n",
    "#     l2_bs = blockstack(l1_id, n=n)\n",
    "#     l2_id = res_block(l2_bs, increase_dim=True)\n",
    "\n",
    "#     l3_bs = blockstack(l2_id, n=n)\n",
    "\n",
    "    l3_bs = blockstack(l1, n=n)\n",
    "\n",
    "    l3_do = dropout(l3_bs, p=.5)\n",
    "    \n",
    "    # And, finally, the 10-unit output layer:\n",
    "    network = lasagne.layers.DenseLayer(\n",
    "            l3_do,\n",
    "#             l1,\n",
    "            num_units=2,\n",
    "            nonlinearity=lasagne.nonlinearities.softmax)\n",
    "\n",
    "    return network\n",
    "\n",
    "\n",
    "# ############################# Batch iterator ###############################\n",
    "# This is just a simple helper function iterating over training data in\n",
    "# mini-batches of a particular size, optionally in random order. It assumes\n",
    "# data is available as numpy arrays. For big datasets, you could load numpy\n",
    "# arrays as memory-mapped files (np.load(..., mmap_mode='r')), or write your\n",
    "# own custom data iteration function. For small datasets, you can also copy\n",
    "# them to GPU at once for slightly improved performance. This would involve\n",
    "# several changes in the main program, though, and is not demonstrated here.\n",
    "\n",
    "def iterate_minibatches(inputs, targets, batchsize, shuffle=False):\n",
    "    assert len(inputs) == len(targets)\n",
    "    if shuffle:\n",
    "        indices = np.arange(len(inputs))\n",
    "        np.random.shuffle(indices)\n",
    "    for start_idx in range(0, len(inputs) - batchsize + 1, batchsize):\n",
    "        if shuffle:\n",
    "            excerpt = indices[start_idx:start_idx + batchsize]\n",
    "        else:\n",
    "            excerpt = slice(start_idx, start_idx + batchsize)\n",
    "            \n",
    "            Xb = inputs[excerpt]\n",
    "            yb = targets[excerpt]\n",
    "            \n",
    "            Xb = Xb - .5\n",
    "            \n",
    "            k_s = np.array([0,1,2,3],dtype=np.uint8)\n",
    "            for i in range(len(Xb)):\n",
    "                k = np.random.choice(k_s)\n",
    "                for j in range(Xb.shape[1]):\n",
    "                    Xb[j][0] = np.rot90(Xb[j][0], k)\n",
    "                    \n",
    "            yield Xb, yb\n",
    "            \n",
    "#         yield inputs[excerpt], targets[excerpt]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded /home/d/patches//ipmlb/ in 0.000614881515503 seconds.\n"
     ]
    }
   ],
   "source": [
    "PATCH_PATH = ('ipmlb')\n",
    "X_train, y_train, X_test, y_test = gp.Patch.load_rgba(PATCH_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_val = X_train[-X_train.shape[0]/4:]\n",
    "y_val = y_train[-X_train.shape[0]/4:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train2 = X_train[:-X_train.shape[0]/4]\n",
    "y_train2 = y_train[:-X_train.shape[0]/4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "n=2\n",
    "num_filters=64\n",
    "num_epochs=200\n",
    "cudnn='yes'\n",
    "print(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building model and compiling functions...\n",
      "NNN 2\n",
      "new\n",
      "adding dropout\n",
      "new\n",
      "adding dropout\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Prepare Theano variables for inputs and targets\n",
    "input_var = T.tensor4('inputs')\n",
    "target_var = T.ivector('targets')\n",
    "\n",
    "# Create neural network model (depending on first command line parameter)\n",
    "print(\"Building model and compiling functions...\")\n",
    "network = build_cnn(input_var, n, num_filters, cudnn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with np.load('/home/d/resnet2.npz') as f:\n",
    "    param_values = [f['arr_%d' % i] for i in range(len(f.files))]\n",
    "    lasagne.layers.set_all_param_values(network, param_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_prediction = lasagne.layers.get_output(network, deterministic=True)\n",
    "test_loss = lasagne.objectives.categorical_crossentropy(test_prediction,\n",
    "                                                        target_var)\n",
    "test_loss = test_loss.mean()\n",
    "# As a bonus, also create an expression for the classification accuracy:\n",
    "test_acc = T.mean(T.eq(T.argmax(test_prediction, axis=1), target_var),\n",
    "                  dtype=theano.config.floatX)\n",
    "\n",
    "pred_fn = theano.function([input_var, target_var], [test_prediction, test_loss, test_acc])\n",
    "pred2_fn = theano.function([input_var], [test_prediction])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fcb616b9390>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP4AAAD/CAYAAADRymv0AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nrkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fcb617472d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "imshow(X_test[0][3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "6000\n",
      "7000\n",
      "8000\n",
      "9000\n",
      "10000\n",
      "11000\n",
      "12000\n",
      "13000\n",
      "14000\n",
      "15000\n",
      "16000\n",
      "17000\n"
     ]
    }
   ],
   "source": [
    "all_preds = []\n",
    "for i,p in enumerate(X_test):\n",
    "    pred = pred2_fn(p.reshape(1,4,75,75))\n",
    "    all_preds.append(pred[0][:,1][0].astype(np.uint8))\n",
    "    if i % 1000 == 0:\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15429"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, accuracy_score, roc_curve, auc, precision_recall_fscore_support, f1_score, precision_recall_curve, average_precision_score, zero_one_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.51      0.99      0.68      8780\n",
      "          1       0.90      0.06      0.11      8780\n",
      "\n",
      "avg / total       0.71      0.53      0.39     17560\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, all_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.51      0.99      0.68      8780\n",
      "          1       0.89      0.05      0.10      8780\n",
      "\n",
      "avg / total       0.70      0.52      0.39     17560\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, all_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 1, 0, 0], dtype=uint8)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred[0][:,1].astype(np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1,\n",
       "       0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0,\n",
       "       0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0,\n",
       "       1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0,\n",
       "       0, 0, 0, 1, 0, 1, 1, 1], dtype=uint8)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test[0:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.], dtype=float32)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.round(pred[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
