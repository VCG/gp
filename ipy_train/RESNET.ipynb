{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using gpu device 0: GeForce GTX TITAN (CNMeM is disabled, CuDNN 4007)\n",
      "/home/d/nolearn/local/lib/python2.7/site-packages/matplotlib/font_manager.py:273: UserWarning: Matplotlib is building the font cache using fc-list. This may take a moment.\n",
      "  warnings.warn('Matplotlib is building the font cache using fc-list. This may take a moment.')\n",
      "/home/d/nolearn/local/lib/python2.7/site-packages/theano/tensor/signal/downsample.py:6: UserWarning: downsample module has been moved to the theano.tensor.signal.pool module.\n",
      "  \"downsample module has been moved to the theano.tensor.signal.pool module.\")\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import time\n",
    "import string\n",
    "\n",
    "import numpy as np\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "\n",
    "import os; import sys; sys.path.append('..')\n",
    "import gp\n",
    "\n",
    "import gp.nets as nets\n",
    "import gp.nets.BatchNormLayer as BatchNormLayer\n",
    "\n",
    "import lasagne\n",
    "\n",
    "sys.setrecursionlimit(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# helper function for projection_b\n",
    "def ceildiv(a, b):\n",
    "    return -(-a // b)\n",
    "\n",
    "def build_cnn(input_var=None, n=1, num_filters=8, cudnn='no'):\n",
    "    import lasagne # For some odd reason it can't read the global import, please PR/Issue if you know why\n",
    "    projection_type = 'B'\n",
    "    # Setting up layers\n",
    "    if cudnn == 'yes':\n",
    "        import lasagne.layers.dnn\n",
    "        conv = lasagne.layers.dnn.Conv2DDNNLayer # cuDNN\n",
    "    else:\n",
    "        conv = lasagne.layers.Conv2DLayer\n",
    "    nonlin = lasagne.nonlinearities.rectify\n",
    "    nonlin_layer = lasagne.layers.NonlinearityLayer\n",
    "    sumlayer = lasagne.layers.ElemwiseSumLayer\n",
    "    #batchnorm = BatchNormLayer.BatchNormLayer\n",
    "    batchnorm = lasagne.layers.BatchNormLayer\n",
    "\n",
    "    # Setting the projection type for when reducing height/width\n",
    "    # and increasing dimensions.\n",
    "    # Default is 'B' as B performs slightly better\n",
    "    # and A requires newer version of lasagne with ExpressionLayer\n",
    "    projection_type = 'B'\n",
    "    if projection_type == 'A':\n",
    "        expression = lasagne.layers.ExpressionLayer\n",
    "        pad = lasagne.layers.PadLayer\n",
    "\n",
    "    if projection_type == 'A':\n",
    "        # option A for projection as described in paper\n",
    "        # (should perform slightly worse than B)\n",
    "        def projection(l_inp):\n",
    "            n_filters = l_inp.output_shape[1]*2\n",
    "            l = expression(l_inp, lambda X: X[:, :, ::2, ::2], lambda s: (s[0], s[1], ceildiv(s[2], 2), ceildiv(s[3], 2)))\n",
    "            l = pad(l, [n_filters//4,0,0], batch_ndim=1)\n",
    "            return l\n",
    "\n",
    "    if projection_type == 'B':\n",
    "        # option B for projection as described in paper\n",
    "        def projection(l_inp):\n",
    "            # twice normal channels when projecting!\n",
    "            n_filters = l_inp.output_shape[1]*2 \n",
    "            l = conv(l_inp, num_filters=n_filters, filter_size=(1, 1),\n",
    "                     stride=(2, 2), nonlinearity=None, pad='same', b=None)\n",
    "            l = batchnorm(l)\n",
    "            return l\n",
    "\n",
    "    # helper function to handle filters/strides when increasing dims\n",
    "    def filters_increase_dims(l, increase_dims):\n",
    "        in_num_filters = l.output_shape[1]\n",
    "        if increase_dims:\n",
    "            first_stride = (2, 2)\n",
    "            out_num_filters = in_num_filters*2\n",
    "        else:\n",
    "            first_stride = (1, 1)\n",
    "            out_num_filters = in_num_filters\n",
    " \n",
    "        return out_num_filters, first_stride\n",
    "\n",
    "    # block as described and used in cifar in the original paper:\n",
    "    # http://arxiv.org/abs/1512.03385\n",
    "    def res_block_v1(l_inp, nonlinearity=nonlin, increase_dim=False):\n",
    "        # first figure filters/strides\n",
    "        n_filters, first_stride = filters_increase_dims(l_inp, increase_dim)\n",
    "        # conv -> BN -> nonlin -> conv -> BN -> sum -> nonlin\n",
    "        l = conv(l_inp, num_filters=n_filters, filter_size=(3, 3),\n",
    "                 stride=first_stride, nonlinearity=None, pad='same',\n",
    "                 W=lasagne.init.HeNormal(gain='relu'))\n",
    "        l = batchnorm(l)\n",
    "        l = nonlin_layer(l, nonlinearity=nonlin)\n",
    "        l = conv(l, num_filters=n_filters, filter_size=(3, 3),\n",
    "                 stride=(1, 1), nonlinearity=None, pad='same',\n",
    "                 W=lasagne.init.HeNormal(gain='relu'))\n",
    "        l = batchnorm(l)\n",
    "        if increase_dim:\n",
    "            # Use projection (A, B) as described in paper\n",
    "            p = projection(l_inp)\n",
    "        else:\n",
    "            # Identity shortcut\n",
    "            p = l_inp\n",
    "        l = sumlayer([l, p])\n",
    "        l = nonlin_layer(l, nonlinearity=nonlin)\n",
    "        return l\n",
    "\n",
    "    # block as described in second paper on the subject (by same authors):\n",
    "    # http://arxiv.org/abs/1603.05027\n",
    "    def res_block_v2(l_inp, nonlinearity=nonlin, increase_dim=False):\n",
    "        # first figure filters/strides\n",
    "        n_filters, first_stride = filters_increase_dims(l_inp, increase_dim)\n",
    "        # BN -> nonlin -> conv -> BN -> nonlin -> conv -> sum\n",
    "        l = batchnorm(l_inp)\n",
    "        l = nonlin_layer(l, nonlinearity=nonlin)\n",
    "        l = conv(l, num_filters=n_filters, filter_size=(3, 3),\n",
    "                 stride=first_stride, nonlinearity=None, pad='same',\n",
    "                 W=lasagne.init.HeNormal(gain='relu'))\n",
    "        l = batchnorm(l)\n",
    "        l = nonlin_layer(l, nonlinearity=nonlin)\n",
    "        l = conv(l, num_filters=n_filters, filter_size=(3, 3),\n",
    "                 stride=(1, 1), nonlinearity=None, pad='same',\n",
    "                 W=lasagne.init.HeNormal(gain='relu'))\n",
    "        if increase_dim:\n",
    "            # Use projection (A, B) as described in paper\n",
    "            p = projection(l_inp)\n",
    "        else:\n",
    "            # Identity shortcut\n",
    "            p = l_inp\n",
    "        l = sumlayer([l, p])\n",
    "        return l\n",
    "\n",
    "    def bottleneck_block(l_inp, nonlinearity=nonlin, increase_dim=False):\n",
    "        # first figure filters/strides\n",
    "        n_filters, first_stride = filters_increase_dims(l_inp, increase_dim)\n",
    "        # conv -> BN -> nonlin -> conv -> BN -> nonlin -> conv -> BN -> sum\n",
    "        # -> nonlin\n",
    "        # first make the bottleneck, scale the filters ..!\n",
    "        scale = 4 # as per bottleneck architecture used in paper\n",
    "        scaled_filters = n_filters/scale\n",
    "        l = conv(l_inp, num_filters=scaled_filters, filter_size=(1, 1),\n",
    "                 stride=first_stride, nonlinearity=None, pad='same',\n",
    "                 W=lasagne.init.HeNormal(gain='relu'))\n",
    "        l = batchnorm(l)\n",
    "        l = nonlin_layer(l, nonlinearity=nonlin)\n",
    "        l = conv(l, num_filters=scaled_filters, filter_size=(3, 3),\n",
    "                 stride=(1, 1), nonlinearity=None, pad='same',\n",
    "                 W=lasagne.init.HeNormal(gain='relu'))\n",
    "        l = batchnorm(l)\n",
    "        l = nonlin_layer(l, nonlinearity=nonlin)\n",
    "        l = conv(l, num_filters=n_filters, filter_size=(1, 1),\n",
    "                 stride=(1, 1), nonlinearity=None, pad='same',\n",
    "                 W=lasagne.init.HeNormal(gain='relu'))\n",
    "        if increase_dim:\n",
    "            # Use projection (A, B) as described in paper\n",
    "            p = projection(l_inp)\n",
    "        else:\n",
    "            # Identity shortcut\n",
    "            p = l_inp\n",
    "        l = sumlayer([l, p])\n",
    "        l = nonlin_layer(l, nonlinearity=nonlin)\n",
    "        return l\n",
    "\n",
    "    # Bottleneck architecture with more efficiency (the post with Kaiming He's response)\n",
    "    # https://www.reddit.com/r/MachineLearning/comments/3ywi6x/deep_residual_learning_the_bottleneck/ \n",
    "    def bottleneck_block_fast(l_inp, nonlinearity=nonlin, increase_dim=False):\n",
    "        # first figure filters/strides\n",
    "        n_filters, last_stride = filters_increase_dims(l_inp, increase_dim)\n",
    "        # conv -> BN -> nonlin -> conv -> BN -> nonlin -> conv -> BN -> sum\n",
    "        # -> nonlin\n",
    "        # first make the bottleneck, scale the filters ..!\n",
    "        scale = 4 # as per bottleneck architecture used in paper\n",
    "        scaled_filters = n_filters/scale\n",
    "        l = conv(l_inp, num_filters=scaled_filters, filter_size=(1, 1),\n",
    "                 stride=(1, 1), nonlinearity=None, pad='same',\n",
    "                 W=lasagne.init.HeNormal(gain='relu'))\n",
    "        l = batchnorm(l)\n",
    "        l = nonlin_layer(l, nonlinearity=nonlin)\n",
    "        l = conv(l, num_filters=scaled_filters, filter_size=(3, 3),\n",
    "                 stride=(1, 1), nonlinearity=None, pad='same',\n",
    "                 W=lasagne.init.HeNormal(gain='relu'))\n",
    "        l = batchnorm(l)\n",
    "        l = nonlin_layer(l, nonlinearity=nonlin)\n",
    "        l = conv(l, num_filters=n_filters, filter_size=(1, 1),\n",
    "                 stride=last_stride, nonlinearity=None, pad='same',\n",
    "                 W=lasagne.init.HeNormal(gain='relu'))\n",
    "        if increase_dim:\n",
    "            # Use projection (A, B) as described in paper\n",
    "            p = projection(l_inp)\n",
    "        else:\n",
    "            # Identity shortcut\n",
    "            p = l_inp\n",
    "        l = sumlayer([l, p])\n",
    "        l = nonlin_layer(l, nonlinearity=nonlin)\n",
    "        return l\n",
    "       \n",
    "    res_block = res_block_v1\n",
    "\n",
    "    # Stacks the residual blocks, makes it easy to model size of architecture with int n   \n",
    "    def blockstack(l, n, nonlinearity=nonlin):\n",
    "        for _ in range(n):\n",
    "            l = res_block(l, nonlinearity=nonlin)\n",
    "        return l\n",
    "\n",
    "    # Building the network\n",
    "    l_in = lasagne.layers.InputLayer(shape=(None, 4, 75, 75),\n",
    "                                        input_var=input_var)\n",
    "    # First layer! just a plain convLayer\n",
    "    l1 = conv(l_in, num_filters=num_filters, stride=(1, 1),\n",
    "              filter_size=(3, 3), nonlinearity=None, pad='same')\n",
    "    l1 = batchnorm(l1)\n",
    "    l1 = nonlin_layer(l1, nonlinearity=nonlin)\n",
    "\n",
    "    # Stacking bottlenecks and increasing dims! (while reducing shape size)\n",
    "    l1_bs = blockstack(l1, n=n)\n",
    "    l1_id = res_block(l1_bs, increase_dim=True)\n",
    "\n",
    "    l2_bs = blockstack(l1_id, n=n)\n",
    "    l2_id = res_block(l2_bs, increase_dim=True)\n",
    "\n",
    "    l3_bs = blockstack(l2_id, n=n)\n",
    "\n",
    "    # And, finally, the 10-unit output layer:\n",
    "    network = lasagne.layers.DenseLayer(\n",
    "            l3_bs,\n",
    "            num_units=2,\n",
    "            nonlinearity=lasagne.nonlinearities.softmax)\n",
    "\n",
    "    return network\n",
    "\n",
    "\n",
    "# ############################# Batch iterator ###############################\n",
    "# This is just a simple helper function iterating over training data in\n",
    "# mini-batches of a particular size, optionally in random order. It assumes\n",
    "# data is available as numpy arrays. For big datasets, you could load numpy\n",
    "# arrays as memory-mapped files (np.load(..., mmap_mode='r')), or write your\n",
    "# own custom data iteration function. For small datasets, you can also copy\n",
    "# them to GPU at once for slightly improved performance. This would involve\n",
    "# several changes in the main program, though, and is not demonstrated here.\n",
    "\n",
    "def iterate_minibatches(inputs, targets, batchsize, shuffle=False):\n",
    "    assert len(inputs) == len(targets)\n",
    "    if shuffle:\n",
    "        indices = np.arange(len(inputs))\n",
    "        np.random.shuffle(indices)\n",
    "    for start_idx in range(0, len(inputs) - batchsize + 1, batchsize):\n",
    "        if shuffle:\n",
    "            excerpt = indices[start_idx:start_idx + batchsize]\n",
    "        else:\n",
    "            excerpt = slice(start_idx, start_idx + batchsize)\n",
    "            \n",
    "            Xb = inputs[excerpt]\n",
    "            yb = targets[excerpt]\n",
    "            \n",
    "            Xb = Xb - .5\n",
    "            \n",
    "            k_s = np.array([0,1,2,3],dtype=np.uint8)\n",
    "            for i in range(len(Xb)):\n",
    "                k = np.random.choice(k_s)\n",
    "                for j in range(Xb.shape[1]):\n",
    "                    Xb[j][0] = np.rot90(Xb[j][0], k)\n",
    "                    \n",
    "            yield Xb, yb\n",
    "            \n",
    "#         yield inputs[excerpt], targets[excerpt]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded /home/d/patches//ipmlb/ in 0.161231994629 seconds.\n"
     ]
    }
   ],
   "source": [
    "PATCH_PATH = ('ipmlb')\n",
    "X_train, y_train, X_test, y_test = gp.Patch.load_rgba(PATCH_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_val = X_train[-X_train.shape[0]/4:]\n",
    "y_val = y_train[-X_train.shape[0]/4:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train2 = X_train[:-X_train.shape[0]/4]\n",
    "y_train2 = y_train[:-X_train.shape[0]/4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n=1\n",
    "num_filters=16\n",
    "num_epochs=200\n",
    "cudnn='yes'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building model and compiling functions...\n",
      "  layer output shapes:\n",
      "    InputLayer                       (None, 4, 75, 75)\n",
      "    Conv2DDNNLayer                   (None, 16, 75, 75)\n",
      "    BatchNormLayer                   (None, 16, 75, 75)\n",
      "    NonlinearityLayer                (None, 16, 75, 75)\n",
      "    Conv2DDNNLayer                   (None, 16, 75, 75)\n",
      "    BatchNormLayer                   (None, 16, 75, 75)\n",
      "    NonlinearityLayer                (None, 16, 75, 75)\n",
      "    Conv2DDNNLayer                   (None, 16, 75, 75)\n",
      "    BatchNormLayer                   (None, 16, 75, 75)\n",
      "    ElemwiseSumLayer                 (None, 16, 75, 75)\n",
      "    NonlinearityLayer                (None, 16, 75, 75)\n",
      "    Conv2DDNNLayer                   (None, 32, 38, 38)\n",
      "    BatchNormLayer                   (None, 32, 38, 38)\n",
      "    NonlinearityLayer                (None, 32, 38, 38)\n",
      "    Conv2DDNNLayer                   (None, 32, 38, 38)\n",
      "    BatchNormLayer                   (None, 32, 38, 38)\n",
      "    Conv2DDNNLayer                   (None, 32, 38, 38)\n",
      "    BatchNormLayer                   (None, 32, 38, 38)\n",
      "    ElemwiseSumLayer                 (None, 32, 38, 38)\n",
      "    NonlinearityLayer                (None, 32, 38, 38)\n",
      "    Conv2DDNNLayer                   (None, 32, 38, 38)\n",
      "    BatchNormLayer                   (None, 32, 38, 38)\n",
      "    NonlinearityLayer                (None, 32, 38, 38)\n",
      "    Conv2DDNNLayer                   (None, 32, 38, 38)\n",
      "    BatchNormLayer                   (None, 32, 38, 38)\n",
      "    ElemwiseSumLayer                 (None, 32, 38, 38)\n",
      "    NonlinearityLayer                (None, 32, 38, 38)\n",
      "    Conv2DDNNLayer                   (None, 64, 19, 19)\n",
      "    BatchNormLayer                   (None, 64, 19, 19)\n",
      "    NonlinearityLayer                (None, 64, 19, 19)\n",
      "    Conv2DDNNLayer                   (None, 64, 19, 19)\n",
      "    BatchNormLayer                   (None, 64, 19, 19)\n",
      "    Conv2DDNNLayer                   (None, 64, 19, 19)\n",
      "    BatchNormLayer                   (None, 64, 19, 19)\n",
      "    ElemwiseSumLayer                 (None, 64, 19, 19)\n",
      "    NonlinearityLayer                (None, 64, 19, 19)\n",
      "    Conv2DDNNLayer                   (None, 64, 19, 19)\n",
      "    BatchNormLayer                   (None, 64, 19, 19)\n",
      "    NonlinearityLayer                (None, 64, 19, 19)\n",
      "    Conv2DDNNLayer                   (None, 64, 19, 19)\n",
      "    BatchNormLayer                   (None, 64, 19, 19)\n",
      "    ElemwiseSumLayer                 (None, 64, 19, 19)\n",
      "    NonlinearityLayer                (None, 64, 19, 19)\n",
      "    DenseLayer                       (None, 2)\n",
      "  no. of InputLayers: 1\n",
      "  no. of Conv2DLayers: 13\n",
      "  no. of BatchNormLayers: 13\n",
      "  no. of NonlinearityLayers: 11\n",
      "  no. of DenseLayers: 1\n",
      "  no. of ElemwiseSumLayers: 5\n",
      "  no. of Unknown Layers: 0\n",
      "  total no. of layers: 44\n",
      "  no. of parameters: 217778\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Prepare Theano variables for inputs and targets\n",
    "input_var = T.tensor4('inputs')\n",
    "target_var = T.ivector('targets')\n",
    "\n",
    "# Create neural network model (depending on first command line parameter)\n",
    "print(\"Building model and compiling functions...\")\n",
    "network = build_cnn(input_var, n, num_filters, cudnn)\n",
    "all_layers = lasagne.layers.get_all_layers(network)\n",
    "num_params = lasagne.layers.count_params(network)\n",
    "num_conv = 0\n",
    "num_nonlin = 0\n",
    "num_input = 0\n",
    "num_batchnorm = 0\n",
    "num_elemsum = 0\n",
    "num_dense = 0\n",
    "num_unknown = 0\n",
    "print(\"  layer output shapes:\")\n",
    "for layer in all_layers:\n",
    "    name = string.ljust(layer.__class__.__name__, 32)\n",
    "    print(\"    %s %s\" %(name, lasagne.layers.get_output_shape(layer)))\n",
    "    if \"Conv2D\" in name:\n",
    "        num_conv += 1\n",
    "    elif \"NonlinearityLayer\" in name:\n",
    "        num_nonlin += 1\n",
    "    elif \"InputLayer\" in name:\n",
    "        num_input += 1\n",
    "    elif \"BatchNormLayer\" in name:\n",
    "        num_batchnorm += 1\n",
    "    elif \"ElemwiseSumLayer\" in name:\n",
    "        num_elemsum += 1\n",
    "    elif \"DenseLayer\" in name:\n",
    "        num_dense += 1\n",
    "    else:\n",
    "        num_unknown += 1\n",
    "print(\"  no. of InputLayers: %d\" % num_input)\n",
    "print(\"  no. of Conv2DLayers: %d\" % num_conv)\n",
    "print(\"  no. of BatchNormLayers: %d\" % num_batchnorm)\n",
    "print(\"  no. of NonlinearityLayers: %d\" % num_nonlin)\n",
    "print(\"  no. of DenseLayers: %d\" % num_dense)\n",
    "print(\"  no. of ElemwiseSumLayers: %d\" % num_elemsum)\n",
    "print(\"  no. of Unknown Layers: %d\" % num_unknown)\n",
    "print(\"  total no. of layers: %d\" % len(all_layers))\n",
    "print(\"  no. of parameters: %d\" % num_params)\n",
    "# Create a loss expression for training, i.e., a scalar objective we want\n",
    "# to minimize (for our multi-class problem, it is the cross-entropy loss):\n",
    "prediction = lasagne.layers.get_output(network)\n",
    "loss = lasagne.objectives.categorical_crossentropy(prediction, target_var)\n",
    "loss = loss.mean()\n",
    "# We could add some weight decay as well here, see lasagne.regularization.\n",
    "\n",
    "# Create update expressions for training, i.e., how to modify the\n",
    "# parameters at each training step. Here, we'll use Stochastic Gradient\n",
    "# Descent (SGD) with Nesterov momentum, but Lasagne offers plenty more.\n",
    "params = lasagne.layers.get_all_params(network, trainable=True)\n",
    "\n",
    "# several learning rates for low initial learning rates and\n",
    "# learning rate anealing (id is epoch)\n",
    "learning_rate_schedule = {\n",
    "0: 0.0001, # low initial learning rate as described in paper\n",
    "2: 0.01,\n",
    "100: 0.001,\n",
    "150: 0.0001\n",
    "}\n",
    "\n",
    "\n",
    "learning_rate = theano.shared(np.float32(learning_rate_schedule[0]))\n",
    "\n",
    "updates = lasagne.updates.nesterov_momentum(\n",
    "        loss, params, learning_rate=learning_rate, momentum=0.9)\n",
    "\n",
    "# Create a loss expression for validation/testing. The crucial difference\n",
    "# here is that we do a deterministic forward pass through the network,\n",
    "# disabling dropout layers.\n",
    "test_prediction = lasagne.layers.get_output(network, deterministic=True)\n",
    "test_loss = lasagne.objectives.categorical_crossentropy(test_prediction,\n",
    "                                                        target_var)\n",
    "test_loss = test_loss.mean()\n",
    "# As a bonus, also create an expression for the classification accuracy:\n",
    "test_acc = T.mean(T.eq(T.argmax(test_prediction, axis=1), target_var),\n",
    "                  dtype=theano.config.floatX)\n",
    "\n",
    "# Compile a function performing a training step on a mini-batch (by giving\n",
    "# the updates dictionary) and returning the corresponding training loss:\n",
    "train_fn = theano.function([input_var, target_var], loss, updates=updates)\n",
    "\n",
    "# Compile a second function computing the validation loss and accuracy:\n",
    "val_fn = theano.function([input_var, target_var], [test_loss, test_acc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n",
      " setting learning rate to 0.0001000\n",
      "Epoch 1 of 500 took 273.906s\n",
      "  training loss:\t\t0.525972\n",
      "  validation loss:\t\t0.411783\n",
      "  validation accuracy:\t\t82.94 %\n",
      "Epoch 2 of 500 took 274.976s\n",
      "  training loss:\t\t0.364997\n",
      "  validation loss:\t\t0.341984\n",
      "  validation accuracy:\t\t86.23 %\n",
      " setting learning rate to 0.0100000\n",
      "Epoch 3 of 500 took 274.992s\n",
      "  training loss:\t\t0.603202\n",
      "  validation loss:\t\t0.231432\n",
      "  validation accuracy:\t\t90.50 %\n",
      "Epoch 4 of 500 took 274.845s\n",
      "  training loss:\t\t0.214063\n",
      "  validation loss:\t\t0.209323\n",
      "  validation accuracy:\t\t91.97 %\n",
      "Epoch 5 of 500 took 274.957s\n",
      "  training loss:\t\t0.200622\n",
      "  validation loss:\t\t0.199121\n",
      "  validation accuracy:\t\t92.53 %\n",
      "Epoch 6 of 500 took 274.935s\n",
      "  training loss:\t\t0.190989\n",
      "  validation loss:\t\t0.195666\n",
      "  validation accuracy:\t\t92.80 %\n",
      "Epoch 7 of 500 took 274.876s\n",
      "  training loss:\t\t0.182546\n",
      "  validation loss:\t\t0.193178\n",
      "  validation accuracy:\t\t92.82 %\n",
      "Epoch 8 of 500 took 275.007s\n",
      "  training loss:\t\t0.174501\n",
      "  validation loss:\t\t0.181994\n",
      "  validation accuracy:\t\t93.24 %\n",
      "Epoch 9 of 500 took 274.913s\n",
      "  training loss:\t\t0.166674\n",
      "  validation loss:\t\t0.177469\n",
      "  validation accuracy:\t\t93.35 %\n",
      "Epoch 10 of 500 took 274.961s\n",
      "  training loss:\t\t0.158963\n",
      "  validation loss:\t\t0.173296\n",
      "  validation accuracy:\t\t93.47 %\n",
      "Epoch 11 of 500 took 274.939s\n",
      "  training loss:\t\t0.151424\n",
      "  validation loss:\t\t0.170585\n",
      "  validation accuracy:\t\t93.66 %\n",
      "Epoch 12 of 500 took 274.959s\n",
      "  training loss:\t\t0.143806\n",
      "  validation loss:\t\t0.166776\n",
      "  validation accuracy:\t\t93.77 %\n",
      "Epoch 13 of 500 took 274.940s\n",
      "  training loss:\t\t0.136393\n",
      "  validation loss:\t\t0.167214\n",
      "  validation accuracy:\t\t93.70 %\n",
      "Epoch 14 of 500 took 275.084s\n",
      "  training loss:\t\t0.129090\n",
      "  validation loss:\t\t0.162699\n",
      "  validation accuracy:\t\t93.94 %\n",
      "Epoch 15 of 500 took 275.135s\n",
      "  training loss:\t\t0.121785\n",
      "  validation loss:\t\t0.162171\n",
      "  validation accuracy:\t\t94.01 %\n",
      "Epoch 16 of 500 took 274.811s\n",
      "  training loss:\t\t0.114568\n",
      "  validation loss:\t\t0.160853\n",
      "  validation accuracy:\t\t93.92 %\n",
      "Epoch 17 of 500 took 274.952s\n",
      "  training loss:\t\t0.107391\n",
      "  validation loss:\t\t0.157906\n",
      "  validation accuracy:\t\t94.15 %\n",
      "Epoch 18 of 500 took 274.957s\n",
      "  training loss:\t\t0.100383\n",
      "  validation loss:\t\t0.156327\n",
      "  validation accuracy:\t\t94.34 %\n",
      "Epoch 19 of 500 took 274.838s\n",
      "  training loss:\t\t0.093513\n",
      "  validation loss:\t\t0.159147\n",
      "  validation accuracy:\t\t94.26 %\n",
      "Epoch 20 of 500 took 274.856s\n",
      "  training loss:\t\t0.087196\n",
      "  validation loss:\t\t0.162578\n",
      "  validation accuracy:\t\t94.38 %\n",
      "Epoch 21 of 500 took 274.940s\n",
      "  training loss:\t\t0.081014\n",
      "  validation loss:\t\t0.155076\n",
      "  validation accuracy:\t\t94.36 %\n",
      "Epoch 22 of 500 took 274.782s\n",
      "  training loss:\t\t0.075149\n",
      "  validation loss:\t\t0.148147\n",
      "  validation accuracy:\t\t94.73 %\n",
      "Epoch 23 of 500 took 274.919s\n",
      "  training loss:\t\t0.069111\n",
      "  validation loss:\t\t0.145106\n",
      "  validation accuracy:\t\t94.96 %\n",
      "Epoch 24 of 500 took 275.030s\n",
      "  training loss:\t\t0.063785\n",
      "  validation loss:\t\t0.140536\n",
      "  validation accuracy:\t\t95.24 %\n",
      "Epoch 25 of 500 took 274.778s\n",
      "  training loss:\t\t0.058663\n",
      "  validation loss:\t\t0.134384\n",
      "  validation accuracy:\t\t95.50 %\n",
      "Epoch 26 of 500 took 275.080s\n",
      "  training loss:\t\t0.054045\n",
      "  validation loss:\t\t0.135181\n",
      "  validation accuracy:\t\t95.38 %\n",
      "Epoch 27 of 500 took 274.826s\n",
      "  training loss:\t\t0.049663\n",
      "  validation loss:\t\t0.131245\n",
      "  validation accuracy:\t\t95.59 %\n",
      "Epoch 28 of 500 took 274.875s\n",
      "  training loss:\t\t0.045703\n",
      "  validation loss:\t\t0.134718\n",
      "  validation accuracy:\t\t95.58 %\n",
      "Epoch 29 of 500 took 274.803s\n",
      "  training loss:\t\t0.041939\n",
      "  validation loss:\t\t0.134026\n",
      "  validation accuracy:\t\t95.70 %\n",
      "Epoch 30 of 500 took 274.235s\n",
      "  training loss:\t\t0.038375\n",
      "  validation loss:\t\t0.132246\n",
      "  validation accuracy:\t\t95.68 %\n",
      "Epoch 31 of 500 took 274.234s\n",
      "  training loss:\t\t0.034664\n",
      "  validation loss:\t\t0.134482\n",
      "  validation accuracy:\t\t95.49 %\n",
      "Epoch 32 of 500 took 274.243s\n",
      "  training loss:\t\t0.031810\n",
      "  validation loss:\t\t0.131226\n",
      "  validation accuracy:\t\t95.66 %\n",
      "Epoch 33 of 500 took 274.193s\n",
      "  training loss:\t\t0.029503\n",
      "  validation loss:\t\t0.116156\n",
      "  validation accuracy:\t\t96.42 %\n",
      "Epoch 34 of 500 took 274.230s\n",
      "  training loss:\t\t0.026646\n",
      "  validation loss:\t\t0.110445\n",
      "  validation accuracy:\t\t96.62 %\n",
      "Epoch 35 of 500 took 274.234s\n",
      "  training loss:\t\t0.024195\n",
      "  validation loss:\t\t0.109905\n",
      "  validation accuracy:\t\t96.74 %\n",
      "Epoch 36 of 500 took 274.229s\n",
      "  training loss:\t\t0.022112\n",
      "  validation loss:\t\t0.109599\n",
      "  validation accuracy:\t\t96.92 %\n",
      "Epoch 37 of 500 took 274.231s\n",
      "  training loss:\t\t0.020033\n",
      "  validation loss:\t\t0.114158\n",
      "  validation accuracy:\t\t96.73 %\n",
      "Epoch 38 of 500 took 274.246s\n",
      "  training loss:\t\t0.018206\n",
      "  validation loss:\t\t0.112010\n",
      "  validation accuracy:\t\t96.87 %\n",
      "Epoch 39 of 500 took 274.293s\n",
      "  training loss:\t\t0.017151\n",
      "  validation loss:\t\t0.105058\n",
      "  validation accuracy:\t\t97.18 %\n",
      "Epoch 40 of 500 took 274.195s\n",
      "  training loss:\t\t0.015018\n",
      "  validation loss:\t\t0.113442\n",
      "  validation accuracy:\t\t97.00 %\n",
      "Epoch 41 of 500 took 274.228s\n",
      "  training loss:\t\t0.014090\n",
      "  validation loss:\t\t0.129953\n",
      "  validation accuracy:\t\t96.72 %\n",
      "Epoch 42 of 500 took 274.238s\n",
      "  training loss:\t\t0.012604\n",
      "  validation loss:\t\t0.122794\n",
      "  validation accuracy:\t\t96.87 %\n",
      "Epoch 43 of 500 took 274.254s\n",
      "  training loss:\t\t0.011587\n",
      "  validation loss:\t\t0.139061\n",
      "  validation accuracy:\t\t96.59 %\n",
      "Epoch 44 of 500 took 274.197s\n",
      "  training loss:\t\t0.009657\n",
      "  validation loss:\t\t0.151959\n",
      "  validation accuracy:\t\t96.18 %\n",
      "Epoch 45 of 500 took 274.280s\n",
      "  training loss:\t\t0.008691\n",
      "  validation loss:\t\t0.137810\n",
      "  validation accuracy:\t\t96.65 %\n",
      "Epoch 46 of 500 took 274.182s\n",
      "  training loss:\t\t0.007600\n",
      "  validation loss:\t\t0.123054\n",
      "  validation accuracy:\t\t97.10 %\n",
      "Epoch 47 of 500 took 274.228s\n",
      "  training loss:\t\t0.006489\n",
      "  validation loss:\t\t0.095529\n",
      "  validation accuracy:\t\t98.03 %\n",
      "Epoch 48 of 500 took 274.227s\n",
      "  training loss:\t\t0.005587\n",
      "  validation loss:\t\t0.087004\n",
      "  validation accuracy:\t\t98.27 %\n",
      "Epoch 49 of 500 took 283.544s\n",
      "  training loss:\t\t0.004895\n",
      "  validation loss:\t\t0.084536\n",
      "  validation accuracy:\t\t98.37 %\n",
      "Epoch 50 of 500 took 274.315s\n",
      "  training loss:\t\t0.003969\n",
      "  validation loss:\t\t0.082435\n",
      "  validation accuracy:\t\t98.49 %\n",
      "Epoch 51 of 500 took 274.895s\n",
      "  training loss:\t\t0.003324\n",
      "  validation loss:\t\t0.081999\n",
      "  validation accuracy:\t\t98.49 %\n",
      "Epoch 52 of 500 took 275.043s\n",
      "  training loss:\t\t0.002745\n",
      "  validation loss:\t\t0.080958\n",
      "  validation accuracy:\t\t98.56 %\n",
      "Epoch 53 of 500 took 274.832s\n",
      "  training loss:\t\t0.002297\n",
      "  validation loss:\t\t0.080562\n",
      "  validation accuracy:\t\t98.64 %\n",
      "Epoch 54 of 500 took 274.881s\n",
      "  training loss:\t\t0.002008\n",
      "  validation loss:\t\t0.080135\n",
      "  validation accuracy:\t\t98.65 %\n",
      "Epoch 55 of 500 took 274.816s\n",
      "  training loss:\t\t0.001766\n",
      "  validation loss:\t\t0.079283\n",
      "  validation accuracy:\t\t98.71 %\n",
      "Epoch 56 of 500 took 275.066s\n",
      "  training loss:\t\t0.001591\n",
      "  validation loss:\t\t0.078649\n",
      "  validation accuracy:\t\t98.76 %\n",
      "Epoch 57 of 500 took 274.595s\n",
      "  training loss:\t\t0.001448\n",
      "  validation loss:\t\t0.078596\n",
      "  validation accuracy:\t\t98.78 %\n",
      "Epoch 58 of 500 took 283.904s\n",
      "  training loss:\t\t0.001337\n",
      "  validation loss:\t\t0.078527\n",
      "  validation accuracy:\t\t98.81 %\n",
      "Epoch 59 of 500 took 275.067s\n",
      "  training loss:\t\t0.001237\n",
      "  validation loss:\t\t0.078694\n",
      "  validation accuracy:\t\t98.83 %\n",
      "Epoch 60 of 500 took 284.265s\n",
      "  training loss:\t\t0.001153\n",
      "  validation loss:\t\t0.079122\n",
      "  validation accuracy:\t\t98.84 %\n",
      "Epoch 61 of 500 took 274.808s\n",
      "  training loss:\t\t0.001076\n",
      "  validation loss:\t\t0.079790\n",
      "  validation accuracy:\t\t98.85 %\n",
      "Epoch 62 of 500 took 276.327s\n",
      "  training loss:\t\t0.001010\n",
      "  validation loss:\t\t0.080642\n",
      "  validation accuracy:\t\t98.85 %\n",
      "Epoch 63 of 500 took 286.318s\n",
      "  training loss:\t\t0.000950\n",
      "  validation loss:\t\t0.081524\n",
      "  validation accuracy:\t\t98.84 %\n",
      "Epoch 64 of 500 took 285.662s\n",
      "  training loss:\t\t0.000898\n",
      "  validation loss:\t\t0.082597\n",
      "  validation accuracy:\t\t98.83 %\n",
      "Epoch 65 of 500 took 292.515s\n",
      "  training loss:\t\t0.000850\n",
      "  validation loss:\t\t0.083504\n",
      "  validation accuracy:\t\t98.83 %\n",
      "Epoch 66 of 500 took 298.238s\n",
      "  training loss:\t\t0.000807\n",
      "  validation loss:\t\t0.084363\n",
      "  validation accuracy:\t\t98.82 %\n",
      "Epoch 67 of 500 took 286.525s\n",
      "  training loss:\t\t0.000768\n",
      "  validation loss:\t\t0.085236\n",
      "  validation accuracy:\t\t98.82 %\n",
      "Epoch 68 of 500 took 286.586s\n",
      "  training loss:\t\t0.000732\n",
      "  validation loss:\t\t0.086014\n",
      "  validation accuracy:\t\t98.81 %\n",
      "Epoch 69 of 500 took 279.426s\n",
      "  training loss:\t\t0.000700\n",
      "  validation loss:\t\t0.086727\n",
      "  validation accuracy:\t\t98.81 %\n",
      "Epoch 70 of 500 took 281.917s\n",
      "  training loss:\t\t0.000671\n",
      "  validation loss:\t\t0.087458\n",
      "  validation accuracy:\t\t98.81 %\n",
      "Epoch 71 of 500 took 292.987s\n",
      "  training loss:\t\t0.000644\n",
      "  validation loss:\t\t0.088091\n",
      "  validation accuracy:\t\t98.80 %\n",
      "Epoch 72 of 500 took 289.481s\n",
      "  training loss:\t\t0.000619\n",
      "  validation loss:\t\t0.088636\n",
      "  validation accuracy:\t\t98.79 %\n",
      "Epoch 73 of 500 took 290.789s\n",
      "  training loss:\t\t0.000596\n",
      "  validation loss:\t\t0.089315\n",
      "  validation accuracy:\t\t98.80 %\n",
      "Epoch 74 of 500 took 286.087s\n",
      "  training loss:\t\t0.000575\n",
      "  validation loss:\t\t0.089846\n",
      "  validation accuracy:\t\t98.80 %\n",
      "Epoch 75 of 500 took 285.556s\n",
      "  training loss:\t\t0.000555\n",
      "  validation loss:\t\t0.090368\n",
      "  validation accuracy:\t\t98.79 %\n",
      "Epoch 76 of 500 took 280.500s\n",
      "  training loss:\t\t0.000537\n",
      "  validation loss:\t\t0.090868\n",
      "  validation accuracy:\t\t98.79 %\n",
      "Epoch 77 of 500 took 287.787s\n",
      "  training loss:\t\t0.000520\n",
      "  validation loss:\t\t0.091207\n",
      "  validation accuracy:\t\t98.79 %\n",
      "Epoch 78 of 500 took 287.568s\n",
      "  training loss:\t\t0.000503\n",
      "  validation loss:\t\t0.091478\n",
      "  validation accuracy:\t\t98.79 %\n",
      "Epoch 79 of 500 took 283.057s\n",
      "  training loss:\t\t0.000489\n",
      "  validation loss:\t\t0.091713\n",
      "  validation accuracy:\t\t98.79 %\n",
      "Epoch 80 of 500 took 284.904s\n",
      "  training loss:\t\t0.000474\n",
      "  validation loss:\t\t0.091850\n",
      "  validation accuracy:\t\t98.79 %\n",
      "Epoch 81 of 500 took 284.965s\n",
      "  training loss:\t\t0.000461\n",
      "  validation loss:\t\t0.091944\n",
      "  validation accuracy:\t\t98.79 %\n",
      "Epoch 82 of 500 took 279.892s\n",
      "  training loss:\t\t0.000448\n",
      "  validation loss:\t\t0.092132\n",
      "  validation accuracy:\t\t98.79 %\n",
      "Epoch 83 of 500 took 285.364s\n",
      "  training loss:\t\t0.000436\n",
      "  validation loss:\t\t0.092168\n",
      "  validation accuracy:\t\t98.80 %\n",
      "Epoch 84 of 500 took 280.957s\n",
      "  training loss:\t\t0.000425\n",
      "  validation loss:\t\t0.092250\n",
      "  validation accuracy:\t\t98.80 %\n",
      "Epoch 85 of 500 took 283.706s\n",
      "  training loss:\t\t0.000414\n",
      "  validation loss:\t\t0.092320\n",
      "  validation accuracy:\t\t98.81 %\n",
      "Epoch 86 of 500 took 281.311s\n",
      "  training loss:\t\t0.000404\n",
      "  validation loss:\t\t0.092421\n",
      "  validation accuracy:\t\t98.82 %\n",
      "Epoch 87 of 500 took 282.482s\n",
      "  training loss:\t\t0.000394\n",
      "  validation loss:\t\t0.092375\n",
      "  validation accuracy:\t\t98.82 %\n",
      "Epoch 88 of 500 took 283.276s\n",
      "  training loss:\t\t0.000385\n",
      "  validation loss:\t\t0.092533\n",
      "  validation accuracy:\t\t98.82 %\n",
      "Epoch 89 of 500 took 283.266s\n",
      "  training loss:\t\t0.000376\n",
      "  validation loss:\t\t0.092512\n",
      "  validation accuracy:\t\t98.83 %\n",
      "Epoch 90 of 500 took 280.765s\n",
      "  training loss:\t\t0.000368\n",
      "  validation loss:\t\t0.092549\n",
      "  validation accuracy:\t\t98.83 %\n",
      "Epoch 91 of 500 took 285.472s\n",
      "  training loss:\t\t0.000360\n",
      "  validation loss:\t\t0.092601\n",
      "  validation accuracy:\t\t98.83 %\n",
      "Epoch 92 of 500 took 280.316s\n",
      "  training loss:\t\t0.000352\n",
      "  validation loss:\t\t0.092599\n",
      "  validation accuracy:\t\t98.84 %\n",
      "Epoch 93 of 500 took 284.024s\n",
      "  training loss:\t\t0.000345\n",
      "  validation loss:\t\t0.092650\n",
      "  validation accuracy:\t\t98.85 %\n",
      "Epoch 94 of 500 took 283.598s\n",
      "  training loss:\t\t0.000337\n",
      "  validation loss:\t\t0.092645\n",
      "  validation accuracy:\t\t98.85 %\n",
      "Epoch 95 of 500 took 285.868s\n",
      "  training loss:\t\t0.000331\n",
      "  validation loss:\t\t0.092665\n",
      "  validation accuracy:\t\t98.86 %\n",
      "Epoch 96 of 500 took 288.291s\n",
      "  training loss:\t\t0.000324\n",
      "  validation loss:\t\t0.092730\n",
      "  validation accuracy:\t\t98.86 %\n",
      "Epoch 97 of 500 took 283.991s\n",
      "  training loss:\t\t0.000318\n",
      "  validation loss:\t\t0.092707\n",
      "  validation accuracy:\t\t98.86 %\n",
      "Epoch 98 of 500 took 285.237s\n",
      "  training loss:\t\t0.000312\n",
      "  validation loss:\t\t0.092736\n",
      "  validation accuracy:\t\t98.86 %\n",
      "Epoch 99 of 500 took 282.943s\n",
      "  training loss:\t\t0.000306\n",
      "  validation loss:\t\t0.092767\n",
      "  validation accuracy:\t\t98.86 %\n",
      "Epoch 100 of 500 took 283.561s\n",
      "  training loss:\t\t0.000300\n",
      "  validation loss:\t\t0.092815\n",
      "  validation accuracy:\t\t98.87 %\n",
      " setting learning rate to 0.0010000\n",
      "Epoch 101 of 500 took 282.367s\n",
      "  training loss:\t\t0.000312\n",
      "  validation loss:\t\t0.086174\n",
      "  validation accuracy:\t\t98.95 %\n",
      "Epoch 102 of 500 took 283.792s\n",
      "  training loss:\t\t0.000295\n",
      "  validation loss:\t\t0.086169\n",
      "  validation accuracy:\t\t98.95 %\n",
      "Epoch 103 of 500 took 282.886s\n",
      "  training loss:\t\t0.000292\n",
      "  validation loss:\t\t0.086167\n",
      "  validation accuracy:\t\t98.95 %\n",
      "Epoch 104 of 500 took 281.913s\n",
      "  training loss:\t\t0.000291\n",
      "  validation loss:\t\t0.086168\n",
      "  validation accuracy:\t\t98.96 %\n",
      "Epoch 105 of 500 took 281.863s\n",
      "  training loss:\t\t0.000289\n",
      "  validation loss:\t\t0.086172\n",
      "  validation accuracy:\t\t98.96 %\n",
      "Epoch 106 of 500 took 281.447s\n",
      "  training loss:\t\t0.000288\n",
      "  validation loss:\t\t0.086173\n",
      "  validation accuracy:\t\t98.96 %\n",
      "Epoch 107 of 500 took 283.382s\n",
      "  training loss:\t\t0.000287\n",
      "  validation loss:\t\t0.086180\n",
      "  validation accuracy:\t\t98.96 %\n",
      "Epoch 108 of 500 took 284.308s\n",
      "  training loss:\t\t0.000286\n",
      "  validation loss:\t\t0.086192\n",
      "  validation accuracy:\t\t98.96 %\n",
      "Epoch 109 of 500 took 288.158s\n",
      "  training loss:\t\t0.000285\n",
      "  validation loss:\t\t0.086201\n",
      "  validation accuracy:\t\t98.96 %\n",
      "Epoch 110 of 500 took 284.187s\n",
      "  training loss:\t\t0.000284\n",
      "  validation loss:\t\t0.086209\n",
      "  validation accuracy:\t\t98.96 %\n",
      "Epoch 111 of 500 took 284.599s\n",
      "  training loss:\t\t0.000283\n",
      "  validation loss:\t\t0.086219\n",
      "  validation accuracy:\t\t98.96 %\n",
      "Epoch 112 of 500 took 290.558s\n",
      "  training loss:\t\t0.000282\n",
      "  validation loss:\t\t0.086229\n",
      "  validation accuracy:\t\t98.96 %\n",
      "Epoch 113 of 500 took 286.051s\n",
      "  training loss:\t\t0.000282\n",
      "  validation loss:\t\t0.086239\n",
      "  validation accuracy:\t\t98.96 %\n",
      "Epoch 114 of 500 took 289.379s\n",
      "  training loss:\t\t0.000281\n",
      "  validation loss:\t\t0.086250\n",
      "  validation accuracy:\t\t98.96 %\n",
      "Epoch 115 of 500 took 288.092s\n",
      "  training loss:\t\t0.000280\n",
      "  validation loss:\t\t0.086262\n",
      "  validation accuracy:\t\t98.96 %\n",
      "Epoch 116 of 500 took 290.835s\n",
      "  training loss:\t\t0.000279\n",
      "  validation loss:\t\t0.086276\n",
      "  validation accuracy:\t\t98.96 %\n",
      "Epoch 117 of 500 took 290.050s\n",
      "  training loss:\t\t0.000279\n",
      "  validation loss:\t\t0.086286\n",
      "  validation accuracy:\t\t98.96 %\n",
      "Epoch 118 of 500 took 285.373s\n",
      "  training loss:\t\t0.000278\n",
      "  validation loss:\t\t0.086298\n",
      "  validation accuracy:\t\t98.96 %\n",
      "Epoch 119 of 500 took 288.308s\n",
      "  training loss:\t\t0.000277\n",
      "  validation loss:\t\t0.086309\n",
      "  validation accuracy:\t\t98.96 %\n",
      "Epoch 120 of 500 took 287.893s\n",
      "  training loss:\t\t0.000276\n",
      "  validation loss:\t\t0.086321\n",
      "  validation accuracy:\t\t98.95 %\n",
      "Epoch 121 of 500 took 285.907s\n",
      "  training loss:\t\t0.000276\n",
      "  validation loss:\t\t0.086333\n",
      "  validation accuracy:\t\t98.95 %\n",
      "Epoch 122 of 500 took 288.010s\n",
      "  training loss:\t\t0.000275\n",
      "  validation loss:\t\t0.086344\n",
      "  validation accuracy:\t\t98.95 %\n",
      "Epoch 123 of 500 took 288.690s\n",
      "  training loss:\t\t0.000274\n",
      "  validation loss:\t\t0.086357\n",
      "  validation accuracy:\t\t98.96 %\n",
      "Epoch 124 of 500 took 287.711s\n",
      "  training loss:\t\t0.000274\n",
      "  validation loss:\t\t0.086370\n",
      "  validation accuracy:\t\t98.96 %\n",
      "Epoch 125 of 500 took 288.504s\n",
      "  training loss:\t\t0.000273\n",
      "  validation loss:\t\t0.086383\n",
      "  validation accuracy:\t\t98.96 %\n",
      "Epoch 126 of 500 took 288.261s\n",
      "  training loss:\t\t0.000272\n",
      "  validation loss:\t\t0.086397\n",
      "  validation accuracy:\t\t98.96 %\n",
      "Epoch 127 of 500 took 288.398s\n",
      "  training loss:\t\t0.000272\n",
      "  validation loss:\t\t0.086409\n",
      "  validation accuracy:\t\t98.96 %\n",
      "Epoch 128 of 500 took 287.932s\n",
      "  training loss:\t\t0.000271\n",
      "  validation loss:\t\t0.086420\n",
      "  validation accuracy:\t\t98.96 %\n",
      "Epoch 129 of 500 took 288.111s\n",
      "  training loss:\t\t0.000271\n",
      "  validation loss:\t\t0.086433\n",
      "  validation accuracy:\t\t98.96 %\n",
      "Epoch 130 of 500 took 288.333s\n",
      "  training loss:\t\t0.000270\n",
      "  validation loss:\t\t0.086445\n",
      "  validation accuracy:\t\t98.96 %\n",
      "Epoch 131 of 500 took 288.360s\n",
      "  training loss:\t\t0.000269\n",
      "  validation loss:\t\t0.086456\n",
      "  validation accuracy:\t\t98.96 %\n",
      "Epoch 132 of 500 took 287.854s\n",
      "  training loss:\t\t0.000269\n",
      "  validation loss:\t\t0.086468\n",
      "  validation accuracy:\t\t98.96 %\n",
      "Epoch 133 of 500 took 288.271s\n",
      "  training loss:\t\t0.000268\n",
      "  validation loss:\t\t0.086479\n",
      "  validation accuracy:\t\t98.96 %\n",
      "Epoch 134 of 500 took 288.583s\n",
      "  training loss:\t\t0.000268\n",
      "  validation loss:\t\t0.086494\n",
      "  validation accuracy:\t\t98.96 %\n",
      "Epoch 135 of 500 took 289.036s\n",
      "  training loss:\t\t0.000267\n",
      "  validation loss:\t\t0.086504\n",
      "  validation accuracy:\t\t98.96 %\n",
      "Epoch 136 of 500 took 287.969s\n",
      "  training loss:\t\t0.000266\n",
      "  validation loss:\t\t0.086515\n",
      "  validation accuracy:\t\t98.96 %\n",
      "Epoch 137 of 500 took 288.340s\n",
      "  training loss:\t\t0.000266\n",
      "  validation loss:\t\t0.086527\n",
      "  validation accuracy:\t\t98.95 %\n",
      "Epoch 138 of 500 took 288.571s\n",
      "  training loss:\t\t0.000265\n",
      "  validation loss:\t\t0.086540\n",
      "  validation accuracy:\t\t98.95 %\n",
      "Epoch 139 of 500 took 288.801s\n",
      "  training loss:\t\t0.000265\n",
      "  validation loss:\t\t0.086554\n",
      "  validation accuracy:\t\t98.95 %\n",
      "Epoch 140 of 500 took 288.259s\n",
      "  training loss:\t\t0.000264\n",
      "  validation loss:\t\t0.086565\n",
      "  validation accuracy:\t\t98.95 %\n",
      "Epoch 141 of 500 took 288.531s\n",
      "  training loss:\t\t0.000264\n",
      "  validation loss:\t\t0.086577\n",
      "  validation accuracy:\t\t98.95 %\n",
      "Epoch 142 of 500 took 289.307s\n",
      "  training loss:\t\t0.000263\n",
      "  validation loss:\t\t0.086591\n",
      "  validation accuracy:\t\t98.95 %\n",
      "Epoch 143 of 500 took 289.069s\n",
      "  training loss:\t\t0.000263\n",
      "  validation loss:\t\t0.086603\n",
      "  validation accuracy:\t\t98.95 %\n",
      "Epoch 144 of 500 took 289.144s\n",
      "  training loss:\t\t0.000262\n",
      "  validation loss:\t\t0.086615\n",
      "  validation accuracy:\t\t98.95 %\n",
      "Epoch 145 of 500 took 288.686s\n",
      "  training loss:\t\t0.000262\n",
      "  validation loss:\t\t0.086628\n",
      "  validation accuracy:\t\t98.95 %\n",
      "Epoch 146 of 500 took 288.911s\n",
      "  training loss:\t\t0.000261\n",
      "  validation loss:\t\t0.086639\n",
      "  validation accuracy:\t\t98.95 %\n",
      "Epoch 147 of 500 took 288.361s\n",
      "  training loss:\t\t0.000260\n",
      "  validation loss:\t\t0.086653\n",
      "  validation accuracy:\t\t98.95 %\n",
      "Epoch 148 of 500 took 289.204s\n",
      "  training loss:\t\t0.000260\n",
      "  validation loss:\t\t0.086665\n",
      "  validation accuracy:\t\t98.95 %\n",
      "Epoch 149 of 500 took 289.983s\n",
      "  training loss:\t\t0.000259\n",
      "  validation loss:\t\t0.086678\n",
      "  validation accuracy:\t\t98.95 %\n",
      "Epoch 150 of 500 took 288.225s\n",
      "  training loss:\t\t0.000259\n",
      "  validation loss:\t\t0.086690\n",
      "  validation accuracy:\t\t98.95 %\n",
      " setting learning rate to 0.0001000\n",
      "Epoch 151 of 500 took 288.741s\n",
      "  training loss:\t\t0.000257\n",
      "  validation loss:\t\t0.086443\n",
      "  validation accuracy:\t\t98.96 %\n",
      "Epoch 152 of 500 took 289.040s\n",
      "  training loss:\t\t0.000256\n",
      "  validation loss:\t\t0.086320\n",
      "  validation accuracy:\t\t98.96 %\n",
      "Epoch 153 of 500 took 288.632s\n",
      "  training loss:\t\t0.000256\n",
      "  validation loss:\t\t0.086271\n",
      "  validation accuracy:\t\t98.96 %\n",
      "Epoch 154 of 500 took 288.908s\n",
      "  training loss:\t\t0.000256\n",
      "  validation loss:\t\t0.086252\n",
      "  validation accuracy:\t\t98.96 %\n",
      "Epoch 155 of 500 took 288.897s\n",
      "  training loss:\t\t0.000256\n",
      "  validation loss:\t\t0.086245\n",
      "  validation accuracy:\t\t98.96 %\n",
      "Epoch 156 of 500 took 289.511s\n",
      "  training loss:\t\t0.000256\n",
      "  validation loss:\t\t0.086244\n",
      "  validation accuracy:\t\t98.96 %\n",
      "Epoch 157 of 500 took 289.073s\n",
      "  training loss:\t\t0.000256\n",
      "  validation loss:\t\t0.086243\n",
      "  validation accuracy:\t\t98.96 %\n",
      "Epoch 158 of 500 took 289.281s\n",
      "  training loss:\t\t0.000256\n",
      "  validation loss:\t\t0.086244\n",
      "  validation accuracy:\t\t98.96 %\n",
      "Epoch 159 of 500 took 289.040s\n",
      "  training loss:\t\t0.000256\n",
      "  validation loss:\t\t0.086246\n",
      "  validation accuracy:\t\t98.95 %\n",
      "Epoch 160 of 500 took 288.893s\n",
      "  training loss:\t\t0.000255\n",
      "  validation loss:\t\t0.086247\n",
      "  validation accuracy:\t\t98.95 %\n",
      "Epoch 161 of 500 took 289.348s\n",
      "  training loss:\t\t0.000255\n",
      "  validation loss:\t\t0.086248\n",
      "  validation accuracy:\t\t98.95 %\n",
      "Epoch 162 of 500 took 288.633s\n",
      "  training loss:\t\t0.000255\n",
      "  validation loss:\t\t0.086248\n",
      "  validation accuracy:\t\t98.95 %\n",
      "Epoch 163 of 500 took 289.382s\n",
      "  training loss:\t\t0.000255\n",
      "  validation loss:\t\t0.086250\n",
      "  validation accuracy:\t\t98.95 %\n",
      "Epoch 164 of 500 took 289.298s\n",
      "  training loss:\t\t0.000255\n",
      "  validation loss:\t\t0.086250\n",
      "  validation accuracy:\t\t98.95 %\n",
      "Epoch 165 of 500 took 289.562s\n",
      "  training loss:\t\t0.000255\n",
      "  validation loss:\t\t0.086252\n",
      "  validation accuracy:\t\t98.95 %\n",
      "Epoch 166 of 500 took 289.184s\n",
      "  training loss:\t\t0.000255\n",
      "  validation loss:\t\t0.086253\n",
      "  validation accuracy:\t\t98.95 %\n",
      "Epoch 167 of 500 took 289.654s\n",
      "  training loss:\t\t0.000255\n",
      "  validation loss:\t\t0.086254\n",
      "  validation accuracy:\t\t98.95 %\n",
      "Epoch 168 of 500 took 289.304s\n",
      "  training loss:\t\t0.000255\n",
      "  validation loss:\t\t0.086255\n",
      "  validation accuracy:\t\t98.95 %\n",
      "Epoch 169 of 500 took 289.509s\n",
      "  training loss:\t\t0.000255\n",
      "  validation loss:\t\t0.086256\n",
      "  validation accuracy:\t\t98.95 %\n",
      "Epoch 170 of 500 took 289.957s\n",
      "  training loss:\t\t0.000255\n",
      "  validation loss:\t\t0.086257\n",
      "  validation accuracy:\t\t98.95 %\n",
      "Epoch 171 of 500 took 290.333s\n",
      "  training loss:\t\t0.000255\n",
      "  validation loss:\t\t0.086258\n",
      "  validation accuracy:\t\t98.95 %\n",
      "Epoch 172 of 500 took 290.147s\n",
      "  training loss:\t\t0.000255\n",
      "  validation loss:\t\t0.086259\n",
      "  validation accuracy:\t\t98.95 %\n",
      "Epoch 173 of 500 took 290.691s\n",
      "  training loss:\t\t0.000255\n",
      "  validation loss:\t\t0.086260\n",
      "  validation accuracy:\t\t98.95 %\n",
      "Epoch 174 of 500 took 291.176s\n",
      "  training loss:\t\t0.000255\n",
      "  validation loss:\t\t0.086262\n",
      "  validation accuracy:\t\t98.95 %\n",
      "Epoch 175 of 500 took 290.241s\n",
      "  training loss:\t\t0.000255\n",
      "  validation loss:\t\t0.086262\n",
      "  validation accuracy:\t\t98.95 %\n",
      "Epoch 176 of 500 took 289.769s\n",
      "  training loss:\t\t0.000255\n",
      "  validation loss:\t\t0.086264\n",
      "  validation accuracy:\t\t98.95 %\n",
      "Epoch 177 of 500 took 289.547s\n",
      "  training loss:\t\t0.000255\n",
      "  validation loss:\t\t0.086265\n",
      "  validation accuracy:\t\t98.95 %\n",
      "Epoch 178 of 500 took 290.000s\n",
      "  training loss:\t\t0.000255\n",
      "  validation loss:\t\t0.086266\n",
      "  validation accuracy:\t\t98.95 %\n",
      "Epoch 179 of 500 took 290.148s\n",
      "  training loss:\t\t0.000255\n",
      "  validation loss:\t\t0.086267\n",
      "  validation accuracy:\t\t98.95 %\n",
      "Epoch 180 of 500 took 290.243s\n",
      "  training loss:\t\t0.000254\n",
      "  validation loss:\t\t0.086268\n",
      "  validation accuracy:\t\t98.95 %\n",
      "Epoch 181 of 500 took 290.287s\n",
      "  training loss:\t\t0.000254\n",
      "  validation loss:\t\t0.086269\n",
      "  validation accuracy:\t\t98.95 %\n",
      "Epoch 182 of 500 took 289.804s\n",
      "  training loss:\t\t0.000254\n",
      "  validation loss:\t\t0.086271\n",
      "  validation accuracy:\t\t98.95 %\n",
      "Epoch 183 of 500 took 289.973s\n",
      "  training loss:\t\t0.000254\n",
      "  validation loss:\t\t0.086272\n",
      "  validation accuracy:\t\t98.95 %\n",
      "Epoch 184 of 500 took 290.091s\n",
      "  training loss:\t\t0.000254\n",
      "  validation loss:\t\t0.086273\n",
      "  validation accuracy:\t\t98.95 %\n",
      "Epoch 185 of 500 took 289.957s\n",
      "  training loss:\t\t0.000254\n",
      "  validation loss:\t\t0.086274\n",
      "  validation accuracy:\t\t98.95 %\n",
      "Epoch 186 of 500 took 289.963s\n",
      "  training loss:\t\t0.000254\n",
      "  validation loss:\t\t0.086275\n",
      "  validation accuracy:\t\t98.95 %\n",
      "Epoch 187 of 500 took 289.610s\n",
      "  training loss:\t\t0.000254\n",
      "  validation loss:\t\t0.086276\n",
      "  validation accuracy:\t\t98.95 %\n",
      "Epoch 188 of 500 took 289.780s\n",
      "  training loss:\t\t0.000254\n",
      "  validation loss:\t\t0.086277\n",
      "  validation accuracy:\t\t98.95 %\n",
      "Epoch 189 of 500 took 290.574s\n",
      "  training loss:\t\t0.000254\n",
      "  validation loss:\t\t0.086278\n",
      "  validation accuracy:\t\t98.95 %\n",
      "Epoch 190 of 500 took 289.804s\n",
      "  training loss:\t\t0.000254\n",
      "  validation loss:\t\t0.086279\n",
      "  validation accuracy:\t\t98.95 %\n",
      "Epoch 191 of 500 took 290.119s\n",
      "  training loss:\t\t0.000254\n",
      "  validation loss:\t\t0.086281\n",
      "  validation accuracy:\t\t98.95 %\n",
      "Epoch 192 of 500 took 289.869s\n",
      "  training loss:\t\t0.000254\n",
      "  validation loss:\t\t0.086282\n",
      "  validation accuracy:\t\t98.95 %\n",
      "Epoch 193 of 500 took 289.650s\n",
      "  training loss:\t\t0.000254\n",
      "  validation loss:\t\t0.086283\n",
      "  validation accuracy:\t\t98.95 %\n",
      "Epoch 194 of 500 took 290.255s\n",
      "  training loss:\t\t0.000254\n",
      "  validation loss:\t\t0.086284\n",
      "  validation accuracy:\t\t98.95 %\n",
      "Epoch 195 of 500 took 289.889s\n",
      "  training loss:\t\t0.000254\n",
      "  validation loss:\t\t0.086285\n",
      "  validation accuracy:\t\t98.95 %\n",
      "Epoch 196 of 500 took 289.835s\n",
      "  training loss:\t\t0.000254\n",
      "  validation loss:\t\t0.086286\n",
      "  validation accuracy:\t\t98.95 %\n",
      "Epoch 197 of 500 took 290.162s\n",
      "  training loss:\t\t0.000254\n",
      "  validation loss:\t\t0.086287\n",
      "  validation accuracy:\t\t98.95 %\n",
      "Epoch 198 of 500 took 289.995s\n",
      "  training loss:\t\t0.000254\n",
      "  validation loss:\t\t0.086288\n",
      "  validation accuracy:\t\t98.95 %\n",
      "Epoch 199 of 500 took 289.836s\n",
      "  training loss:\t\t0.000254\n",
      "  validation loss:\t\t0.086289\n",
      "  validation accuracy:\t\t98.95 %\n",
      "Epoch 200 of 500 took 290.196s\n",
      "  training loss:\t\t0.000254\n",
      "  validation loss:\t\t0.086291\n",
      "  validation accuracy:\t\t98.95 %\n",
      "Epoch 201 of 500 took 290.268s\n",
      "  training loss:\t\t0.000254\n",
      "  validation loss:\t\t0.086292\n",
      "  validation accuracy:\t\t98.95 %\n",
      "Epoch 202 of 500 took 289.369s\n",
      "  training loss:\t\t0.000253\n",
      "  validation loss:\t\t0.086293\n",
      "  validation accuracy:\t\t98.95 %\n",
      "Epoch 203 of 500 took 290.226s\n",
      "  training loss:\t\t0.000253\n",
      "  validation loss:\t\t0.086294\n",
      "  validation accuracy:\t\t98.95 %\n",
      "Epoch 204 of 500 took 290.078s\n",
      "  training loss:\t\t0.000253\n",
      "  validation loss:\t\t0.086295\n",
      "  validation accuracy:\t\t98.95 %\n",
      "Epoch 205 of 500 took 290.231s\n",
      "  training loss:\t\t0.000253\n",
      "  validation loss:\t\t0.086296\n",
      "  validation accuracy:\t\t98.95 %\n",
      "Epoch 206 of 500 took 289.764s\n",
      "  training loss:\t\t0.000253\n",
      "  validation loss:\t\t0.086298\n",
      "  validation accuracy:\t\t98.95 %\n",
      "Epoch 207 of 500 took 289.346s\n",
      "  training loss:\t\t0.000253\n",
      "  validation loss:\t\t0.086299\n",
      "  validation accuracy:\t\t98.95 %\n",
      "Epoch 208 of 500 took 289.578s\n",
      "  training loss:\t\t0.000253\n",
      "  validation loss:\t\t0.086300\n",
      "  validation accuracy:\t\t98.95 %\n",
      "Epoch 209 of 500 took 289.686s\n",
      "  training loss:\t\t0.000253\n",
      "  validation loss:\t\t0.086301\n",
      "  validation accuracy:\t\t98.95 %\n",
      "Epoch 210 of 500 took 289.548s\n",
      "  training loss:\t\t0.000253\n",
      "  validation loss:\t\t0.086302\n",
      "  validation accuracy:\t\t98.95 %\n",
      "Epoch 211 of 500 took 289.326s\n",
      "  training loss:\t\t0.000253\n",
      "  validation loss:\t\t0.086303\n",
      "  validation accuracy:\t\t98.95 %\n",
      "Epoch 212 of 500 took 289.387s\n",
      "  training loss:\t\t0.000253\n",
      "  validation loss:\t\t0.086304\n",
      "  validation accuracy:\t\t98.95 %\n",
      "Epoch 213 of 500 took 289.469s\n",
      "  training loss:\t\t0.000253\n",
      "  validation loss:\t\t0.086306\n",
      "  validation accuracy:\t\t98.95 %\n",
      "Epoch 214 of 500 took 289.987s\n",
      "  training loss:\t\t0.000253\n",
      "  validation loss:\t\t0.086307\n",
      "  validation accuracy:\t\t98.95 %\n",
      "Epoch 215 of 500 took 289.651s\n",
      "  training loss:\t\t0.000253\n",
      "  validation loss:\t\t0.086308\n",
      "  validation accuracy:\t\t98.95 %\n",
      "Epoch 216 of 500 took 289.323s\n",
      "  training loss:\t\t0.000253\n",
      "  validation loss:\t\t0.086309\n",
      "  validation accuracy:\t\t98.95 %\n",
      "Epoch 217 of 500 took 289.544s\n",
      "  training loss:\t\t0.000253\n",
      "  validation loss:\t\t0.086310\n",
      "  validation accuracy:\t\t98.95 %\n",
      "Epoch 218 of 500 took 289.085s\n",
      "  training loss:\t\t0.000253\n",
      "  validation loss:\t\t0.086311\n",
      "  validation accuracy:\t\t98.95 %\n",
      "Epoch 219 of 500 took 289.575s\n",
      "  training loss:\t\t0.000253\n",
      "  validation loss:\t\t0.086313\n",
      "  validation accuracy:\t\t98.95 %\n",
      "Epoch 220 of 500 took 289.471s\n",
      "  training loss:\t\t0.000253\n",
      "  validation loss:\t\t0.086314\n",
      "  validation accuracy:\t\t98.95 %\n",
      "Epoch 221 of 500 took 288.950s\n",
      "  training loss:\t\t0.000253\n",
      "  validation loss:\t\t0.086315\n",
      "  validation accuracy:\t\t98.95 %\n",
      "Epoch 222 of 500 took 288.720s\n",
      "  training loss:\t\t0.000253\n",
      "  validation loss:\t\t0.086316\n",
      "  validation accuracy:\t\t98.95 %\n",
      "Epoch 223 of 500 took 289.372s\n",
      "  training loss:\t\t0.000252\n",
      "  validation loss:\t\t0.086318\n",
      "  validation accuracy:\t\t98.95 %\n",
      "Epoch 224 of 500 took 288.776s\n",
      "  training loss:\t\t0.000252\n",
      "  validation loss:\t\t0.086319\n",
      "  validation accuracy:\t\t98.95 %\n",
      "Epoch 225 of 500 took 289.457s\n",
      "  training loss:\t\t0.000252\n",
      "  validation loss:\t\t0.086320\n",
      "  validation accuracy:\t\t98.95 %\n",
      "Epoch 226 of 500 took 289.039s\n",
      "  training loss:\t\t0.000252\n",
      "  validation loss:\t\t0.086321\n",
      "  validation accuracy:\t\t98.95 %\n",
      "Epoch 227 of 500 took 288.928s\n",
      "  training loss:\t\t0.000252\n",
      "  validation loss:\t\t0.086322\n",
      "  validation accuracy:\t\t98.95 %\n",
      "Epoch 228 of 500 took 288.539s\n",
      "  training loss:\t\t0.000252\n",
      "  validation loss:\t\t0.086323\n",
      "  validation accuracy:\t\t98.95 %\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-49-d557231b7b71>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0miterate_minibatches\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m500\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m         \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtargets\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m         \u001b[0mtrain_err\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mtrain_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m         \u001b[0mtrain_batches\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/d/nolearn/local/lib/python2.7/site-packages/theano/compile/function_module.pyc\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    857\u001b[0m         \u001b[0mt0_fn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    858\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 859\u001b[1;33m             \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    860\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    861\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'position_of_error'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "# Finally, launch the training loop.\n",
    "print(\"Starting training...\")\n",
    "# We iterate over epochs:\n",
    "for epoch in range(num_epochs):\n",
    "    if epoch in learning_rate_schedule:\n",
    "        lr = np.float32(learning_rate_schedule[epoch])\n",
    "        print(\" setting learning rate to %.7f\" % lr)\n",
    "        learning_rate.set_value(lr)\n",
    "    # In each epoch, we do a full pass over the training data:\n",
    "    train_err = 0\n",
    "    train_batches = 0\n",
    "    start_time = time.time()\n",
    "    for batch in iterate_minibatches(X_train2, y_train2, 500, shuffle=False):\n",
    "        inputs, targets = batch\n",
    "        train_err += train_fn(inputs, targets)\n",
    "        train_batches += 1\n",
    "\n",
    "    # And a full pass over the validation data:\n",
    "    val_err = 0\n",
    "    val_acc = 0\n",
    "    val_batches = 0\n",
    "    for batch in iterate_minibatches(X_val, y_val, 500, shuffle=False):\n",
    "        inputs, targets = batch\n",
    "        err, acc = val_fn(inputs, targets)\n",
    "        val_err += err\n",
    "        val_acc += acc\n",
    "        val_batches += 1\n",
    "\n",
    "    # Then we print the results for this epoch:\n",
    "    print(\"Epoch {} of {} took {:.3f}s\".format(\n",
    "        epoch + 1, num_epochs, time.time() - start_time))\n",
    "    print(\"  training loss:\\t\\t{:.6f}\".format(train_err / train_batches))\n",
    "    print(\"  validation loss:\\t\\t{:.6f}\".format(val_err / val_batches))\n",
    "    print(\"  validation accuracy:\\t\\t{:.2f} %\".format(\n",
    "        val_acc / val_batches * 100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final results:\n",
      "  test loss:\t\t\t0.831464\n",
      "  test accuracy:\t\t90.46 %\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# After training, we compute and print the test error:\n",
    "test_err = 0\n",
    "test_acc = 0\n",
    "test_batches = 0\n",
    "for batch in iterate_minibatches(X_test, y_test, 500, shuffle=False):\n",
    "    inputs, targets = batch\n",
    "    err, acc = val_fn(inputs, targets)\n",
    "    test_err += err\n",
    "    test_acc += acc\n",
    "    test_batches += 1\n",
    "print(\"Final results:\")\n",
    "print(\"  test loss:\\t\\t\\t{:.6f}\".format(test_err / test_batches))\n",
    "print(\"  test accuracy:\\t\\t{:.2f} %\".format(\n",
    "    test_acc / test_batches * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n",
      " setting learning rate to 0.0001000\n",
      "Epoch 1 of 200 took 470.882s\n",
      "  training loss:\t\t0.469967\n",
      "  validation loss:\t\t0.358939\n",
      "  validation accuracy:\t\t85.52 %\n",
      "Epoch 2 of 200 took 482.666s\n",
      "  training loss:\t\t0.311313\n",
      "  validation loss:\t\t0.299062\n",
      "  validation accuracy:\t\t88.38 %\n",
      " setting learning rate to 0.0100000\n",
      "Epoch 3 of 200 took 481.257s\n",
      "  training loss:\t\t1.173967\n",
      "  validation loss:\t\t0.294466\n",
      "  validation accuracy:\t\t86.91 %\n",
      "Epoch 4 of 200 took 476.688s\n",
      "  training loss:\t\t0.223470\n",
      "  validation loss:\t\t0.254502\n",
      "  validation accuracy:\t\t89.41 %\n",
      "Epoch 5 of 200 took 481.711s\n",
      "  training loss:\t\t0.208328\n",
      "  validation loss:\t\t0.230796\n",
      "  validation accuracy:\t\t90.81 %\n",
      "Epoch 6 of 200 took 481.467s\n",
      "  training loss:\t\t0.199105\n",
      "  validation loss:\t\t0.211668\n",
      "  validation accuracy:\t\t91.84 %\n",
      "Epoch 7 of 200 took 481.601s\n",
      "  training loss:\t\t0.192134\n",
      "  validation loss:\t\t0.203365\n",
      "  validation accuracy:\t\t92.26 %\n",
      "Epoch 8 of 200 took 481.462s\n",
      "  training loss:\t\t0.186225\n",
      "  validation loss:\t\t0.200710\n",
      "  validation accuracy:\t\t92.39 %\n",
      "Epoch 9 of 200 took 481.613s\n",
      "  training loss:\t\t0.180898\n",
      "  validation loss:\t\t0.197895\n",
      "  validation accuracy:\t\t92.43 %\n",
      "Epoch 10 of 200 took 480.563s\n",
      "  training loss:\t\t0.175844\n",
      "  validation loss:\t\t0.192544\n",
      "  validation accuracy:\t\t92.69 %\n",
      "Epoch 11 of 200 took 479.999s\n",
      "  training loss:\t\t0.170885\n",
      "  validation loss:\t\t0.190165\n",
      "  validation accuracy:\t\t92.68 %\n",
      "Epoch 12 of 200 took 479.896s\n",
      "  training loss:\t\t0.165902\n",
      "  validation loss:\t\t0.185151\n",
      "  validation accuracy:\t\t92.93 %\n",
      "Epoch 13 of 200 took 479.353s\n",
      "  training loss:\t\t0.161054\n",
      "  validation loss:\t\t0.183555\n",
      "  validation accuracy:\t\t92.96 %\n",
      "Epoch 14 of 200 took 478.997s\n",
      "  training loss:\t\t0.156245\n",
      "  validation loss:\t\t0.183042\n",
      "  validation accuracy:\t\t92.98 %\n",
      "Epoch 15 of 200 took 479.438s\n",
      "  training loss:\t\t0.151298\n",
      "  validation loss:\t\t0.180539\n",
      "  validation accuracy:\t\t93.14 %\n",
      "Epoch 16 of 200 took 478.290s\n",
      "  training loss:\t\t0.146110\n",
      "  validation loss:\t\t0.179271\n",
      "  validation accuracy:\t\t93.12 %\n",
      "Epoch 17 of 200 took 473.067s\n",
      "  training loss:\t\t0.140677\n",
      "  validation loss:\t\t0.175448\n",
      "  validation accuracy:\t\t93.33 %\n",
      "Epoch 18 of 200 took 474.051s\n",
      "  training loss:\t\t0.135166\n",
      "  validation loss:\t\t0.169781\n",
      "  validation accuracy:\t\t93.62 %\n",
      "Epoch 19 of 200 took 479.337s\n",
      "  training loss:\t\t0.129518\n",
      "  validation loss:\t\t0.163130\n",
      "  validation accuracy:\t\t93.92 %\n",
      "Epoch 20 of 200 took 474.061s\n",
      "  training loss:\t\t0.123747\n",
      "  validation loss:\t\t0.158072\n",
      "  validation accuracy:\t\t94.06 %\n",
      "Epoch 21 of 200 took 474.005s\n",
      "  training loss:\t\t0.117733\n",
      "  validation loss:\t\t0.152016\n",
      "  validation accuracy:\t\t94.35 %\n",
      "Epoch 22 of 200 took 472.428s\n",
      "  training loss:\t\t0.111381\n",
      "  validation loss:\t\t0.151203\n",
      "  validation accuracy:\t\t94.41 %\n",
      "Epoch 23 of 200 took 471.523s\n",
      "  training loss:\t\t0.105136\n",
      "  validation loss:\t\t0.144045\n",
      "  validation accuracy:\t\t94.72 %\n",
      "Epoch 24 of 200 took 473.634s\n",
      "  training loss:\t\t0.098774\n",
      "  validation loss:\t\t0.142352\n",
      "  validation accuracy:\t\t94.88 %\n",
      "Epoch 25 of 200 took 474.666s\n",
      "  training loss:\t\t0.092555\n",
      "  validation loss:\t\t0.139601\n",
      "  validation accuracy:\t\t95.01 %\n",
      "Epoch 26 of 200 took 475.457s\n",
      "  training loss:\t\t0.086278\n",
      "  validation loss:\t\t0.132363\n",
      "  validation accuracy:\t\t95.26 %\n",
      "Epoch 27 of 200 took 472.943s\n",
      "  training loss:\t\t0.080092\n",
      "  validation loss:\t\t0.134367\n",
      "  validation accuracy:\t\t95.21 %\n",
      "Epoch 28 of 200 took 470.925s\n",
      "  training loss:\t\t0.073954\n",
      "  validation loss:\t\t0.130861\n",
      "  validation accuracy:\t\t95.30 %\n",
      "Epoch 29 of 200 took 471.571s\n",
      "  training loss:\t\t0.068291\n",
      "  validation loss:\t\t0.125399\n",
      "  validation accuracy:\t\t95.57 %\n",
      "Epoch 30 of 200 took 470.100s\n",
      "  training loss:\t\t0.062280\n",
      "  validation loss:\t\t0.119888\n",
      "  validation accuracy:\t\t95.86 %\n",
      "Epoch 31 of 200 took 469.430s\n",
      "  training loss:\t\t0.057161\n",
      "  validation loss:\t\t0.119014\n",
      "  validation accuracy:\t\t95.74 %\n",
      "Epoch 32 of 200 took 463.650s\n",
      "  training loss:\t\t0.052001\n",
      "  validation loss:\t\t0.111229\n",
      "  validation accuracy:\t\t96.32 %\n",
      "Epoch 33 of 200 took 468.104s\n",
      "  training loss:\t\t0.046916\n",
      "  validation loss:\t\t0.104690\n",
      "  validation accuracy:\t\t96.50 %\n",
      "Epoch 34 of 200 took 466.045s\n",
      "  training loss:\t\t0.042138\n",
      "  validation loss:\t\t0.102622\n",
      "  validation accuracy:\t\t96.58 %\n",
      "Epoch 35 of 200 took 467.376s\n",
      "  training loss:\t\t0.037936\n",
      "  validation loss:\t\t0.106572\n",
      "  validation accuracy:\t\t96.45 %\n",
      "Epoch 36 of 200 took 468.242s\n",
      "  training loss:\t\t0.033836\n",
      "  validation loss:\t\t0.105033\n",
      "  validation accuracy:\t\t96.49 %\n",
      "Epoch 37 of 200 took 463.128s\n",
      "  training loss:\t\t0.030328\n",
      "  validation loss:\t\t0.102595\n",
      "  validation accuracy:\t\t96.61 %\n",
      "Epoch 38 of 200 took 467.011s\n",
      "  training loss:\t\t0.026724\n",
      "  validation loss:\t\t0.097706\n",
      "  validation accuracy:\t\t96.95 %\n",
      "Epoch 39 of 200 took 464.165s\n",
      "  training loss:\t\t0.024014\n",
      "  validation loss:\t\t0.091947\n",
      "  validation accuracy:\t\t97.08 %\n",
      "Epoch 40 of 200 took 462.945s\n",
      "  training loss:\t\t0.020890\n",
      "  validation loss:\t\t0.092923\n",
      "  validation accuracy:\t\t97.12 %\n",
      "Epoch 41 of 200 took 464.860s\n",
      "  training loss:\t\t0.018433\n",
      "  validation loss:\t\t0.088140\n",
      "  validation accuracy:\t\t97.33 %\n",
      "Epoch 42 of 200 took 461.848s\n",
      "  training loss:\t\t0.016581\n",
      "  validation loss:\t\t0.083593\n",
      "  validation accuracy:\t\t97.54 %\n",
      "Epoch 43 of 200 took 461.054s\n",
      "  training loss:\t\t0.014476\n",
      "  validation loss:\t\t0.081696\n",
      "  validation accuracy:\t\t97.77 %\n",
      "Epoch 44 of 200 took 458.289s\n",
      "  training loss:\t\t0.012780\n",
      "  validation loss:\t\t0.080388\n",
      "  validation accuracy:\t\t97.92 %\n",
      "Epoch 45 of 200 took 452.453s\n",
      "  training loss:\t\t0.011065\n",
      "  validation loss:\t\t0.077044\n",
      "  validation accuracy:\t\t98.01 %\n",
      "Epoch 46 of 200 took 456.479s\n",
      "  training loss:\t\t0.009711\n",
      "  validation loss:\t\t0.080158\n",
      "  validation accuracy:\t\t98.07 %\n",
      "Epoch 47 of 200 took 456.711s\n",
      "  training loss:\t\t0.008738\n",
      "  validation loss:\t\t0.084959\n",
      "  validation accuracy:\t\t97.98 %\n",
      "Epoch 48 of 200 took 457.218s\n",
      "  training loss:\t\t0.007678\n",
      "  validation loss:\t\t0.080651\n",
      "  validation accuracy:\t\t98.19 %\n",
      "Epoch 49 of 200 took 455.677s\n",
      "  training loss:\t\t0.006589\n",
      "  validation loss:\t\t0.075482\n",
      "  validation accuracy:\t\t98.41 %\n",
      "Epoch 50 of 200 took 454.161s\n",
      "  training loss:\t\t0.005566\n",
      "  validation loss:\t\t0.068332\n",
      "  validation accuracy:\t\t98.60 %\n",
      "Epoch 51 of 200 took 453.764s\n",
      "  training loss:\t\t0.004571\n",
      "  validation loss:\t\t0.066132\n",
      "  validation accuracy:\t\t98.69 %\n",
      "Epoch 52 of 200 took 449.711s\n",
      "  training loss:\t\t0.003740\n",
      "  validation loss:\t\t0.066036\n",
      "  validation accuracy:\t\t98.72 %\n",
      "Epoch 53 of 200 took 454.130s\n",
      "  training loss:\t\t0.002995\n",
      "  validation loss:\t\t0.064127\n",
      "  validation accuracy:\t\t98.87 %\n",
      "Epoch 54 of 200 took 453.757s\n",
      "  training loss:\t\t0.002406\n",
      "  validation loss:\t\t0.065001\n",
      "  validation accuracy:\t\t98.82 %\n",
      "Epoch 55 of 200 took 449.373s\n",
      "  training loss:\t\t0.002019\n",
      "  validation loss:\t\t0.061982\n",
      "  validation accuracy:\t\t98.86 %\n",
      "Epoch 56 of 200 took 453.159s\n",
      "  training loss:\t\t0.001711\n",
      "  validation loss:\t\t0.059337\n",
      "  validation accuracy:\t\t99.04 %\n",
      "Epoch 57 of 200 took 453.193s\n",
      "  training loss:\t\t0.001486\n",
      "  validation loss:\t\t0.058226\n",
      "  validation accuracy:\t\t99.12 %\n",
      "Epoch 58 of 200 took 453.144s\n",
      "  training loss:\t\t0.001318\n",
      "  validation loss:\t\t0.058024\n",
      "  validation accuracy:\t\t99.17 %\n",
      "Epoch 59 of 200 took 453.589s\n",
      "  training loss:\t\t0.001187\n",
      "  validation loss:\t\t0.058371\n",
      "  validation accuracy:\t\t99.19 %\n",
      "Epoch 60 of 200 took 453.743s\n",
      "  training loss:\t\t0.001083\n",
      "  validation loss:\t\t0.059028\n",
      "  validation accuracy:\t\t99.16 %\n",
      "Epoch 61 of 200 took 453.090s\n",
      "  training loss:\t\t0.000995\n",
      "  validation loss:\t\t0.059877\n",
      "  validation accuracy:\t\t99.18 %\n",
      "Epoch 62 of 200 took 452.814s\n",
      "  training loss:\t\t0.000921\n",
      "  validation loss:\t\t0.060679\n",
      "  validation accuracy:\t\t99.16 %\n",
      "Epoch 63 of 200 took 452.933s\n",
      "  training loss:\t\t0.000859\n",
      "  validation loss:\t\t0.061381\n",
      "  validation accuracy:\t\t99.14 %\n",
      "Epoch 64 of 200 took 448.693s\n",
      "  training loss:\t\t0.000804\n",
      "  validation loss:\t\t0.062220\n",
      "  validation accuracy:\t\t99.13 %\n",
      "Epoch 65 of 200 took 448.494s\n",
      "  training loss:\t\t0.000757\n",
      "  validation loss:\t\t0.063036\n",
      "  validation accuracy:\t\t99.13 %\n",
      "Epoch 66 of 200 took 452.810s\n",
      "  training loss:\t\t0.000715\n",
      "  validation loss:\t\t0.063841\n",
      "  validation accuracy:\t\t99.12 %\n",
      "Epoch 67 of 200 took 448.481s\n",
      "  training loss:\t\t0.000678\n",
      "  validation loss:\t\t0.064554\n",
      "  validation accuracy:\t\t99.11 %\n",
      "Epoch 68 of 200 took 448.375s\n",
      "  training loss:\t\t0.000644\n",
      "  validation loss:\t\t0.065249\n",
      "  validation accuracy:\t\t99.12 %\n",
      "Epoch 69 of 200 took 452.767s\n",
      "  training loss:\t\t0.000614\n",
      "  validation loss:\t\t0.065866\n",
      "  validation accuracy:\t\t99.12 %\n",
      "Epoch 70 of 200 took 452.712s\n",
      "  training loss:\t\t0.000587\n",
      "  validation loss:\t\t0.066505\n",
      "  validation accuracy:\t\t99.12 %\n",
      "Epoch 71 of 200 took 453.547s\n",
      "  training loss:\t\t0.000563\n",
      "  validation loss:\t\t0.067044\n",
      "  validation accuracy:\t\t99.12 %\n",
      "Epoch 72 of 200 took 453.544s\n",
      "  training loss:\t\t0.000540\n",
      "  validation loss:\t\t0.067551\n",
      "  validation accuracy:\t\t99.12 %\n",
      "Epoch 73 of 200 took 449.495s\n",
      "  training loss:\t\t0.000519\n",
      "  validation loss:\t\t0.067959\n",
      "  validation accuracy:\t\t99.11 %\n",
      "Epoch 74 of 200 took 452.704s\n",
      "  training loss:\t\t0.000499\n",
      "  validation loss:\t\t0.068419\n",
      "  validation accuracy:\t\t99.10 %\n",
      "Epoch 75 of 200 took 453.015s\n",
      "  training loss:\t\t0.000481\n",
      "  validation loss:\t\t0.068760\n",
      "  validation accuracy:\t\t99.10 %\n",
      "Epoch 76 of 200 took 453.102s\n",
      "  training loss:\t\t0.000464\n",
      "  validation loss:\t\t0.069116\n",
      "  validation accuracy:\t\t99.10 %\n",
      "Epoch 77 of 200 took 448.380s\n",
      "  training loss:\t\t0.000449\n",
      "  validation loss:\t\t0.069384\n",
      "  validation accuracy:\t\t99.11 %\n",
      "Epoch 78 of 200 took 452.773s\n",
      "  training loss:\t\t0.000434\n",
      "  validation loss:\t\t0.069629\n",
      "  validation accuracy:\t\t99.10 %\n",
      "Epoch 79 of 200 took 452.933s\n",
      "  training loss:\t\t0.000420\n",
      "  validation loss:\t\t0.069859\n",
      "  validation accuracy:\t\t99.10 %\n",
      "Epoch 80 of 200 took 453.075s\n",
      "  training loss:\t\t0.000407\n",
      "  validation loss:\t\t0.070053\n",
      "  validation accuracy:\t\t99.10 %\n",
      "Epoch 81 of 200 took 450.184s\n",
      "  training loss:\t\t0.000395\n",
      "  validation loss:\t\t0.070198\n",
      "  validation accuracy:\t\t99.10 %\n",
      "Epoch 82 of 200 took 452.732s\n",
      "  training loss:\t\t0.000383\n",
      "  validation loss:\t\t0.070346\n",
      "  validation accuracy:\t\t99.10 %\n",
      "Epoch 83 of 200 took 452.075s\n",
      "  training loss:\t\t0.000372\n",
      "  validation loss:\t\t0.070420\n",
      "  validation accuracy:\t\t99.10 %\n",
      "Epoch 84 of 200 took 448.519s\n",
      "  training loss:\t\t0.000362\n",
      "  validation loss:\t\t0.070513\n",
      "  validation accuracy:\t\t99.10 %\n",
      "Epoch 85 of 200 took 452.684s\n",
      "  training loss:\t\t0.000352\n",
      "  validation loss:\t\t0.070547\n",
      "  validation accuracy:\t\t99.10 %\n",
      "Epoch 86 of 200 took 449.268s\n",
      "  training loss:\t\t0.000343\n",
      "  validation loss:\t\t0.070676\n",
      "  validation accuracy:\t\t99.11 %\n",
      "Epoch 87 of 200 took 448.791s\n",
      "  training loss:\t\t0.000334\n",
      "  validation loss:\t\t0.070726\n",
      "  validation accuracy:\t\t99.11 %\n",
      "Epoch 88 of 200 took 448.350s\n",
      "  training loss:\t\t0.000326\n",
      "  validation loss:\t\t0.070785\n",
      "  validation accuracy:\t\t99.12 %\n",
      "Epoch 89 of 200 took 448.096s\n",
      "  training loss:\t\t0.000318\n",
      "  validation loss:\t\t0.070841\n",
      "  validation accuracy:\t\t99.12 %\n",
      "Epoch 90 of 200 took 447.995s\n",
      "  training loss:\t\t0.000310\n",
      "  validation loss:\t\t0.070887\n",
      "  validation accuracy:\t\t99.12 %\n",
      "Epoch 91 of 200 took 448.423s\n",
      "  training loss:\t\t0.000303\n",
      "  validation loss:\t\t0.070953\n",
      "  validation accuracy:\t\t99.12 %\n",
      "Epoch 92 of 200 took 448.096s\n",
      "  training loss:\t\t0.000296\n",
      "  validation loss:\t\t0.070981\n",
      "  validation accuracy:\t\t99.13 %\n",
      "Epoch 93 of 200 took 448.200s\n",
      "  training loss:\t\t0.000290\n",
      "  validation loss:\t\t0.071006\n",
      "  validation accuracy:\t\t99.13 %\n",
      "Epoch 94 of 200 took 448.063s\n",
      "  training loss:\t\t0.000283\n",
      "  validation loss:\t\t0.071040\n",
      "  validation accuracy:\t\t99.13 %\n",
      "Epoch 95 of 200 took 448.213s\n",
      "  training loss:\t\t0.000277\n",
      "  validation loss:\t\t0.071081\n",
      "  validation accuracy:\t\t99.13 %\n",
      "Epoch 96 of 200 took 448.007s\n",
      "  training loss:\t\t0.000271\n",
      "  validation loss:\t\t0.071105\n",
      "  validation accuracy:\t\t99.13 %\n",
      "Epoch 97 of 200 took 448.070s\n",
      "  training loss:\t\t0.000266\n",
      "  validation loss:\t\t0.071153\n",
      "  validation accuracy:\t\t99.13 %\n",
      "Epoch 98 of 200 took 448.187s\n",
      "  training loss:\t\t0.000260\n",
      "  validation loss:\t\t0.071209\n",
      "  validation accuracy:\t\t99.13 %\n",
      "Epoch 99 of 200 took 447.945s\n",
      "  training loss:\t\t0.000255\n",
      "  validation loss:\t\t0.071242\n",
      "  validation accuracy:\t\t99.13 %\n",
      "Epoch 100 of 200 took 448.028s\n",
      "  training loss:\t\t0.000250\n",
      "  validation loss:\t\t0.071282\n",
      "  validation accuracy:\t\t99.13 %\n",
      " setting learning rate to 0.0010000\n",
      "Epoch 101 of 200 took 448.164s\n",
      "  training loss:\t\t0.000255\n",
      "  validation loss:\t\t0.067507\n",
      "  validation accuracy:\t\t99.18 %\n",
      "Epoch 102 of 200 took 447.995s\n",
      "  training loss:\t\t0.000245\n",
      "  validation loss:\t\t0.067498\n",
      "  validation accuracy:\t\t99.18 %\n",
      "Epoch 103 of 200 took 448.219s\n",
      "  training loss:\t\t0.000243\n",
      "  validation loss:\t\t0.067491\n",
      "  validation accuracy:\t\t99.18 %\n",
      "Epoch 104 of 200 took 448.155s\n",
      "  training loss:\t\t0.000241\n",
      "  validation loss:\t\t0.067491\n",
      "  validation accuracy:\t\t99.18 %\n",
      "Epoch 105 of 200 took 448.222s\n",
      "  training loss:\t\t0.000240\n",
      "  validation loss:\t\t0.067495\n",
      "  validation accuracy:\t\t99.18 %\n",
      "Epoch 106 of 200 took 448.655s\n",
      "  training loss:\t\t0.000240\n",
      "  validation loss:\t\t0.067501\n",
      "  validation accuracy:\t\t99.18 %\n",
      "Epoch 107 of 200 took 448.215s\n",
      "  training loss:\t\t0.000239\n",
      "  validation loss:\t\t0.067510\n",
      "  validation accuracy:\t\t99.18 %\n",
      "Epoch 108 of 200 took 447.969s\n",
      "  training loss:\t\t0.000238\n",
      "  validation loss:\t\t0.067521\n",
      "  validation accuracy:\t\t99.18 %\n",
      "Epoch 109 of 200 took 448.020s\n",
      "  training loss:\t\t0.000238\n",
      "  validation loss:\t\t0.067532\n",
      "  validation accuracy:\t\t99.18 %\n",
      "Epoch 110 of 200 took 448.257s\n",
      "  training loss:\t\t0.000237\n",
      "  validation loss:\t\t0.067543\n",
      "  validation accuracy:\t\t99.18 %\n",
      "Epoch 111 of 200 took 447.820s\n",
      "  training loss:\t\t0.000236\n",
      "  validation loss:\t\t0.067556\n",
      "  validation accuracy:\t\t99.18 %\n",
      "Epoch 112 of 200 took 448.135s\n",
      "  training loss:\t\t0.000236\n",
      "  validation loss:\t\t0.067568\n",
      "  validation accuracy:\t\t99.18 %\n",
      "Epoch 113 of 200 took 447.936s\n",
      "  training loss:\t\t0.000235\n",
      "  validation loss:\t\t0.067580\n",
      "  validation accuracy:\t\t99.18 %\n",
      "Epoch 114 of 200 took 448.655s\n",
      "  training loss:\t\t0.000234\n",
      "  validation loss:\t\t0.067594\n",
      "  validation accuracy:\t\t99.18 %\n",
      "Epoch 115 of 200 took 448.239s\n",
      "  training loss:\t\t0.000234\n",
      "  validation loss:\t\t0.067606\n",
      "  validation accuracy:\t\t99.18 %\n",
      "Epoch 116 of 200 took 448.215s\n",
      "  training loss:\t\t0.000233\n",
      "  validation loss:\t\t0.067619\n",
      "  validation accuracy:\t\t99.18 %\n",
      "Epoch 117 of 200 took 448.072s\n",
      "  training loss:\t\t0.000233\n",
      "  validation loss:\t\t0.067632\n",
      "  validation accuracy:\t\t99.18 %\n",
      "Epoch 118 of 200 took 447.842s\n",
      "  training loss:\t\t0.000232\n",
      "  validation loss:\t\t0.067645\n",
      "  validation accuracy:\t\t99.18 %\n",
      "Epoch 119 of 200 took 448.010s\n",
      "  training loss:\t\t0.000232\n",
      "  validation loss:\t\t0.067656\n",
      "  validation accuracy:\t\t99.18 %\n",
      "Epoch 120 of 200 took 447.966s\n",
      "  training loss:\t\t0.000231\n",
      "  validation loss:\t\t0.067670\n",
      "  validation accuracy:\t\t99.18 %\n",
      "Epoch 121 of 200 took 448.076s\n",
      "  training loss:\t\t0.000231\n",
      "  validation loss:\t\t0.067681\n",
      "  validation accuracy:\t\t99.18 %\n",
      "Epoch 122 of 200 took 447.805s\n",
      "  training loss:\t\t0.000230\n",
      "  validation loss:\t\t0.067693\n",
      "  validation accuracy:\t\t99.18 %\n",
      "Epoch 123 of 200 took 448.090s\n",
      "  training loss:\t\t0.000229\n",
      "  validation loss:\t\t0.067706\n",
      "  validation accuracy:\t\t99.18 %\n",
      "Epoch 124 of 200 took 447.891s\n",
      "  training loss:\t\t0.000229\n",
      "  validation loss:\t\t0.067717\n",
      "  validation accuracy:\t\t99.18 %\n",
      "Epoch 125 of 200 took 447.785s\n",
      "  training loss:\t\t0.000228\n",
      "  validation loss:\t\t0.067728\n",
      "  validation accuracy:\t\t99.18 %\n",
      "Epoch 126 of 200 took 448.248s\n",
      "  training loss:\t\t0.000228\n",
      "  validation loss:\t\t0.067740\n",
      "  validation accuracy:\t\t99.18 %\n",
      "Epoch 127 of 200 took 448.025s\n",
      "  training loss:\t\t0.000227\n",
      "  validation loss:\t\t0.067750\n",
      "  validation accuracy:\t\t99.18 %\n",
      "Epoch 128 of 200 took 447.897s\n",
      "  training loss:\t\t0.000227\n",
      "  validation loss:\t\t0.067763\n",
      "  validation accuracy:\t\t99.18 %\n",
      "Epoch 129 of 200 took 447.921s\n",
      "  training loss:\t\t0.000226\n",
      "  validation loss:\t\t0.067774\n",
      "  validation accuracy:\t\t99.18 %\n",
      "Epoch 130 of 200 took 447.910s\n",
      "  training loss:\t\t0.000226\n",
      "  validation loss:\t\t0.067785\n",
      "  validation accuracy:\t\t99.18 %\n",
      "Epoch 131 of 200 took 448.055s\n",
      "  training loss:\t\t0.000225\n",
      "  validation loss:\t\t0.067796\n",
      "  validation accuracy:\t\t99.18 %\n",
      "Epoch 132 of 200 took 447.967s\n",
      "  training loss:\t\t0.000225\n",
      "  validation loss:\t\t0.067807\n",
      "  validation accuracy:\t\t99.18 %\n",
      "Epoch 133 of 200 took 447.830s\n",
      "  training loss:\t\t0.000224\n",
      "  validation loss:\t\t0.067819\n",
      "  validation accuracy:\t\t99.18 %\n",
      "Epoch 134 of 200 took 447.748s\n",
      "  training loss:\t\t0.000224\n",
      "  validation loss:\t\t0.067830\n",
      "  validation accuracy:\t\t99.18 %\n",
      "Epoch 135 of 200 took 447.897s\n",
      "  training loss:\t\t0.000224\n",
      "  validation loss:\t\t0.067841\n",
      "  validation accuracy:\t\t99.18 %\n",
      "Epoch 136 of 200 took 447.732s\n",
      "  training loss:\t\t0.000223\n",
      "  validation loss:\t\t0.067851\n",
      "  validation accuracy:\t\t99.18 %\n",
      "Epoch 137 of 200 took 447.722s\n",
      "  training loss:\t\t0.000223\n",
      "  validation loss:\t\t0.067862\n",
      "  validation accuracy:\t\t99.18 %\n",
      "Epoch 138 of 200 took 447.840s\n",
      "  training loss:\t\t0.000222\n",
      "  validation loss:\t\t0.067872\n",
      "  validation accuracy:\t\t99.18 %\n",
      "Epoch 139 of 200 took 447.782s\n",
      "  training loss:\t\t0.000222\n",
      "  validation loss:\t\t0.067883\n",
      "  validation accuracy:\t\t99.18 %\n",
      "Epoch 140 of 200 took 447.934s\n",
      "  training loss:\t\t0.000221\n",
      "  validation loss:\t\t0.067894\n",
      "  validation accuracy:\t\t99.18 %\n",
      "Epoch 141 of 200 took 448.163s\n",
      "  training loss:\t\t0.000221\n",
      "  validation loss:\t\t0.067906\n",
      "  validation accuracy:\t\t99.18 %\n",
      "Epoch 142 of 200 took 447.935s\n",
      "  training loss:\t\t0.000220\n",
      "  validation loss:\t\t0.067916\n",
      "  validation accuracy:\t\t99.18 %\n",
      "Epoch 143 of 200 took 448.110s\n",
      "  training loss:\t\t0.000220\n",
      "  validation loss:\t\t0.067926\n",
      "  validation accuracy:\t\t99.18 %\n",
      "Epoch 144 of 200 took 447.993s\n",
      "  training loss:\t\t0.000219\n",
      "  validation loss:\t\t0.067936\n",
      "  validation accuracy:\t\t99.18 %\n",
      "Epoch 145 of 200 took 447.970s\n",
      "  training loss:\t\t0.000219\n",
      "  validation loss:\t\t0.067947\n",
      "  validation accuracy:\t\t99.18 %\n",
      "Epoch 146 of 200 took 447.918s\n",
      "  training loss:\t\t0.000219\n",
      "  validation loss:\t\t0.067958\n",
      "  validation accuracy:\t\t99.18 %\n",
      "Epoch 147 of 200 took 447.984s\n",
      "  training loss:\t\t0.000218\n",
      "  validation loss:\t\t0.067967\n",
      "  validation accuracy:\t\t99.18 %\n",
      "Epoch 148 of 200 took 447.849s\n",
      "  training loss:\t\t0.000218\n",
      "  validation loss:\t\t0.067978\n",
      "  validation accuracy:\t\t99.18 %\n",
      "Epoch 149 of 200 took 447.891s\n",
      "  training loss:\t\t0.000217\n",
      "  validation loss:\t\t0.067989\n",
      "  validation accuracy:\t\t99.18 %\n",
      "Epoch 150 of 200 took 447.771s\n",
      "  training loss:\t\t0.000217\n",
      "  validation loss:\t\t0.067999\n",
      "  validation accuracy:\t\t99.18 %\n",
      " setting learning rate to 0.0001000\n",
      "Epoch 151 of 200 took 448.041s\n",
      "  training loss:\t\t0.000216\n",
      "  validation loss:\t\t0.067961\n",
      "  validation accuracy:\t\t99.18 %\n",
      "Epoch 152 of 200 took 448.017s\n",
      "  training loss:\t\t0.000215\n",
      "  validation loss:\t\t0.067899\n",
      "  validation accuracy:\t\t99.18 %\n",
      "Epoch 153 of 200 took 447.962s\n",
      "  training loss:\t\t0.000215\n",
      "  validation loss:\t\t0.067875\n",
      "  validation accuracy:\t\t99.18 %\n",
      "Epoch 154 of 200 took 447.751s\n",
      "  training loss:\t\t0.000215\n",
      "  validation loss:\t\t0.067865\n",
      "  validation accuracy:\t\t99.18 %\n",
      "Epoch 155 of 200 took 448.082s\n",
      "  training loss:\t\t0.000215\n",
      "  validation loss:\t\t0.067862\n",
      "  validation accuracy:\t\t99.18 %\n",
      "Epoch 156 of 200 took 447.968s\n",
      "  training loss:\t\t0.000215\n",
      "  validation loss:\t\t0.067860\n",
      "  validation accuracy:\t\t99.18 %\n",
      "Epoch 157 of 200 took 447.833s\n",
      "  training loss:\t\t0.000215\n",
      "  validation loss:\t\t0.067860\n",
      "  validation accuracy:\t\t99.18 %\n",
      "Epoch 158 of 200 took 447.916s\n",
      "  training loss:\t\t0.000215\n",
      "  validation loss:\t\t0.067861\n",
      "  validation accuracy:\t\t99.18 %\n",
      "Epoch 159 of 200 took 448.028s\n",
      "  training loss:\t\t0.000215\n",
      "  validation loss:\t\t0.067862\n",
      "  validation accuracy:\t\t99.18 %\n",
      "Epoch 160 of 200 took 447.987s\n",
      "  training loss:\t\t0.000215\n",
      "  validation loss:\t\t0.067862\n",
      "  validation accuracy:\t\t99.18 %\n",
      "Epoch 161 of 200 took 447.831s\n",
      "  training loss:\t\t0.000215\n",
      "  validation loss:\t\t0.067863\n",
      "  validation accuracy:\t\t99.18 %\n",
      "Epoch 162 of 200 took 447.939s\n",
      "  training loss:\t\t0.000214\n",
      "  validation loss:\t\t0.067864\n",
      "  validation accuracy:\t\t99.18 %\n",
      "Epoch 163 of 200 took 447.776s\n",
      "  training loss:\t\t0.000214\n",
      "  validation loss:\t\t0.067864\n",
      "  validation accuracy:\t\t99.18 %\n",
      "Epoch 164 of 200 took 447.694s\n",
      "  training loss:\t\t0.000214\n",
      "  validation loss:\t\t0.067865\n",
      "  validation accuracy:\t\t99.18 %\n",
      "Epoch 165 of 200 took 447.785s\n",
      "  training loss:\t\t0.000214\n",
      "  validation loss:\t\t0.067866\n",
      "  validation accuracy:\t\t99.18 %\n",
      "Epoch 166 of 200 took 447.931s\n",
      "  training loss:\t\t0.000214\n",
      "  validation loss:\t\t0.067867\n",
      "  validation accuracy:\t\t99.18 %\n",
      "Epoch 167 of 200 took 447.747s\n",
      "  training loss:\t\t0.000214\n",
      "  validation loss:\t\t0.067867\n",
      "  validation accuracy:\t\t99.18 %\n",
      "Epoch 168 of 200 took 447.762s\n",
      "  training loss:\t\t0.000214\n",
      "  validation loss:\t\t0.067868\n",
      "  validation accuracy:\t\t99.18 %\n",
      "Epoch 169 of 200 took 447.800s\n",
      "  training loss:\t\t0.000214\n",
      "  validation loss:\t\t0.067869\n",
      "  validation accuracy:\t\t99.18 %\n",
      "Epoch 170 of 200 took 447.765s\n",
      "  training loss:\t\t0.000214\n",
      "  validation loss:\t\t0.067870\n",
      "  validation accuracy:\t\t99.18 %\n",
      "Epoch 171 of 200 took 447.775s\n",
      "  training loss:\t\t0.000214\n",
      "  validation loss:\t\t0.067871\n",
      "  validation accuracy:\t\t99.18 %\n",
      "Epoch 172 of 200 took 447.840s\n",
      "  training loss:\t\t0.000214\n",
      "  validation loss:\t\t0.067871\n",
      "  validation accuracy:\t\t99.18 %\n",
      "Epoch 173 of 200 took 447.961s\n",
      "  training loss:\t\t0.000214\n",
      "  validation loss:\t\t0.067872\n",
      "  validation accuracy:\t\t99.18 %\n",
      "Epoch 174 of 200 took 447.894s\n",
      "  training loss:\t\t0.000214\n",
      "  validation loss:\t\t0.067873\n",
      "  validation accuracy:\t\t99.18 %\n",
      "Epoch 175 of 200 took 447.788s\n",
      "  training loss:\t\t0.000214\n",
      "  validation loss:\t\t0.067874\n",
      "  validation accuracy:\t\t99.18 %\n",
      "Epoch 176 of 200 took 447.814s\n",
      "  training loss:\t\t0.000214\n",
      "  validation loss:\t\t0.067875\n",
      "  validation accuracy:\t\t99.18 %\n",
      "Epoch 177 of 200 took 447.906s\n",
      "  training loss:\t\t0.000214\n",
      "  validation loss:\t\t0.067875\n",
      "  validation accuracy:\t\t99.18 %\n",
      "Epoch 178 of 200 took 447.891s\n",
      "  training loss:\t\t0.000214\n",
      "  validation loss:\t\t0.067876\n",
      "  validation accuracy:\t\t99.18 %\n",
      "Epoch 179 of 200 took 447.759s\n",
      "  training loss:\t\t0.000214\n",
      "  validation loss:\t\t0.067877\n",
      "  validation accuracy:\t\t99.18 %\n",
      "Epoch 180 of 200 took 447.675s\n",
      "  training loss:\t\t0.000214\n",
      "  validation loss:\t\t0.067878\n",
      "  validation accuracy:\t\t99.18 %\n",
      "Epoch 181 of 200 took 447.829s\n",
      "  training loss:\t\t0.000214\n",
      "  validation loss:\t\t0.067879\n",
      "  validation accuracy:\t\t99.18 %\n",
      "Epoch 182 of 200 took 447.811s\n",
      "  training loss:\t\t0.000214\n",
      "  validation loss:\t\t0.067879\n",
      "  validation accuracy:\t\t99.18 %\n",
      "Epoch 183 of 200 took 447.942s\n",
      "  training loss:\t\t0.000214\n",
      "  validation loss:\t\t0.067880\n",
      "  validation accuracy:\t\t99.18 %\n",
      "Epoch 184 of 200 took 448.123s\n",
      "  training loss:\t\t0.000214\n",
      "  validation loss:\t\t0.067881\n",
      "  validation accuracy:\t\t99.18 %\n",
      "Epoch 185 of 200 took 448.242s\n",
      "  training loss:\t\t0.000214\n",
      "  validation loss:\t\t0.067882\n",
      "  validation accuracy:\t\t99.18 %\n",
      "Epoch 186 of 200 took 448.107s\n",
      "  training loss:\t\t0.000214\n",
      "  validation loss:\t\t0.067882\n",
      "  validation accuracy:\t\t99.18 %\n",
      "Epoch 187 of 200 took 450.294s\n",
      "  training loss:\t\t0.000214\n",
      "  validation loss:\t\t0.067883\n",
      "  validation accuracy:\t\t99.18 %\n",
      "Epoch 188 of 200 took 448.929s\n",
      "  training loss:\t\t0.000213\n",
      "  validation loss:\t\t0.067884\n",
      "  validation accuracy:\t\t99.18 %\n",
      "Epoch 189 of 200 took 453.019s\n",
      "  training loss:\t\t0.000213\n",
      "  validation loss:\t\t0.067885\n",
      "  validation accuracy:\t\t99.18 %\n",
      "Epoch 190 of 200 took 453.020s\n",
      "  training loss:\t\t0.000213\n",
      "  validation loss:\t\t0.067885\n",
      "  validation accuracy:\t\t99.18 %\n",
      "Epoch 191 of 200 took 453.102s\n",
      "  training loss:\t\t0.000213\n",
      "  validation loss:\t\t0.067886\n",
      "  validation accuracy:\t\t99.18 %\n",
      "Epoch 192 of 200 took 453.259s\n",
      "  training loss:\t\t0.000213\n",
      "  validation loss:\t\t0.067887\n",
      "  validation accuracy:\t\t99.18 %\n",
      "Epoch 193 of 200 took 453.896s\n",
      "  training loss:\t\t0.000213\n",
      "  validation loss:\t\t0.067887\n",
      "  validation accuracy:\t\t99.18 %\n",
      "Epoch 194 of 200 took 453.337s\n",
      "  training loss:\t\t0.000213\n",
      "  validation loss:\t\t0.067888\n",
      "  validation accuracy:\t\t99.18 %\n",
      "Epoch 195 of 200 took 456.470s\n",
      "  training loss:\t\t0.000213\n",
      "  validation loss:\t\t0.067889\n",
      "  validation accuracy:\t\t99.18 %\n",
      "Epoch 196 of 200 took 458.355s\n",
      "  training loss:\t\t0.000213\n",
      "  validation loss:\t\t0.067890\n",
      "  validation accuracy:\t\t99.18 %\n",
      "Epoch 197 of 200 took 454.195s\n",
      "  training loss:\t\t0.000213\n",
      "  validation loss:\t\t0.067891\n",
      "  validation accuracy:\t\t99.18 %\n",
      "Epoch 198 of 200 took 458.123s\n",
      "  training loss:\t\t0.000213\n",
      "  validation loss:\t\t0.067891\n",
      "  validation accuracy:\t\t99.18 %\n",
      "Epoch 199 of 200 took 453.852s\n",
      "  training loss:\t\t0.000213\n",
      "  validation loss:\t\t0.067892\n",
      "  validation accuracy:\t\t99.18 %\n",
      "Epoch 200 of 200 took 458.350s\n",
      "  training loss:\t\t0.000213\n",
      "  validation loss:\t\t0.067893\n",
      "  validation accuracy:\t\t99.18 %\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Finally, launch the training loop.\n",
    "print(\"Starting training...\")\n",
    "# We iterate over epochs:\n",
    "for epoch in range(num_epochs):\n",
    "    if epoch in learning_rate_schedule:\n",
    "        lr = np.float32(learning_rate_schedule[epoch])\n",
    "        print(\" setting learning rate to %.7f\" % lr)\n",
    "        learning_rate.set_value(lr)\n",
    "    # In each epoch, we do a full pass over the training data:\n",
    "    train_err = 0\n",
    "    train_batches = 0\n",
    "    start_time = time.time()\n",
    "    for batch in iterate_minibatches(X_train2, y_train2, 500, shuffle=False):\n",
    "        inputs, targets = batch\n",
    "        train_err += train_fn(inputs, targets)\n",
    "        train_batches += 1\n",
    "\n",
    "    # And a full pass over the validation data:\n",
    "    val_err = 0\n",
    "    val_acc = 0\n",
    "    val_batches = 0\n",
    "    for batch in iterate_minibatches(X_val, y_val, 500, shuffle=False):\n",
    "        inputs, targets = batch\n",
    "        err, acc = val_fn(inputs, targets)\n",
    "        val_err += err\n",
    "        val_acc += acc\n",
    "        val_batches += 1\n",
    "\n",
    "    # Then we print the results for this epoch:\n",
    "    print(\"Epoch {} of {} took {:.3f}s\".format(\n",
    "        epoch + 1, num_epochs, time.time() - start_time))\n",
    "    print(\"  training loss:\\t\\t{:.6f}\".format(train_err / train_batches))\n",
    "    print(\"  validation loss:\\t\\t{:.6f}\".format(val_err / val_batches))\n",
    "    print(\"  validation accuracy:\\t\\t{:.2f} %\".format(\n",
    "        val_acc / val_batches * 100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final results:\n",
      "  test loss:\t\t\t0.728302\n",
      "  test accuracy:\t\t91.67 %\n"
     ]
    }
   ],
   "source": [
    "# After training, we compute and print the test error:\n",
    "test_err = 0\n",
    "test_acc = 0\n",
    "test_batches = 0\n",
    "for batch in iterate_minibatches(X_test, y_test, 500, shuffle=False):\n",
    "    inputs, targets = batch\n",
    "    err, acc = val_fn(inputs, targets)\n",
    "    test_err += err\n",
    "    test_acc += acc\n",
    "    test_batches += 1\n",
    "print(\"Final results:\")\n",
    "print(\"  test loss:\\t\\t\\t{:.6f}\".format(test_err / test_batches))\n",
    "print(\"  test accuracy:\\t\\t{:.2f} %\".format(\n",
    "    test_acc / test_batches * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n",
      " setting learning rate to 0.0001000\n",
      "Epoch 1 of 200 took 520.895s\n",
      "  training loss:\t\t0.317072\n",
      "  validation loss:\t\t0.254573\n",
      "  validation accuracy:\t\t90.61 %\n",
      "Epoch 2 of 200 took 521.402s\n",
      "  training loss:\t\t0.225555\n",
      "  validation loss:\t\t0.222932\n",
      "  validation accuracy:\t\t91.76 %\n",
      " setting learning rate to 0.0100000\n",
      "Epoch 3 of 200 took 518.592s\n",
      "  training loss:\t\t0.575495\n",
      "  validation loss:\t\t0.214508\n",
      "  validation accuracy:\t\t91.87 %\n",
      "Epoch 4 of 200 took 518.519s\n",
      "  training loss:\t\t0.209358\n",
      "  validation loss:\t\t0.202045\n",
      "  validation accuracy:\t\t92.51 %\n",
      "Epoch 5 of 200 took 518.630s\n",
      "  training loss:\t\t0.196816\n",
      "  validation loss:\t\t0.194025\n",
      "  validation accuracy:\t\t92.88 %\n",
      "Epoch 6 of 200 took 518.534s\n",
      "  training loss:\t\t0.187074\n",
      "  validation loss:\t\t0.186987\n",
      "  validation accuracy:\t\t93.22 %\n",
      "Epoch 7 of 200 took 518.552s\n",
      "  training loss:\t\t0.178218\n",
      "  validation loss:\t\t0.180372\n",
      "  validation accuracy:\t\t93.37 %\n",
      "Epoch 8 of 200 took 518.561s\n",
      "  training loss:\t\t0.169869\n",
      "  validation loss:\t\t0.174328\n",
      "  validation accuracy:\t\t93.68 %\n",
      "Epoch 9 of 200 took 518.510s\n",
      "  training loss:\t\t0.161694\n",
      "  validation loss:\t\t0.169427\n",
      "  validation accuracy:\t\t93.89 %\n",
      "Epoch 10 of 200 took 518.569s\n",
      "  training loss:\t\t0.153662\n",
      "  validation loss:\t\t0.164586\n",
      "  validation accuracy:\t\t94.12 %\n",
      "Epoch 11 of 200 took 518.548s\n",
      "  training loss:\t\t0.145438\n",
      "  validation loss:\t\t0.160083\n",
      "  validation accuracy:\t\t94.36 %\n",
      "Epoch 12 of 200 took 518.511s\n",
      "  training loss:\t\t0.137135\n",
      "  validation loss:\t\t0.156126\n",
      "  validation accuracy:\t\t94.49 %\n",
      "Epoch 13 of 200 took 518.572s\n",
      "  training loss:\t\t0.128795\n",
      "  validation loss:\t\t0.152451\n",
      "  validation accuracy:\t\t94.60 %\n",
      "Epoch 14 of 200 took 518.577s\n",
      "  training loss:\t\t0.120065\n",
      "  validation loss:\t\t0.147664\n",
      "  validation accuracy:\t\t94.78 %\n",
      "Epoch 15 of 200 took 518.541s\n",
      "  training loss:\t\t0.111027\n",
      "  validation loss:\t\t0.142362\n",
      "  validation accuracy:\t\t94.97 %\n",
      "Epoch 16 of 200 took 518.511s\n",
      "  training loss:\t\t0.102170\n",
      "  validation loss:\t\t0.140912\n",
      "  validation accuracy:\t\t95.16 %\n",
      "Epoch 17 of 200 took 518.523s\n",
      "  training loss:\t\t0.093407\n",
      "  validation loss:\t\t0.135118\n",
      "  validation accuracy:\t\t95.49 %\n",
      "Epoch 18 of 200 took 518.567s\n",
      "  training loss:\t\t0.084493\n",
      "  validation loss:\t\t0.136140\n",
      "  validation accuracy:\t\t95.55 %\n",
      "Epoch 19 of 200 took 518.555s\n",
      "  training loss:\t\t0.075657\n",
      "  validation loss:\t\t0.131702\n",
      "  validation accuracy:\t\t95.74 %\n",
      "Epoch 20 of 200 took 518.491s\n",
      "  training loss:\t\t0.067624\n",
      "  validation loss:\t\t0.130959\n",
      "  validation accuracy:\t\t95.85 %\n",
      "Epoch 21 of 200 took 518.471s\n",
      "  training loss:\t\t0.059544\n",
      "  validation loss:\t\t0.123196\n",
      "  validation accuracy:\t\t96.21 %\n",
      "Epoch 22 of 200 took 518.517s\n",
      "  training loss:\t\t0.051988\n",
      "  validation loss:\t\t0.127672\n",
      "  validation accuracy:\t\t96.24 %\n",
      "Epoch 23 of 200 took 518.481s\n",
      "  training loss:\t\t0.045835\n",
      "  validation loss:\t\t0.120618\n",
      "  validation accuracy:\t\t96.38 %\n",
      "Epoch 24 of 200 took 518.559s\n",
      "  training loss:\t\t0.039916\n",
      "  validation loss:\t\t0.121272\n",
      "  validation accuracy:\t\t96.49 %\n",
      "Epoch 25 of 200 took 518.543s\n",
      "  training loss:\t\t0.034880\n",
      "  validation loss:\t\t0.126857\n",
      "  validation accuracy:\t\t96.40 %\n",
      "Epoch 26 of 200 took 518.493s\n",
      "  training loss:\t\t0.031243\n",
      "  validation loss:\t\t0.118250\n",
      "  validation accuracy:\t\t96.76 %\n",
      "Epoch 27 of 200 took 518.433s\n",
      "  training loss:\t\t0.026594\n",
      "  validation loss:\t\t0.126496\n",
      "  validation accuracy:\t\t96.67 %\n",
      "Epoch 28 of 200 took 518.534s\n",
      "  training loss:\t\t0.022666\n",
      "  validation loss:\t\t0.119378\n",
      "  validation accuracy:\t\t96.95 %\n",
      "Epoch 29 of 200 took 518.449s\n",
      "  training loss:\t\t0.020202\n",
      "  validation loss:\t\t0.105542\n",
      "  validation accuracy:\t\t97.38 %\n",
      "Epoch 30 of 200 took 518.486s\n",
      "  training loss:\t\t0.017515\n",
      "  validation loss:\t\t0.102463\n",
      "  validation accuracy:\t\t97.56 %\n",
      "Epoch 31 of 200 took 518.494s\n",
      "  training loss:\t\t0.015175\n",
      "  validation loss:\t\t0.105400\n",
      "  validation accuracy:\t\t97.66 %\n",
      "Epoch 32 of 200 took 518.548s\n",
      "  training loss:\t\t0.013565\n",
      "  validation loss:\t\t0.100701\n",
      "  validation accuracy:\t\t97.63 %\n",
      "Epoch 33 of 200 took 518.504s\n",
      "  training loss:\t\t0.011527\n",
      "  validation loss:\t\t0.088880\n",
      "  validation accuracy:\t\t98.13 %\n",
      "Epoch 34 of 200 took 518.541s\n",
      "  training loss:\t\t0.009516\n",
      "  validation loss:\t\t0.083056\n",
      "  validation accuracy:\t\t98.25 %\n",
      "Epoch 35 of 200 took 518.499s\n",
      "  training loss:\t\t0.008707\n",
      "  validation loss:\t\t0.079188\n",
      "  validation accuracy:\t\t98.39 %\n",
      "Epoch 36 of 200 took 518.540s\n",
      "  training loss:\t\t0.006080\n",
      "  validation loss:\t\t0.082402\n",
      "  validation accuracy:\t\t98.56 %\n",
      "Epoch 37 of 200 took 518.557s\n",
      "  training loss:\t\t0.005334\n",
      "  validation loss:\t\t0.076367\n",
      "  validation accuracy:\t\t98.49 %\n",
      "Epoch 38 of 200 took 518.475s\n",
      "  training loss:\t\t0.004400\n",
      "  validation loss:\t\t0.076664\n",
      "  validation accuracy:\t\t98.70 %\n",
      "Epoch 39 of 200 took 518.515s\n",
      "  training loss:\t\t0.002556\n",
      "  validation loss:\t\t0.078912\n",
      "  validation accuracy:\t\t98.83 %\n",
      "Epoch 40 of 200 took 518.562s\n",
      "  training loss:\t\t0.001349\n",
      "  validation loss:\t\t0.075041\n",
      "  validation accuracy:\t\t98.98 %\n",
      "Epoch 41 of 200 took 518.468s\n",
      "  training loss:\t\t0.000932\n",
      "  validation loss:\t\t0.074579\n",
      "  validation accuracy:\t\t99.04 %\n",
      "Epoch 42 of 200 took 518.553s\n",
      "  training loss:\t\t0.000572\n",
      "  validation loss:\t\t0.076382\n",
      "  validation accuracy:\t\t99.03 %\n",
      "Epoch 43 of 200 took 518.555s\n",
      "  training loss:\t\t0.000542\n",
      "  validation loss:\t\t0.076466\n",
      "  validation accuracy:\t\t99.04 %\n",
      "Epoch 44 of 200 took 518.511s\n",
      "  training loss:\t\t0.000445\n",
      "  validation loss:\t\t0.077455\n",
      "  validation accuracy:\t\t99.00 %\n",
      "Epoch 45 of 200 took 518.488s\n",
      "  training loss:\t\t0.000427\n",
      "  validation loss:\t\t0.076310\n",
      "  validation accuracy:\t\t99.04 %\n",
      "Epoch 46 of 200 took 518.510s\n",
      "  training loss:\t\t0.000322\n",
      "  validation loss:\t\t0.078473\n",
      "  validation accuracy:\t\t99.02 %\n",
      "Epoch 47 of 200 took 518.559s\n",
      "  training loss:\t\t0.000280\n",
      "  validation loss:\t\t0.078704\n",
      "  validation accuracy:\t\t99.02 %\n",
      "Epoch 48 of 200 took 518.594s\n",
      "  training loss:\t\t0.000275\n",
      "  validation loss:\t\t0.079658\n",
      "  validation accuracy:\t\t99.03 %\n",
      "Epoch 49 of 200 took 518.589s\n",
      "  training loss:\t\t0.000272\n",
      "  validation loss:\t\t0.080147\n",
      "  validation accuracy:\t\t99.02 %\n",
      "Epoch 50 of 200 took 518.513s\n",
      "  training loss:\t\t0.000237\n",
      "  validation loss:\t\t0.079687\n",
      "  validation accuracy:\t\t99.02 %\n",
      "Epoch 51 of 200 took 518.558s\n",
      "  training loss:\t\t0.000224\n",
      "  validation loss:\t\t0.080518\n",
      "  validation accuracy:\t\t99.04 %\n",
      "Epoch 52 of 200 took 518.501s\n",
      "  training loss:\t\t0.000204\n",
      "  validation loss:\t\t0.080893\n",
      "  validation accuracy:\t\t99.04 %\n",
      "Epoch 53 of 200 took 518.598s\n",
      "  training loss:\t\t0.000194\n",
      "  validation loss:\t\t0.081220\n",
      "  validation accuracy:\t\t99.03 %\n",
      "Epoch 54 of 200 took 518.595s\n",
      "  training loss:\t\t0.000182\n",
      "  validation loss:\t\t0.082189\n",
      "  validation accuracy:\t\t99.03 %\n",
      "Epoch 55 of 200 took 518.687s\n",
      "  training loss:\t\t0.000174\n",
      "  validation loss:\t\t0.082701\n",
      "  validation accuracy:\t\t99.04 %\n",
      "Epoch 56 of 200 took 518.612s\n",
      "  training loss:\t\t0.000168\n",
      "  validation loss:\t\t0.083316\n",
      "  validation accuracy:\t\t99.03 %\n",
      "Epoch 57 of 200 took 518.590s\n",
      "  training loss:\t\t0.000160\n",
      "  validation loss:\t\t0.082631\n",
      "  validation accuracy:\t\t99.04 %\n",
      "Epoch 58 of 200 took 518.699s\n",
      "  training loss:\t\t0.000155\n",
      "  validation loss:\t\t0.082949\n",
      "  validation accuracy:\t\t99.04 %\n",
      "Epoch 59 of 200 took 518.613s\n",
      "  training loss:\t\t0.000147\n",
      "  validation loss:\t\t0.084121\n",
      "  validation accuracy:\t\t99.03 %\n",
      "Epoch 60 of 200 took 518.560s\n",
      "  training loss:\t\t0.000143\n",
      "  validation loss:\t\t0.084106\n",
      "  validation accuracy:\t\t99.03 %\n",
      "Epoch 61 of 200 took 518.630s\n",
      "  training loss:\t\t0.000140\n",
      "  validation loss:\t\t0.084109\n",
      "  validation accuracy:\t\t99.03 %\n",
      "Epoch 62 of 200 took 518.638s\n",
      "  training loss:\t\t0.000133\n",
      "  validation loss:\t\t0.084849\n",
      "  validation accuracy:\t\t99.03 %\n",
      "Epoch 63 of 200 took 518.666s\n",
      "  training loss:\t\t0.000132\n",
      "  validation loss:\t\t0.084728\n",
      "  validation accuracy:\t\t99.03 %\n",
      "Epoch 64 of 200 took 518.645s\n",
      "  training loss:\t\t0.000126\n",
      "  validation loss:\t\t0.085354\n",
      "  validation accuracy:\t\t99.03 %\n",
      "Epoch 65 of 200 took 518.637s\n",
      "  training loss:\t\t0.000121\n",
      "  validation loss:\t\t0.085525\n",
      "  validation accuracy:\t\t99.04 %\n",
      "Epoch 66 of 200 took 518.628s\n",
      "  training loss:\t\t0.000118\n",
      "  validation loss:\t\t0.085811\n",
      "  validation accuracy:\t\t99.04 %\n",
      "Epoch 67 of 200 took 518.510s\n",
      "  training loss:\t\t0.000116\n",
      "  validation loss:\t\t0.085493\n",
      "  validation accuracy:\t\t99.02 %\n",
      "Epoch 68 of 200 took 518.603s\n",
      "  training loss:\t\t0.000112\n",
      "  validation loss:\t\t0.086174\n",
      "  validation accuracy:\t\t99.03 %\n",
      "Epoch 69 of 200 took 518.564s\n",
      "  training loss:\t\t0.000109\n",
      "  validation loss:\t\t0.086267\n",
      "  validation accuracy:\t\t99.03 %\n",
      "Epoch 70 of 200 took 518.486s\n",
      "  training loss:\t\t0.000106\n",
      "  validation loss:\t\t0.086326\n",
      "  validation accuracy:\t\t99.03 %\n",
      "Epoch 71 of 200 took 518.585s\n",
      "  training loss:\t\t0.000104\n",
      "  validation loss:\t\t0.085958\n",
      "  validation accuracy:\t\t99.05 %\n",
      "Epoch 72 of 200 took 518.665s\n",
      "  training loss:\t\t0.000102\n",
      "  validation loss:\t\t0.087503\n",
      "  validation accuracy:\t\t99.02 %\n",
      "Epoch 73 of 200 took 518.688s\n",
      "  training loss:\t\t0.000099\n",
      "  validation loss:\t\t0.086480\n",
      "  validation accuracy:\t\t99.05 %\n",
      "Epoch 74 of 200 took 518.567s\n",
      "  training loss:\t\t0.000096\n",
      "  validation loss:\t\t0.087493\n",
      "  validation accuracy:\t\t99.03 %\n",
      "Epoch 75 of 200 took 518.639s\n",
      "  training loss:\t\t0.000093\n",
      "  validation loss:\t\t0.088101\n",
      "  validation accuracy:\t\t99.03 %\n",
      "Epoch 76 of 200 took 518.631s\n",
      "  training loss:\t\t0.000092\n",
      "  validation loss:\t\t0.087986\n",
      "  validation accuracy:\t\t99.03 %\n",
      "Epoch 77 of 200 took 518.664s\n",
      "  training loss:\t\t0.000090\n",
      "  validation loss:\t\t0.087958\n",
      "  validation accuracy:\t\t99.04 %\n",
      "Epoch 78 of 200 took 518.648s\n",
      "  training loss:\t\t0.000089\n",
      "  validation loss:\t\t0.087689\n",
      "  validation accuracy:\t\t99.04 %\n",
      "Epoch 79 of 200 took 518.641s\n",
      "  training loss:\t\t0.000087\n",
      "  validation loss:\t\t0.087991\n",
      "  validation accuracy:\t\t99.03 %\n",
      "Epoch 80 of 200 took 518.614s\n",
      "  training loss:\t\t0.000085\n",
      "  validation loss:\t\t0.088197\n",
      "  validation accuracy:\t\t99.04 %\n",
      "Epoch 81 of 200 took 518.635s\n",
      "  training loss:\t\t0.000084\n",
      "  validation loss:\t\t0.088901\n",
      "  validation accuracy:\t\t99.03 %\n",
      "Epoch 82 of 200 took 518.526s\n",
      "  training loss:\t\t0.000081\n",
      "  validation loss:\t\t0.088944\n",
      "  validation accuracy:\t\t99.04 %\n",
      "Epoch 83 of 200 took 518.565s\n",
      "  training loss:\t\t0.000080\n",
      "  validation loss:\t\t0.089500\n",
      "  validation accuracy:\t\t99.04 %\n",
      "Epoch 84 of 200 took 518.624s\n",
      "  training loss:\t\t0.000079\n",
      "  validation loss:\t\t0.089642\n",
      "  validation accuracy:\t\t99.03 %\n",
      "Epoch 85 of 200 took 518.603s\n",
      "  training loss:\t\t0.000076\n",
      "  validation loss:\t\t0.089621\n",
      "  validation accuracy:\t\t99.02 %\n",
      "Epoch 86 of 200 took 518.563s\n",
      "  training loss:\t\t0.000075\n",
      "  validation loss:\t\t0.089303\n",
      "  validation accuracy:\t\t99.05 %\n",
      "Epoch 87 of 200 took 518.669s\n",
      "  training loss:\t\t0.000075\n",
      "  validation loss:\t\t0.089364\n",
      "  validation accuracy:\t\t99.05 %\n",
      "Epoch 88 of 200 took 518.625s\n",
      "  training loss:\t\t0.000073\n",
      "  validation loss:\t\t0.089726\n",
      "  validation accuracy:\t\t99.03 %\n",
      "Epoch 89 of 200 took 518.678s\n",
      "  training loss:\t\t0.000071\n",
      "  validation loss:\t\t0.090202\n",
      "  validation accuracy:\t\t99.04 %\n",
      "Epoch 90 of 200 took 518.640s\n",
      "  training loss:\t\t0.000070\n",
      "  validation loss:\t\t0.090502\n",
      "  validation accuracy:\t\t99.03 %\n",
      "Epoch 91 of 200 took 518.663s\n",
      "  training loss:\t\t0.000070\n",
      "  validation loss:\t\t0.090185\n",
      "  validation accuracy:\t\t99.03 %\n",
      "Epoch 92 of 200 took 518.729s\n",
      "  training loss:\t\t0.000069\n",
      "  validation loss:\t\t0.090351\n",
      "  validation accuracy:\t\t99.02 %\n",
      "Epoch 93 of 200 took 518.680s\n",
      "  training loss:\t\t0.000067\n",
      "  validation loss:\t\t0.090103\n",
      "  validation accuracy:\t\t99.04 %\n",
      "Epoch 94 of 200 took 518.591s\n",
      "  training loss:\t\t0.000065\n",
      "  validation loss:\t\t0.090656\n",
      "  validation accuracy:\t\t99.04 %\n",
      "Epoch 95 of 200 took 518.625s\n",
      "  training loss:\t\t0.000065\n",
      "  validation loss:\t\t0.090612\n",
      "  validation accuracy:\t\t99.03 %\n",
      "Epoch 96 of 200 took 518.610s\n",
      "  training loss:\t\t0.000064\n",
      "  validation loss:\t\t0.091399\n",
      "  validation accuracy:\t\t99.03 %\n",
      "Epoch 97 of 200 took 518.673s\n",
      "  training loss:\t\t0.000063\n",
      "  validation loss:\t\t0.091596\n",
      "  validation accuracy:\t\t99.03 %\n",
      "Epoch 98 of 200 took 518.575s\n",
      "  training loss:\t\t0.000061\n",
      "  validation loss:\t\t0.091644\n",
      "  validation accuracy:\t\t99.03 %\n",
      "Epoch 99 of 200 took 518.734s\n",
      "  training loss:\t\t0.000061\n",
      "  validation loss:\t\t0.091931\n",
      "  validation accuracy:\t\t99.02 %\n",
      "Epoch 100 of 200 took 518.632s\n",
      "  training loss:\t\t0.000060\n",
      "  validation loss:\t\t0.091425\n",
      "  validation accuracy:\t\t99.03 %\n",
      " setting learning rate to 0.0010000\n",
      "Epoch 101 of 200 took 518.654s\n",
      "  training loss:\t\t0.000059\n",
      "  validation loss:\t\t0.091799\n",
      "  validation accuracy:\t\t99.03 %\n",
      "Epoch 102 of 200 took 518.617s\n",
      "  training loss:\t\t0.000059\n",
      "  validation loss:\t\t0.091314\n",
      "  validation accuracy:\t\t99.04 %\n",
      "Epoch 103 of 200 took 518.612s\n",
      "  training loss:\t\t0.000059\n",
      "  validation loss:\t\t0.092240\n",
      "  validation accuracy:\t\t99.02 %\n",
      "Epoch 104 of 200 took 518.616s\n",
      "  training loss:\t\t0.000058\n",
      "  validation loss:\t\t0.091728\n",
      "  validation accuracy:\t\t99.03 %\n",
      "Epoch 105 of 200 took 518.684s\n",
      "  training loss:\t\t0.000058\n",
      "  validation loss:\t\t0.091858\n",
      "  validation accuracy:\t\t99.02 %\n",
      "Epoch 106 of 200 took 518.601s\n",
      "  training loss:\t\t0.000058\n",
      "  validation loss:\t\t0.091655\n",
      "  validation accuracy:\t\t99.04 %\n",
      "Epoch 107 of 200 took 518.664s\n",
      "  training loss:\t\t0.000058\n",
      "  validation loss:\t\t0.091630\n",
      "  validation accuracy:\t\t99.03 %\n",
      "Epoch 108 of 200 took 518.624s\n",
      "  training loss:\t\t0.000058\n",
      "  validation loss:\t\t0.091361\n",
      "  validation accuracy:\t\t99.04 %\n",
      "Epoch 109 of 200 took 520.673s\n",
      "  training loss:\t\t0.000058\n",
      "  validation loss:\t\t0.092065\n",
      "  validation accuracy:\t\t99.03 %\n",
      "Epoch 110 of 200 took 521.561s\n",
      "  training loss:\t\t0.000058\n",
      "  validation loss:\t\t0.092168\n",
      "  validation accuracy:\t\t99.02 %\n",
      "Epoch 111 of 200 took 519.679s\n",
      "  training loss:\t\t0.000058\n",
      "  validation loss:\t\t0.092052\n",
      "  validation accuracy:\t\t99.02 %\n",
      "Epoch 112 of 200 took 518.628s\n",
      "  training loss:\t\t0.000058\n",
      "  validation loss:\t\t0.091276\n",
      "  validation accuracy:\t\t99.03 %\n",
      "Epoch 113 of 200 took 518.701s\n",
      "  training loss:\t\t0.000058\n",
      "  validation loss:\t\t0.092112\n",
      "  validation accuracy:\t\t99.03 %\n",
      "Epoch 114 of 200 took 518.630s\n",
      "  training loss:\t\t0.000058\n",
      "  validation loss:\t\t0.092057\n",
      "  validation accuracy:\t\t99.02 %\n",
      "Epoch 115 of 200 took 518.609s\n",
      "  training loss:\t\t0.000057\n",
      "  validation loss:\t\t0.091769\n",
      "  validation accuracy:\t\t99.03 %\n",
      "Epoch 116 of 200 took 518.675s\n",
      "  training loss:\t\t0.000057\n",
      "  validation loss:\t\t0.091724\n",
      "  validation accuracy:\t\t99.03 %\n",
      "Epoch 117 of 200 took 518.662s\n",
      "  training loss:\t\t0.000058\n",
      "  validation loss:\t\t0.092056\n",
      "  validation accuracy:\t\t99.02 %\n",
      "Epoch 118 of 200 took 518.841s\n",
      "  training loss:\t\t0.000057\n",
      "  validation loss:\t\t0.091962\n",
      "  validation accuracy:\t\t99.02 %\n",
      "Epoch 119 of 200 took 519.226s\n",
      "  training loss:\t\t0.000058\n",
      "  validation loss:\t\t0.091550\n",
      "  validation accuracy:\t\t99.03 %\n",
      "Epoch 120 of 200 took 519.148s\n",
      "  training loss:\t\t0.000057\n",
      "  validation loss:\t\t0.091727\n",
      "  validation accuracy:\t\t99.03 %\n",
      "Epoch 121 of 200 took 518.649s\n",
      "  training loss:\t\t0.000057\n",
      "  validation loss:\t\t0.091499\n",
      "  validation accuracy:\t\t99.04 %\n",
      "Epoch 122 of 200 took 518.648s\n",
      "  training loss:\t\t0.000057\n",
      "  validation loss:\t\t0.091426\n",
      "  validation accuracy:\t\t99.03 %\n",
      "Epoch 123 of 200 took 518.691s\n",
      "  training loss:\t\t0.000057\n",
      "  validation loss:\t\t0.091598\n",
      "  validation accuracy:\t\t99.03 %\n",
      "Epoch 124 of 200 took 518.780s\n",
      "  training loss:\t\t0.000057\n",
      "  validation loss:\t\t0.092511\n",
      "  validation accuracy:\t\t99.01 %\n",
      "Epoch 125 of 200 took 518.742s\n",
      "  training loss:\t\t0.000056\n",
      "  validation loss:\t\t0.091404\n",
      "  validation accuracy:\t\t99.03 %\n",
      "Epoch 126 of 200 took 518.692s\n",
      "  training loss:\t\t0.000056\n",
      "  validation loss:\t\t0.092471\n",
      "  validation accuracy:\t\t99.03 %\n",
      "Epoch 127 of 200 took 518.738s\n",
      "  training loss:\t\t0.000057\n",
      "  validation loss:\t\t0.092281\n",
      "  validation accuracy:\t\t99.03 %\n",
      "Epoch 128 of 200 took 518.686s\n",
      "  training loss:\t\t0.000057\n",
      "  validation loss:\t\t0.092041\n",
      "  validation accuracy:\t\t99.03 %\n",
      "Epoch 129 of 200 took 518.683s\n",
      "  training loss:\t\t0.000057\n",
      "  validation loss:\t\t0.091264\n",
      "  validation accuracy:\t\t99.04 %\n",
      "Epoch 130 of 200 took 518.729s\n",
      "  training loss:\t\t0.000056\n",
      "  validation loss:\t\t0.091803\n",
      "  validation accuracy:\t\t99.03 %\n",
      "Epoch 131 of 200 took 518.648s\n",
      "  training loss:\t\t0.000056\n",
      "  validation loss:\t\t0.091846\n",
      "  validation accuracy:\t\t99.03 %\n",
      "Epoch 132 of 200 took 518.723s\n",
      "  training loss:\t\t0.000057\n",
      "  validation loss:\t\t0.092398\n",
      "  validation accuracy:\t\t99.03 %\n",
      "Epoch 133 of 200 took 518.651s\n",
      "  training loss:\t\t0.000056\n",
      "  validation loss:\t\t0.092041\n",
      "  validation accuracy:\t\t99.03 %\n",
      "Epoch 134 of 200 took 518.607s\n",
      "  training loss:\t\t0.000056\n",
      "  validation loss:\t\t0.092550\n",
      "  validation accuracy:\t\t99.02 %\n",
      "Epoch 135 of 200 took 518.727s\n",
      "  training loss:\t\t0.000056\n",
      "  validation loss:\t\t0.092301\n",
      "  validation accuracy:\t\t99.02 %\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-80-46c5a900cd85>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0miterate_minibatches\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m128\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m         \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtargets\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m         \u001b[0mtrain_err\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mtrain_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m         \u001b[0mtrain_batches\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/d/nolearn/local/lib/python2.7/site-packages/theano/compile/function_module.pyc\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    857\u001b[0m         \u001b[0mt0_fn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    858\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 859\u001b[1;33m             \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    860\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    861\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'position_of_error'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "# Finally, launch the training loop.\n",
    "print(\"Starting training...\")\n",
    "# We iterate over epochs:\n",
    "for epoch in range(num_epochs):\n",
    "    if epoch in learning_rate_schedule:\n",
    "        lr = np.float32(learning_rate_schedule[epoch])\n",
    "        print(\" setting learning rate to %.7f\" % lr)\n",
    "        learning_rate.set_value(lr)\n",
    "    # In each epoch, we do a full pass over the training data:\n",
    "    train_err = 0\n",
    "    train_batches = 0\n",
    "    start_time = time.time()\n",
    "    for batch in iterate_minibatches(X_train2, y_train2, 128, shuffle=False):\n",
    "        inputs, targets = batch\n",
    "        train_err += train_fn(inputs, targets)\n",
    "        train_batches += 1\n",
    "\n",
    "    # And a full pass over the validation data:\n",
    "    val_err = 0\n",
    "    val_acc = 0\n",
    "    val_batches = 0\n",
    "    for batch in iterate_minibatches(X_val, y_val, 128, shuffle=False):\n",
    "        inputs, targets = batch\n",
    "        err, acc = val_fn(inputs, targets)\n",
    "        val_err += err\n",
    "        val_acc += acc\n",
    "        val_batches += 1\n",
    "\n",
    "    # Then we print the results for this epoch:\n",
    "    print(\"Epoch {} of {} took {:.3f}s\".format(\n",
    "        epoch + 1, num_epochs, time.time() - start_time))\n",
    "    print(\"  training loss:\\t\\t{:.6f}\".format(train_err / train_batches))\n",
    "    print(\"  validation loss:\\t\\t{:.6f}\".format(val_err / val_batches))\n",
    "    print(\"  validation accuracy:\\t\\t{:.2f} %\".format(\n",
    "        val_acc / val_batches * 100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final results:\n",
      "  test loss:\t\t\t0.836620\n",
      "  test accuracy:\t\t91.32 %\n"
     ]
    }
   ],
   "source": [
    "# After training, we compute and print the test error:\n",
    "test_err = 0\n",
    "test_acc = 0\n",
    "test_batches = 0\n",
    "for batch in iterate_minibatches(X_test, y_test, 128, shuffle=False):\n",
    "    inputs, targets = batch\n",
    "    err, acc = val_fn(inputs, targets)\n",
    "    test_err += err\n",
    "    test_acc += acc\n",
    "    test_batches += 1\n",
    "print(\"Final results:\")\n",
    "print(\"  test loss:\\t\\t\\t{:.6f}\".format(test_err / test_batches))\n",
    "print(\"  test accuracy:\\t\\t{:.2f} %\".format(\n",
    "    test_acc / test_batches * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.savez('/home/d/resnet.npz', *lasagne.layers.get_all_param_values(network))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<lasagne.layers.special.NonlinearityLayer at 0x7f674cf3b410>"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "network.input_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with np.load('/home/d/resnet.npz') as f:\n",
    "    param_values = [f['arr_%d' % i] for i in range(len(f.files))]\n",
    "lasagne.layers.set_all_param_values(network, param_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, accuracy_score, roc_curve, auc, precision_recall_fscore_support, f1_score, precision_recall_curve, average_precision_score, zero_one_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_prediction = lasagne.layers.get_output(network, deterministic=True)\n",
    "test_loss = lasagne.objectives.categorical_crossentropy(test_prediction,\n",
    "                                                        target_var)\n",
    "test_loss = test_loss.mean()\n",
    "# As a bonus, also create an expression for the classification accuracy:\n",
    "test_acc = T.mean(T.eq(T.argmax(test_prediction, axis=1), target_var),\n",
    "                  dtype=theano.config.floatX)\n",
    "\n",
    "pred_fn = theano.function([input_var, target_var], [test_prediction, test_loss, test_acc])\n",
    "pred2_fn = theano.function([input_var], [test_prediction])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "6000\n",
      "7000\n",
      "8000\n",
      "9000\n",
      "10000\n",
      "11000\n",
      "12000\n",
      "13000\n",
      "14000\n",
      "15000\n",
      "16000\n",
      "17000\n"
     ]
    }
   ],
   "source": [
    "all_preds = []\n",
    "for i,p in enumerate(X_test):\n",
    "    pred = pred2_fn(p.reshape(1,4,75,75))\n",
    "    all_preds.append(pred[0][:,1][0].astype(np.uint8))\n",
    "    if i % 1000 == 0:\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.79      0.79      0.79      8780\n",
      "          1       0.79      0.79      0.79      8780\n",
      "\n",
      "avg / total       0.79      0.79      0.79     17560\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, all_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
